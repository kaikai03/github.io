Title: PCA
Date: 2014-08-15 22:49
Modified: 2014-08-15 22:49
Category: MachineLearning
Tags: algorithm,PCA,ML,Math
Slug: PCA_2014_08_15_22_49
Authors: kai_kai03
Summary: 读书笔记--主成份分析！

## Principal Component Analysis ##

PCA 不仅仅是对高维数据进行降维，更重要的是经过降维去除了噪声，发现了数据中的模式。

PCA把原先的n个特征用数目更少的m个特征取代，新特征是旧特征的线性组合，这些线性组合最大化样本方差，尽量使新的m个特征互不相关。从旧特征到新特征的映射捕获数据中的固有变异性。

### 降维的必要性 ###
- 多重共线性--预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。
- 高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有0.02%。
- 过多的变量会妨碍查找规律的建立。
- 仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。

### 降维的目的 ###
- 减少预测变量的个数
- 确保这些变量是相互独立的
- 提供一个框架来解释结果

### 降维的方法 ###
- 主成分分析
- 因子分析
- 用户自定义复合
- 等等......

## 预备知识 ##

### 协方差 ###

样本X和样本Y的协方差(Covariance)：

$$ Cov(X,Y)=\frac{\sum_{i=1}^{n}{(X_i-\overline{X})(Y_i-\overline{Y})}}{(n-1)} $$

协方差为正时说明X和Y是正相关关系，协方差为负时X和Y是负相关关系，协方差为0时X和Y相互独立。

Cov(X,X)就是X的方差(Variance)。

当样本是n维数据时，它们的协方差实际上是协方差矩阵（对称方阵），方阵的边长是。比如对于3维数据(x,y,z)，计算它的协方差就是：$ C_n^2 $。比如对于3维数据(x,y,z)，计算它的协方差就是：

$$ C=\begin{array}{ccc}cov(x,x)& cov(x,y)& cov(x,z) \\\ cov(y,x)&cov(y,y)& cov(y,z) \\\ cov(z,x)& cov(z,y)& cov(z,z)\end{array} $$

若 $ AX=\lambda{X} $，则称$ \lambda $是A的特征值，X是对应的特征向量。实际上可以这样理解：矩阵A作用在它的特征向量X上，仅仅使得X的长度发生了变化，缩放比例就是相应的特征值$ \lambda $。

当A是n阶可逆矩阵时，A与$ P^{-1}Ap $相似，相似矩阵具有相同的特征值。

特别地，当A是对称矩阵时，A的奇异值等于A的特征值，存在正交矩阵$ Q(Q^{-1}=QT)$，使得：

$$ Q^TAQ=\begin{pmatrix} &\lambda_1 & & & \\\ & &\lambda_2 & &\\\ & & &... & \\\ & & & &\lambda_n \end{pmatrix} $$

对A进行奇异值分解就能求出所有特征值和Q矩阵。

$ A\*Q=Q\*D $, D是由特征值组成的对角矩阵

由特征值和特征向量的定义知，Q的列向量就是A的特征向量。

## 正题 ##

### 步骤 ###

1. 指标数据标准化； 　　
2. 指标之间的相关性判定； 　
3. 计算特征值与特征向量
4. 计算主成分贡献率及累计贡献率
5. 计算主成分载荷

*pca.py*

		#-*- coding:utf-8 -*-
		from pylab import *
		from numpy import *
		
		def pca(data,nRedDim=0,normalise=1):
		   
		    # 数据标准化
		    m = mean(data,axis=0)
		    data -= m
		
		    # 协方差矩阵
		    C = cov(transpose(data))
		
		    # 计算特征值特征向量，按降序排序
		    evals,evecs = linalg.eig(C)
		    indices = argsort(evals)
		    indices = indices[::-1]
		    evecs = evecs[:,indices]
		    evals = evals[indices]
		
		    if nRedDim>0:
		        evecs = evecs[:,:nRedDim]
		   
		    if normalise:
		        for i in range(shape(evecs)[1]):
		            evecs[:,i] / linalg.norm(evecs[:,i]) * sqrt(evals[i])
		
		    # 产生新的数据矩阵
		    x = dot(transpose(evecs),transpose(data))
		    # 重新计算原数据
		    y=transpose(dot(evecs,x))+m
		    return x,y,evals,evecs

`__main__:`

	#-*- coding:utf-8 -*-
	from pylab import *
	from numpy import *
	
	import pca
	mpl.rcParams['font.sans-serif'] = ['SimHei']
	mpl.rcParams['axes.unicode_minus'] = False
	x = random.normal(5,.5,1000)
	y = random.normal(3,1,1000)
	a = x*cos(pi/4) + y*sin(pi/4)
	b = -x*sin(pi/4) + y*cos(pi/4)
	
	plot(a,b,'.')
	xlabel('x')
	ylabel('y')
	title('原数据集')
	data = zeros((1000,2))
	data[:,0] = a
	data[:,1] = b
	
	x,y,evals,evecs = pca.pca(data,1)
	print y
	figure()
	plot(y[:,0],y[:,1],'.')
	xlabel('x')
	ylabel('y')
	title('重新构造数据')
	show()

<center>![1.png]({filename}/article_img/PCA_2014_08_15_22_49/1.png)</center>
<center>![2.png]({filename}/article_img/PCA_2014_08_15_22_49/2.png)</center>