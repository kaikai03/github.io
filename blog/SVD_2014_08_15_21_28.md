Title: 奇异值
Date: 2014-08-15 20:55
Modified: 2014-08-15 21:28
Category: MachineLearning
Tags: algorithm,PCA,ML,Math
Slug: SVD_2014_08_15_21_28
Authors: kai_kai03
Summary: 读书笔记--奇异值分解！

## SVD ##

### 先简单概念性质的东西 ###

SVD分解（奇异值分解），本应是本科生就掌握的方法，然而却经常被忽视。实际上，SVD分解不但很直观，而且极其有用。SVD分解提供了一种方法将一个矩阵拆分成简单的，并且有意义的几块。它的几何解释可以看做将一个空间进行旋转，尺度拉伸，再旋转三步过程。

 
首先来看一个对角矩阵

$$ M=\begin{bmatrix}3 &0 \\\ 0 &1 \end{bmatrix} $$

几何上, 我们将一个矩阵理解为对于点(x, y)从一个平面到另一个平面的映射:

$$ \begin{bmatrix}3 &0 \\\ 0 &1 \end{bmatrix} \begin{bmatrix}x \\\ y \end{bmatrix} = \begin{bmatrix}3x \\\ y \end{bmatrix} $$

下图显示了这个映射的效果: 平面被横向拉伸了3倍，纵向没有变化。

<center>![1.jpg]({filename}/article_img/SVD_2014_08_15_21_28/1.jpg)</center>

对于另一个矩阵

$$ M=\begin{bmatrix}2 &1 \\\ 1 &2 \end{bmatrix} $$

它的效果是

<center>![2.jpg]({filename}/article_img/SVD_2014_08_15_21_28/2.jpg)</center>

这样一个变化并不是很好描述，然而当我们将坐标系旋转45度后，我们可以看出

<center>![3.jpg]({filename}/article_img/SVD_2014_08_15_21_28/3.jpg)</center>

这时，我们发现这个新的网格上发生的变化和网格在对角阵下发生变化的效果相似。
 
这是一个对称矩阵的例子，可以看出，对称矩阵经过旋转后，其作用就和对角阵类似了。数学上，对于一个对称矩阵 M, 我们可以找到一组正交向量 $v_i$ 从而 $Mv_i$ 相当于 $v_i$ 上的标量乘积; 也就是

$$ Mv_i = \lambda_i v_i $$

$ \lambda_i $ 是标量，也就是对应对角阵中对角线上的元素. 由于这个性质，我们称 $v_i$ 是 M 的特征向量;  $\lambda_i$ 为特征值. 一个对称矩阵不同特征值对应的特征向量是正交的。

对于更广泛的情况，我们看看是否能从一个正交网格转换到另一个正交网格. 考虑一个非对称矩阵:

$$ M=\begin{bmatrix}1 &1 \\\ 0 &1 \end{bmatrix} $$

这个矩阵的效果形象的称为剃刀（shear）。

<center>![4.jpg]({filename}/article_img/SVD_2014_08_15_21_28/4.jpg)</center>

这个矩阵将网格在水平方向拉伸了，而垂直方向没有变化。如果我们将网格旋转大约58度，这两个网格就又会都变为正交的了。 

<center>![5.jpg]({filename}/article_img/SVD_2014_08_15_21_28/5.jpg)</center>

### 奇异值分解 ###

考虑一个 2*2 矩阵, 我们可以找到两组网格的对应关系。用向量表示，那就是当我们选择合适的单位正交向量 $v_1$ 和 $v_2$, $Mv_1$ 和 $Mv_2$ 也是正交的。

<center>![6.jpg]({filename}/article_img/SVD_2014_08_15_21_28/6.jpg)</center>

我们使用 $u_1$ 和 $u_2$ 代表 $Mv_1$ 和 $Mv_2$的方向.  $Mv_1$ 和 $Mv_2$ 的长度表示为 $\sigma_1$ 和 $\sigma_2$，也就是网格在每个方向的拉伸. 这两个拉伸值叫做M的 奇异值（sigular value）

<center>![7.jpg]({filename}/article_img/SVD_2014_08_15_21_28/7.jpg)</center>

和前面类似，我们可以有  

$$ \begin{align\*}Mv_1 = \sigma_1u_1  \\\ Mv_2 = \sigma_2u_2 \end{align\*} $$

我们一直讨论的 $v_1$ 和 $v_2$ 是一对正交向量， 对于一般的向量 x，我们有这样的投影关系

$$ x=(v_1 \cdot x)v_1 + (v_2 \cdot x)v_2 $$

也就是说  

$$ Mx=(v_1 \cdot x)Mv_1 + (v_2 \cdot x)Mv_2 $$

$$ Mx = (v_1 \cdot x) \sigma_1u_1 + (v_2 \cdot x) \sigma_2u_2 $$

注意点积可以用向量的转置来计算

$$ v \cdot x = v^Tx $$

既

$$ Mx = u_1\sigma_1 v_1^Tx + u_2\sigma_2 v_2^Tx  \rightarrow      M = u_1\sigma_1 v_1^T + u_2\sigma_2 v_2^T $$

这个关系可以写成矩阵形式

$$ M = U \Sigma V^T $$

U 是列向量 $u_1$ 和 $u_2$组成的矩阵, $\Sigma$是非零项为$\sigma_1$ 和 $\sigma_2$ 的对角矩阵,  V是列向量 $v_1$ 和 $v_2$组成的矩阵. 带有上标T的矩阵V是矩阵V的转置. <del>即V描述了域中的一组正交基，U描述了相关域的另一组正交基，$\Sigma$ 表述了U中的向量与V中向量的拉伸关系。</del>

上面描述了怎样将矩阵M分解成三个矩阵的乘积：V描述了原始空间中的正交基，U描述了相关空间的正交基，$\Sigma$描述了V中的向量变成U中的向量时被拉伸的倍数。
  
### 寻找奇异值分解 ###

奇异值分解可以应用于任何矩阵，对于前面的例子，如果我们加上一个单位圆，那它会映射成一个椭圆，椭圆的长轴和短轴定义了新的域中的正交网格，可以被表示为$Mv_1$ and $Mv_2$。

<center>![8.jpg]({filename}/article_img/SVD_2014_08_15_21_28/8.jpg)</center>

<center>![9.jpg]({filename}/article_img/SVD_2014_08_15_21_28/9.jpg)</center>

换句话说，单位圆上的函数 $\left| Mx \right|$ 在  $v_1$ 取得最大值，在 $v_2$取得最小值. 这将单位圆上的函数优化问题简化了。可以证明，这个函数的极值点就出现在$M^TM$的特征向量上，这个矩阵一定是对称的，所以不同特征值对应的特征向量$v_i$是正交的.

$ \sigma_i = \left| Mv_i \right| $就是奇异值,  $u_i$ 是 $Mv_i$方向的单位向量.

$$ Mv_i = \sigma_iu_i \\\ Mv_j = \sigma_ju_j \\\ Mv_i \cdot  Mv_j = v_i^TM^T Mv_j = v_i \cdot M^TMv_j = \lambda_jv_i \cdot  v_j = 0  $$

也就是

$$ Mv_i \cdot Mv_j = \sigma_i\sigma_j u_i \cdot u_j = 0 $$

因此, $u_i 和 u_j$ 也是正交的。所以我们就把一组正交基 $v_i$ 变换到了另一组正交基 $u_i$. 

### 另一个例子 ###

我们来看一个奇异矩阵（秩为1，或只有一个非零奇异值）

$$ M=\begin{bmatrix}1 &1 \\\ 2 &2 \end{bmatrix} $$

它的效果如下

<center>![10.jpg]({filename}/article_img/SVD_2014_08_15_21_28/10.jpg)</center>

在这个例子中，第二个奇异值为0，所以 $M = u_1\sigma_1 v_1^T$. 

也就是说，如果有奇异值为0，那么这个矩阵就有降维的效果。因为0奇异值对应的维度就不会出现在右边。（换句话讲，如果一些奇异值为零，相应的项将不会出现在M的分解中。因此，矩阵M的秩（即线性独立的行或列的个数）等于非零奇异值的个数。）

### 数据压缩 ###

这对于计算机科学中的数据压缩极其有用。例如我们想压缩下面的15*25 像素的黑白图像

<center>![11.jpg]({filename}/article_img/SVD_2014_08_15_21_28/11.jpg)</center>

因为在图像中只有三种类型的列（如下），它可以以更紧凑的形式被表示。 

<center>![12.jpg]({filename}/article_img/SVD_2014_08_15_21_28/12.jpg)</center>

<center>![13.jpg]({filename}/article_img/SVD_2014_08_15_21_28/13.jpg)</center>

<center>![14.jpg]({filename}/article_img/SVD_2014_08_15_21_28/14.jpg)</center>

我们用15*25的矩阵来表示这个图像，其中每个元素非0即1，0表示黑色像素，1表示白色像素。如下所示，共有375个元素。

<center>![15.jpg]({filename}/article_img/SVD_2014_08_15_21_28/15.jpg)</center>

如果对M进行奇异值分解的话，我们只会得到三个非零的奇异值。

$$ \sigma_1 = 14.72, \sigma_2 = 5.22,\sigma_3 = 3.31  $$

因此，矩阵可以如下表示

$$ M=u_1\sigma_1 v_1^T + u_2\sigma_2 v_2^T + u_3\sigma_3 v_3^T $$

我们有三个包含15个元素的向量$v_i$，三个包含25个元素的向量$u_i$，以及三个奇异值$\sigma_i$。这意味着我们可以只用123个数字就能表示这个矩阵而不是出现在矩阵中的375个元素。奇异值分解找到了矩阵中的冗余信息实现了降维。(在这种方式下，我们看到在矩阵中有3个线性独立的列，也就是说矩阵的秩是3)。

### 降噪 ###

从之前的例子看出我们利用了矩阵中有很多奇异值为0的特殊性。通常来说，越大的奇异值对应的信息越令人感兴趣。例如，想象我们用扫描仪将上面的图片输入到我们的计算机。但是，我们的扫描机会在图片上产生一些缺陷（通常称作“噪声”）。

<center>![16.jpg]({filename}/article_img/SVD_2014_08_15_21_28/16.jpg)</center>

我们以同样的方式处理：用15*25矩阵来表示图像，然后进行奇异值分解。我们得到以下奇异值：

$$ \sigma_1 = 14.15,\sigma_2 = 4.67,\sigma_3 = 3.00,\sigma_4 = 0.21,\sigma_5 = 0.19,,,\sigma_15 = 0.05$$

很明显，头三个奇异值是最重要的，所以我们假定其他的都是图像上的噪声，并假设假设

$$ M \approx u_1\sigma_1 v_1^T + u_2\sigma_2 v_2^T + u_3\sigma_3 v_3^T $$

这就产生了如下的优化后的图片

<center>![17.jpg]({filename}/article_img/SVD_2014_08_15_21_28/17.jpg)</center>

### 数据分析 ###
我们在收集数据的时候经常会遇到噪声：无论工具多好，总有一些误差在测量过程中。如果我们记得大的奇异值指向矩阵中重要的特征，很自然地想到用奇异值分解去研究被收集的数据。

例如，我们收集了一些数据如下：

<center>![18.jpg]({filename}/article_img/SVD_2014_08_15_21_28/18.jpg)</center>

如下是我们获得的数据，将其放入矩阵中：

<center>![19.jpg]({filename}/article_img/SVD_2014_08_15_21_28/19.jpg)</center>

然后进行奇异值分解。我们得到奇异值

$\sigma_1 = 6.04,\sigma_2 = 0.22 $

其中第一个奇异值远远大于另外一个，很安全的假设小的奇异值$\sigma_2$是数据中的噪声并且可以理想地认为是0。这个例子中的矩阵的秩是1，意味着所有数据都位于 $u_i$ 定义的线上。

<center>![20.jpg]({filename}/article_img/SVD_2014_08_15_21_28/20.jpg)</center>

这个简短的例子引出了主成分分析领域，展示了一系列用奇异值分解来检测数据依赖和冗余的技术。

同样地，奇异值分解可以用来检测数据中的簇，这就解释了奇异值分解可以用来尝试优化Netflix的电影推荐系统。程序会根据你看过的电影来对与你看过的电影相似的未看过的电影进行排序。推荐系统会挑选出你未看过的电影集合中预估分高的电影。

## 总结 ##
无论是特征值分解还是奇异值分解，都是为了让人们对矩阵（或者线性变换）的作用有一个直观的认识。这是因为我们拿过来一个矩阵，很多情况下只能看到一堆排列有序的数字，而看不到这些数字背后的真实含义，特征值分解和奇异值分解告诉了我们这些数字背后的真实含义，换句话说，它告诉了我们关于矩阵作用的本质信息。

奇异值分解的含义是，把一个矩阵A看成线性变换（当然也可以看成是数据矩阵或者样本矩阵），那么这个线性变换的作用效果是这样的，我们可以在原空间找到一组标准正交基V，同时可以在像空间找到一组标准正交基U，我们知道，看一个矩阵的作用效果只要看它在一组基上的作用效果即可，在内积空间上，我们更希望看到它在一组标准正交基上的作用效果。而矩阵A在标准正交基V上的作用效果恰好可以表示为在U的对应方向上只进行纯粹的伸缩！这就大大简化了我们对矩阵作用的认识，因为我们知道，我们面前不管是多么复杂的矩阵，它在某组标准正交基上的作用就是在另外一组标准正交基上进行伸缩而已。

特征分解也是这样的，也可以简化我们对矩阵的认识。对于可对角化的矩阵，该线性变换的作用就是将某些方向（特征向量方向）在该方向上做伸缩。

有了上述认识，当我们要看该矩阵对任一向量x的作用效果的时候，在特征分解的视角下，我们可以把x往特征向量方向上分解，然后每个方向上做伸缩，最后再把结果加起来即可；在奇异值分解的视角下，我们可以把x往V方向上分解，然后将各个分量分别对应到U方向上做伸缩，最后把各个分量上的结果加起来即可。

当我们注意到，不是所有的矩阵都能对角化（对称矩阵总是可以），而所有矩阵总是可以做奇异值分解的。那么多类型的矩阵，我们居然总是可以从一个统一且简单的视角去看它，我们就会感叹奇异值分解是多么奇妙了！

References:

>Gilbert Strang, Linear Algebra and Its Applications. Brooks Cole.Strang’s book is something of a classic though some may find it to be a little too formal.

>William H. Press et al, Numercial Recipes in C: The Art of Scientific Computing. Cambridge University Press.Authoritative, yet highly readable. Older versions [are available online](http://www.nr.com/oldverswitcher.html).

>Dan Kalman, [A Singularly Valuable Decomposition: The SVD of a Matrix](http://www.math.umn.edu/~lerman/math5467/svd.pdf), The College Mathematics Journal 27 (1996), 2-23.Kalman’s article, like this one, aims to improve the profile of the singular value decomposition. It also a description of how least-squares computations are facilitated by the decomposition.

>[If You Liked This, You’re Sure to Love That](http://www.nytimes.com/2008/11/23/magazine/23Netflix-t.html), The New York Times, November 21, 2008.This article describes Netflix’s prize competition as well as some of the challenges associated with it.
