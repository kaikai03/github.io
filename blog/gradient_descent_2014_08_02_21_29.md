Title: 梯度下降
Date: 2014-08-02 21:29
Modified: 2014-08-02 21:29
Category: MachineLearning
Tags: classification,algorithm,Python,Math
Slug: gradient_descent_2014_08_02_21_29
Authors: kai_kai03
Summary: 练个手，Python手写

## gradient descent ##

这是求解无约束优化问题最简单和最古老的方法，常用于机器学习和人工智能当中用来递归性地逼近最小偏差模型by wiki。
	
	f = lambda x : x**3 #随便写的个函数
	x = 2
	step = 0.01 
	lossChange = f(x)
	lossed = f(x)
	print 'x:', x, 'lossChange:', lossChange, 'loss:', lossed
	while lossChange > 0.00000001: #当变化小到一定程度是认为是局部最小
		x = x - step * 3 *( x**2) #减的是梯度方向上的变化
		lossChange = lossed - f(x)
		lossed = f(x)
		print 'x:', x, 'lossChange:', lossChange, 'loss:', lossed
		print x
		print f(x)

最后找到个局部最优解！

其迭代公式为 $ f(k+1)=f(k)-\theta f'(k) $,其中 $ \theta $是步长，或叫学习速率。步长越小收敛速度越慢，但步长过大有可能跳过不保证每次迭代都减少，甚至不一定收敛。

最后再说说意义：

>在标量场f中的一点处存在一个矢量G(或者上边的f`)，该矢量方向为f在该点处变化率最大的方向，其模也等于这个最大变化率的数值，则矢量G称为标量场f的梯度。<br>
在向量微积分中，标量场的梯度是一个向量场。<br>
标量场中某一点上的梯度指向标量场增长最快的方向，梯度的长度是这个最大的变化率。

通过负梯度，一步步逼近某个局部的最值。


## 进阶--随机梯度下降法 ##
stochastic gradient descent，也叫增量梯度下降。

由于梯度下降法收敛速度慢，而随机梯度下降法会快很多

* 根据某个单独样例的误差增量计算权值更新，得到近似的梯度下降搜索。（随机取一个样例）

* 可以看作为每个单独的训练样例定义不同的误差函数。

* 在迭代所有训练样例时，这些权值更新的序列给出了对于原来误差函数的梯度下降的一个合理近似。

* 通过使下降速率的值足够小，可以使随机梯度下降以任意程度接近于真实梯度下降。

标准梯度下降和随机梯度下降之间的关键区别

* 标准梯度下降是在权值更新前对所有样例汇总误差，而随机梯度下降的权值是通过考查某个训练样例来更新的。

* 在标准梯度下降中，权值更新的每一步对多个样例求和，需要更多的计算。

* 标准梯度下降，由于使用真正的梯度，标准梯度下降对于每一次权值更新经常使用比随机梯度下降大的步长。

* 如果标准误差曲面有多个局部极小值，随机梯度下降有时可能避免陷入这些局部极小值中。

