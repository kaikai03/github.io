<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>kairnsの记事簿</title><link href="/" rel="alternate"></link><link href="/feeds/machinelearning.atom.xml" rel="self"></link><id>/</id><updated>2018-06-27T23:11:00+08:00</updated><entry><title>研究概述样板</title><link href="/2018/sample_2018_06_27_23_11.html" rel="alternate"></link><updated>2018-06-27T23:11:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2018-06-27:2018/sample_2018_06_27_23_11.html</id><summary type="html">&lt;h3&gt;写在前面&lt;/h3&gt;
&lt;p&gt;就如summary所述，这是一篇为了给某大型“疾病+人工智能”的项目立项书写的智能算法方案部分的“参考文献”。完成于至少半年前，当你现在看到这篇东西的时候，它已经完成了它的使命--项目申报。当然正式使用的时候，文内的“医学名词”被替换成它需要申报的方向。&lt;/p&gt;
&lt;p&gt;炮制这篇文的的时候，为了让模版更具真实性，用的是真实的项目，真实的数据，真实的结果。然后再来引出算法。当然算法思路也是对的，引用的也是真实的，唯一缺的仅仅是没去做综合实证。&lt;/p&gt;
&lt;p&gt;想着这玩意可以重复使用，反正又不是出论文没查重问题（当然它的细节也不足以发论文就是了），非DS、CS专业的应该挺缺这种模版的，于是放出来当福利了。&lt;/p&gt;
&lt;h3&gt;XXX医学川崎病系列研究及阶段性概述&lt;/h3&gt;
&lt;p&gt;2017年，上海XXX医学与上海儿童医院合作开展川崎病系列研究，现已完成《小儿川崎病诊断模型的建立与研究》，《川崎病丙球无反应评分模型的建立与研究》，并在此基础上利用大数据及人工智能深度学习方法，建立了“川崎病临床辅助决策系统”，用于帮助医生提高 “是否为川崎病患者”以及“是否将会出现丙球无反应”这两个临床问题的判别准确率。 &lt;/p&gt;
&lt;p&gt;目前研究中，直接研究对象为2013-2016年入院的710例川崎病患儿。由于目的是为了建立精准川崎病预测模型和临床辅助决策系统，从各种相似患者中精准识别该疾病，所以项目实际从儿童医院历史数据中随机提取了40,000例具有发热、杨梅舌、四肢红肿等川崎病相似表现特征的患者诊疗记录作为补充数据，涉及指标项178个，其中71项检查检验指标，107项通过XXX医疗的病历文本结构化模块从患者病历文本中获得。经清洗后建立基于hadoop框架的高维矩阵大数据分析模型。&lt;/p&gt;
&lt;p&gt;传统的聚类方法有C-均值聚类和FCM软聚类算法[1]。在大量数据的场景下，用传统的方法直接聚类效率很低[2]，王宝文(2007)[3]等人提出基于分治方法的聚类算法，克服聚类有效性对样本空间分布的依赖，将高维样本的差异性转化为二维样本的差异性，实现高维样本向二维样本的映射，再利用传统算法进行聚类，得到7个分类。经过各分类对比观察，最终选定样本空间上与川崎病最相似的集合，其中患者诊疗记录6213例，包含已明确的595例川崎病病例。形成进一步研究样本集&lt;code&gt;set_A&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;传统主成份分析PCA[4]需要计算协方差矩阵，其空间复杂度为O(m2)，进行特征分解的时间复杂度为O(m3)。Qi etal.(2004)[5] 和 Kargupta et al.(2001) [6] 建议将数据分块，使用分布式来解决效率问题，但由于仍然需要重建中心化的协方差矩阵，所以依旧难解决在高纬度或多样本情况下的主成份分析的效率问题。对此李晨, 郭跃飞[7] 提出协方差无关的迭代主成分分析CIPCA算法，在效率改进方面表现优异。我们通过CIPCA算法对&lt;code&gt;set_A&lt;/code&gt;进行分析，得到23项，包含方差70%贡献率的指标项：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;发热天数&lt;/code&gt;、&lt;code&gt;年龄&lt;/code&gt;、&lt;code&gt;中性粒细胞与淋巴细胞比值（NLR）&lt;/code&gt;、&lt;code&gt;白蛋白（ALB）&lt;/code&gt;、&lt;code&gt;C反应蛋白（CRP）&lt;/code&gt;、&lt;code&gt;红细胞沉降速率（ESR）&lt;/code&gt;、&lt;code&gt;热退天数&lt;/code&gt;、&lt;code&gt;丙球使用剂量&lt;/code&gt;、&lt;code&gt;激素使用标记&lt;/code&gt;、&lt;code&gt;钾（K+）&lt;/code&gt;、&lt;code&gt;D-D二聚体&lt;/code&gt;、&lt;code&gt;血红蛋白（Hb）&lt;/code&gt;、&lt;code&gt;性别&lt;/code&gt;、&lt;code&gt;白细胞计数（WBC）&lt;/code&gt;、&lt;code&gt;肌酸激酶（CK）&lt;/code&gt;、&lt;code&gt;肌酐水平（Cr）FDP&lt;/code&gt;、&lt;code&gt;乳酸脱氢酶（LDH）&lt;/code&gt;、&lt;code&gt;血钠（Na+）&lt;/code&gt;、&lt;code&gt;肌酸激酶MB同工酶（CK-MB）&lt;/code&gt;、&lt;code&gt;血小板总数（PLT）&lt;/code&gt;、&lt;code&gt;中心粒细胞计数（NE#）&lt;/code&gt;、&lt;code&gt;淋巴细胞计数（LY#）&lt;/code&gt;、&lt;code&gt;谷丙转氨酶（ALT）&lt;/code&gt;、&lt;code&gt;肌酸激酶MB同工酶（CK-MB）&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;根据我们的预定目的，将会对数据做如下预测： &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;是否川崎病&lt;/li&gt;
&lt;li&gt;是否发生丙球反应。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以同样的对研究数据建立相对应的4分类标签:
&lt;code&gt;｛label A | 川崎病丙球有反应，川崎病丙球无反应，非川崎病丙球有反应，非川崎病丙球无反应｝&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;传统的统计学方法对资料有一定的限制，医学现象错综复杂，自变量与因变量间可能存在着复杂的非线性关系以及变量之间可能存在交互作用等，在变量之间关系未正确处理的情况下将会很难估计出最优回归参数[8]。我们当前研究变量过多，回归前预处理极为复杂。谭英(2012)[8]，温变珍(2010)[9]等在过去的研究中，均发现神经网络对资料限制少，泛化能力强，且具有良好的非线性和对未知数据的处理能力等优点。进一步资料搜索，我们发现多层感知器神经网络(MLP)[11]正适合处理本研究中变量之间隐含高维特征的情况。&lt;/p&gt;
&lt;p&gt;我们对&lt;code&gt;set_A&lt;/code&gt;进行人工标记:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据最终诊断确定是否为川崎病。&lt;/li&gt;
&lt;li&gt;根据丙球使用时间与激素使用时间确定是否发生丙球反应。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;并挑选主要方差提供指标，形成一个四分类的训练集&lt;code&gt;train_A&lt;/code&gt;，对MLP模型进行训练。&lt;/p&gt;
&lt;p&gt;预测标准误差在13轮训练后收敛至0.0093并保持平稳。使用40,000例全部病例进行回测，准确率为0.81，结果并不理想。经研究发现误差积累的速度与所使用的预测数据与&lt;code&gt;set_A&lt;/code&gt;的距离呈正比，意味着&lt;code&gt;MLP&lt;/code&gt;对本次使用数据集的个局部特征泛化存在缺陷。&lt;/p&gt;
&lt;p&gt;随后根据&lt;code&gt;马艳东(2015)[12]&lt;/code&gt;改进局部泛化误差模型的方法对原&lt;code&gt;MLP&lt;/code&gt;模型进行优化，创新&lt;code&gt;AG-MLP&lt;/code&gt;(Artifical Generalization-Multi-layer perceptron)。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;以batch形式替代原逐个样本训练。每次训练挑选固定数量的同label样本作为一个batch。&lt;/li&gt;
&lt;li&gt;当训练到负样本时，随机挑选一些&lt;code&gt;set_B&lt;/code&gt;中，与&lt;code&gt;train_A&lt;/code&gt;距离较大的未知样本加入batch中。&lt;/li&gt;
&lt;li&gt;对每个batch进行L2范数正则。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们设定每batch取8个样本重新进行训练，预测标准误差在20轮训练后收敛至0.002一杯，并保持平稳。使用40,000例全部病例进行回测，准确率为0.9236，我们认为达到可用状态。&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[1] Zhang Yuanquan, Rueda L. A Geometric Framework to Visualize Fuzzy-clustered Data[C]//Proceedings of IEEE 25th International Conference of the Chilean Computer Science Society. Valdivia, Chile: [s. n.], 2005: 8-13. &lt;/p&gt;
&lt;p&gt;[2] Davidson I, Satyanarayana A. Speeding Up K-means Clustering by Bootstrap Averaging[C]//Proc. of IEEE Data Mining Workshop on Clustering Large Data Sets. Brighton, UK: [s. n.], 2004: 98-102.&lt;/p&gt;
&lt;p&gt;[3] 王宝文, 阎俊梅, 刘文远等.基于分治法的高维大数据集模糊聚类算法[J]. 计算机工程, 2007, 33(24):60-62.&lt;/p&gt;
&lt;p&gt;[4] Joliffe IT. Principal component analysis[M]. New York, USA: Springer-Verlag, 1986.&lt;/p&gt;
&lt;p&gt;[5] Qi H, Wang T W, Birdwell J D. Global principal component analysis for dimensionality reduction in distributed data mining[J]. Statistical Data Mining and Knowledge Discovery, 2004: 327-342.&lt;/p&gt;
&lt;p&gt;[6] Kargupta H, Huang W Y, Sivakumar K, et al. Distributed clustering using collective principal component analysis[J]. Knowledge and Information System, 2001 3(4): 422-448.&lt;/p&gt;
&lt;p&gt;[7] 李晨, 郭跃飞. 一种用于高维大数据的协方差无关的主成分分析迭代算法(英文)[J]. 复旦学报(自然科学版), 2013, 52(2):207-214. &lt;/p&gt;
&lt;p&gt;[8] 王梅生. 线性回归分析中自变量间的相关性问题[J]. 东北大学学报(自然科学版), 1985, 6(1):93-104.&lt;/p&gt;
&lt;p&gt;[9] 谭英. 用人工神经网络建立缺血性脑卒中患者复发的预测模型[D]. 徐州医学院, 2012.&lt;/p&gt;
&lt;p&gt;[10] 温变珍. BP神经网络在大肠癌预后分析中的应用[D]. 山西医科大学, 2010.&lt;/p&gt;
&lt;p&gt;[11] EthemAlpaydin, 阿培丁, Alpaydin,等. 机器学习导论[M]. 机械工业出版社, 2014.&lt;/p&gt;
&lt;p&gt;[12] 马艳东. 改进的局部泛化误差模型及其在特征选择中的应用[J]. 中国科技信息, 2015(10):41-42.&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="algorithm"></category><category term="ML"></category></entry><entry><title>缺失数据插补</title><link href="/2018/MissingValues_2018_06_11_00_05.html" rel="alternate"></link><updated>2018-06-11T00:05:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2018-06-11:2018/MissingValues_2018_06_11_00_05.html</id><summary type="html">&lt;h2&gt;interpolation&lt;/h2&gt;
&lt;h3&gt;扯个蛋&lt;/h3&gt;
&lt;p&gt;日常工作繁重却只是在不断的重复着基本用不上啥脑子的活。最近产品发展放缓，让我这边终于有时间喘口气，减缓一下精神荒漠化进程。(扯....&lt;/p&gt;
&lt;p&gt;现在产品中，大多研发任务都集中在一般业务功能实现上。在这个漫天都是artificial intelligence除了&lt;code&gt;病历结构化&lt;/code&gt;之外，也没什么纯科技上能拿得出手的亮点，吹逼力不足。&lt;/p&gt;
&lt;p&gt;于是拍拍脑袋，一个看起来很神奇，听起来好牛逼，说起来振奋人心，又人畜无害、没啥成本的功能的开发计划就这么诞生了。&lt;/p&gt;
&lt;h3&gt;溯源&lt;/h3&gt;
&lt;p&gt;“缺失值填补”、“缺失数据插补”、“丢失数据还原”。&lt;/p&gt;
&lt;p&gt;其实这在“数据科研者（狭义上的）”眼中属于徒劳的劳动付出。除了近年来数据不足无法满足机器学习需要做数据扩容之外，这类功能基本上处于毫无意义的状态，至少在科学研究中没有作用。&lt;/p&gt;
&lt;p&gt;不过这个计划的提出并不是空穴来风。最初的想法来自于跟科研医生接触的感觉。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;由于同一个病的治疗标准并不严格，各医生各习惯，导致整体数据“不整齐”。&lt;/li&gt;
&lt;li&gt;大部分医生对数据处理没有太多的技能与概念，基本不拥有维度变换或其他绕过缺失影响的技能。面对缺失数据大多情况下就是直接剔除case。&lt;/li&gt;
&lt;li&gt;往往数据收集整理导出由“小医生”负责，而最终数据由“大医生”使用。缺失会给“小医生”造成工作（精神上）的压力。“小医生”不知道如何数据插补概念的可能性更大。&lt;/li&gt;
&lt;li&gt;今年医疗科研项目申报也喜欢吹人工智能。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最重要的一点，基本我接触到的科研参与人，都不知道他们（假装）常用的spss上就有这么个功能。（小声;^_^）&lt;/p&gt;
&lt;h3&gt;实现&lt;/h3&gt;
&lt;p&gt;先指明实验数据：不同分布的随机参数来产生多个自变量，非线性的随机模型生成因变量，以此来生成实验数据。实验过程中随机丢弃自变量。&lt;/p&gt;
&lt;p&gt;没用真实医疗数据来做尝试是为了保证数据本身信号的完整性，并且预先知道最“正确”的数据关系，有助于“调试”。&lt;/p&gt;
&lt;p&gt;实验过很多方法、方案，最终以spss中的最大期望输出作为参照，挑选出输出结果大约在7成的情况下，缺失填补项与真实值总差和远小于spss结果的方法。&lt;/p&gt;
&lt;h3&gt;具体的&lt;/h3&gt;
&lt;p&gt;尝试过无固定因变量，既循环所有变量，逐一地让其他变量回归于当前变量，效果并不太好，结局波动情况较大。故决定还是以确定因变量的方式来进行。&lt;/p&gt;
&lt;p&gt;尝试和推演过程不细表，直接说方法和结论吧。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;首先去除掉所有数据缺失的case。以广义线性模型先对完整数据进行一次回归，通过最小二乘求解参数。$$X_i = log(x_i);Y_i = log(y_i)  \qquad \qquad $$ $$\theta = (X^{T}X)^{-1}X^{T}Y \qquad \qquad（1）$$ &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对$\theta$进行从大到小排序，既得到了自变量对因变量的效用顺序，相当于“信号”强弱的序列，接下来将根据这个顺序，依次地对自变量进行回归。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;选择信号最强的变量（剔除缺失后），用（1）回归于因变量。此时解得的参数只有两个，通过参数得到该变量的缺失数据。$$x_i = e^{\frac{ln(y_i)-c}{a}};\theta = {a,c}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接下来加入次强的变量，剔除缺失case，回归，取得参数，加入有缺失的case，解出缺失的值，补上。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;不断重复上述步骤，至所有缺失项填补完成。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接下来进入最大期望的迭代过程。对于一个case有多个缺失值的情况，每次迭代只更新其中一个变量，其他变量锁死，既沿用上一轮迭代结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;直至收敛，既整体期望变化小于设定阈值。 &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;一些额外的问题&lt;/h3&gt;
&lt;h4&gt;关于“信号”强弱排序&lt;/h4&gt;
&lt;p&gt;为什么要从强信号至弱信号呢？理论上来说，无论什么顺序都将会得到同一个结果。但在实验过程中意识发现：如果先从弱信号开始，有可能会在一开始就得到一个很大的误差，而在后续的迭代过程中，会有比较大的概率走偏向一个极差的局部解或出现无法收敛的情况。所以以强信号变量做起点，将是一个只有好处没有坏处的选择。&lt;/p&gt;
&lt;h4&gt;关于加入有缺失的case&lt;/h4&gt;
&lt;p&gt;上边没有说，在求协方差$X^{T}X$的时候需要用相关的变量标准差做一次惩罚，否则在加入变量（第4步）或锁死变量（第6步）后更新数据，会过大地估计目标值。&lt;/p&gt;
&lt;h3&gt;写完收工&lt;/h3&gt;
&lt;p&gt;反正总结起来，这是“懂的人一脸冷漠，不懂的人一脸懵逼，半桶水的人一脸兴奋，市场管理的人吹暴”的一个没有价值却有意义的“水货”。本篇也是一篇纯粹的“灌水”文。&lt;/p&gt;</summary><category term="algorithm"></category><category term="ML"></category><category term="Math"></category></entry><entry><title>ZCA 的一种理解</title><link href="/2018/ZCA_2018_04_27_18_25.html" rel="alternate"></link><updated>2018-04-27T18:25:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2018-04-27:2018/ZCA_2018_04_27_18_25.html</id><summary type="html">&lt;h2&gt;ZCA&lt;/h2&gt;
&lt;h3&gt;唠叨&lt;/h3&gt;
&lt;p&gt;今天随手测试了下，用正态随机生成的数据，做PCA旋转原数据的时候直接旋转成近乎完美的QQ图了。几种样本基本叠加在对角线上，同时有很合理表现出“子集”的随机性，有那么点偏差，尝试用zca再转一下，果然漂亮的分开了。&lt;/p&gt;
&lt;p&gt;为什么写教程不用这么漂亮的图来做例子呢？&lt;/p&gt;
&lt;p&gt;&lt;img alt="1.jpg" src="/article_img/ZCA_2018_04_27_18_25/1.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="2.jpg" src="/article_img/ZCA_2018_04_27_18_25/2.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="3.jpg" src="/article_img/ZCA_2018_04_27_18_25/3.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;按照pca的定义，它是通过特征向量来投影，之后将旋转结果用特征值（标准差）去标准化数据，进而将矩阵缩放成单位为1的单位分布。&lt;/p&gt;
&lt;p&gt;zca是在pca的基础上，将数据变换回原有空间。
$$Y_{zcawhite}= U * Y_{pcawhite}$$&lt;/p&gt;
&lt;p&gt;但从今天的结果图来看，zca貌似可以这么解释：你pca不是把两个方向的特征给去掉了嘛，那么给你乘一个回去，那不不同类的数据它之间的“偏差”自然被放大。&lt;/p&gt;
&lt;p&gt;特征向量对各组的效用不一致，假设效用为$ U_i , V_i $ ,原先混在一起 $ U_i * V_i = C $ 是因为各组两个方向的乘积是同一个常数（当前用的例子中），被单位化后也是，但是$ U_i \neq  U_{i+1} $,$ V_i \neq  V_{i+1}$，所以只乘一个方向回去，便对各组产生不用效果，从表现上看就是被PCA聚到一起的各组数据沿不通方向分开了。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt; def zca(df):
    matrix = df.as_matrix()
    norm = (matrix-matrix.mean(axis=0))/matrix.std(axis=0)
    sigma = np.dot(norm, norm.T)/(norm.shape[1])
    U,S,V = np.linalg.svd(sigma)
    PCA_white = np.dot(U, 1.0/np.sqrt(np.diag(S) + 0.01)) 
    ZCAMat = np.dot(PCA_white, U.T)                    
    return np.dot(ZCAMat, df)
&lt;/pre&gt;&lt;/div&gt;</summary><category term="algorithm"></category><category term="ML"></category><category term="Math"></category></entry><entry><title>np.Cov</title><link href="/2017/COV_2017_08_23_20_30.html" rel="alternate"></link><updated>2017-08-23T20:30:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2017-08-23:2017/COV_2017_08_23_20_30.html</id><summary type="html">&lt;h2&gt;numpy&lt;/h2&gt;
&lt;h3&gt;发现&lt;/h3&gt;
&lt;p&gt;起因是无聊，明明协方差矩阵已经拿到了，又对某变量计算了一次方差，发现和协方差矩阵对角上对应值不符。明明同一个工具，这就怪了。
先是动手写了教科书说的方法做个测试，果然结果截然不同。&lt;/p&gt;
&lt;p&gt;很久之前写了个小工具来实现，所以在自己电脑上干活时比较少用numpy提供的方法来算协方差矩阵，偶尔用一下也没注意它的结果。今天这的偶然试了一下，发现结果跟下意识里的操作方式有点不太一样。&lt;/p&gt;
&lt;h3&gt;这个Cov不一样&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;默认变量是以行作为排列的，这点与大多数教科书上的描述方法都不一样了，在计算上要么&lt;code&gt;matrix.T&lt;/code&gt;将向量转到行上,或者将参数&lt;code&gt;rowvar&lt;/code&gt;设置为False。&lt;/li&gt;
&lt;li&gt;结果默认为无偏估计，既认为数据是样本数据，而不是总体，比较令人不舒服的地方是明明&lt;code&gt;np.var&lt;/code&gt;默认却是计算总体方差的。当然如果它连&lt;code&gt;np.var&lt;/code&gt;也是无偏的，那也就没今天这篇东西了。这里嘛，可以将参数&lt;code&gt;bias&lt;/code&gt;设置为False，使用&lt;code&gt;1/n&lt;/code&gt;进行计算。&lt;/li&gt;
&lt;/ol&gt;</summary><category term="algorithm"></category><category term="ML"></category><category term="Math"></category></entry><entry><title>ROC Curve</title><link href="/2017/ROC_2017_07_03_19_46.html" rel="alternate"></link><updated>2017-07-03T19:46:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2017-07-03:2017/ROC_2017_07_03_19_46.html</id><summary type="html">&lt;h2&gt;ROC&lt;/h2&gt;
&lt;h3&gt;哔哔哔哔&lt;/h3&gt;
&lt;p&gt;这次这个统计工具分析不管从原理还是步骤，都比之前的卡方简单好多，但是最麻烦的地方是绘图的结果，试了很多种方法，得到的结果都与SPSS不符。最后全靠蒙，把thershold均匀分1000份，啊！！！结果看起来一样了。。。。。&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="1.png" src="/article_img/ROC_2017_07_03_19_46/1.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h3&gt;正文&lt;/h3&gt;
&lt;p&gt;ROC的解释是反映敏感性和特异性连续变量的综合指标。简单来说就是显示&lt;code&gt;真阳性/(真阳性+真阴性)&lt;/code&gt; 与 &lt;code&gt;假阳性/(假阳性+真阴性)&lt;/code&gt;之间的关系，用别人文章里的话说就是预测命中率与错判率之间的关系。&lt;/p&gt;
&lt;p&gt;如果曲线下面积为1，既填满整个矩形，意味着预测准确率能够达到100%！&lt;/p&gt;
&lt;p&gt;具体过程比较简单，不分段解释了，直接贴完整的实验性质的代码吧。想知道详细过程的同学参看引用，特别是&lt;code&gt;第三篇&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;这里有一点需要提的是，关于ROC，在最常被引用的、最容易被搜出来的某篇论文中，把AUC的标准误差写成标准差，而这里上面提到的那个&lt;code&gt;第三篇&lt;/code&gt;则写的是标准误差，至于谁对谁错，又或是我理解错误，就没精力去深究了，至少在本文的前提下，这个&lt;code&gt;第三篇&lt;/code&gt;中所描述的，和SPSS的结果输出相符。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;# coding=utf-8&lt;/span&gt;
&lt;span class="n"&gt;__author__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;kai_kai03&amp;#39;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;

&lt;span class="c1"&gt;##这里我假装我的product是sex，sample为预测值。&lt;/span&gt;
&lt;span class="kp"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.65432&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.65484&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.65555&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.65835&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.65841&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.65847&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.65873&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.65879&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.65919&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.65951&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66061&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66067&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66106&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66120&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66133&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66145&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66164&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66164&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66165&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66167&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66171&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66177&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66184&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66184&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66222&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66261&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66275&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66287&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66293&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66294&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66300&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66301&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66325&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66332&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66338&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66365&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66383&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66383&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66390&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66390&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66423&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66428&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66437&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66441&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66448&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66448&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66450&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66473&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66481&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66493&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66494&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.66506&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;sex&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;poz&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;Z_MAX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;6.0&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;fabs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z_MAX&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
            &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
                &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((((((((&lt;/span&gt;&lt;span class="mf"&gt;0.000124818987&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;
                        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.001075204047&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.005198775019&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;
                        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.019198292004&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.059054035642&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;
                        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.151968751364&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.319152932694&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;
                        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.531923007300&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.797884560593&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;
                &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(((((((((((((&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.000045255659&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
                        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.000152529290&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.000019538132&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
                        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.000676904986&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.001390604284&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
                        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.000794620820&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.002034254874&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
                        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.006549791214&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.010557625006&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
                        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.011630447319&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.009279453341&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
                        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.005353579108&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.002141268741&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
                        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.000535310849&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.999936657524&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;getROCPiont&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;thershold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kp"&gt;product&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;boundary&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;sumTp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="n"&gt;sumFn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="n"&gt;sumFp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="n"&gt;sumTn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;

    &lt;span class="n"&gt;tpr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="n"&gt;fpr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;

    &lt;span class="n"&gt;leng&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;product&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;leng&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;thershold&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="kp"&gt;product&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;boundary&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;sumTp&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;thershold&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="kp"&gt;product&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;boundary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;sumFp&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;thershold&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="kp"&gt;product&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;boundary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;sumFn&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;thershold&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="kp"&gt;product&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;boundary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;sumTn&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;sumTp&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;tpr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;tpr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sumTp&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sumTp&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;sumFn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;sumFp&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;fpr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;fpr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sumFp&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sumFp&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;sumTn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;fpr&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;min_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;max_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;## 最大值最小值均匀分成1000份，得到一个阈值数组&lt;/span&gt;
    &lt;span class="n"&gt;thr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;min_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="n"&gt;roc_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;roc_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;thr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;## 这里我认为性别为1的为真。&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;getROCPiont&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;sex&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kp"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;roc_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;roc_y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


    &lt;span class="c1"&gt;## 获取输出表格的数据选取,均匀取出50个用于输出&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;thr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;roc_y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;  &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;roc_x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;##计算曲面下的面积。&lt;/span&gt;
    &lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;AUCList&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;last&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;continue&lt;/span&gt;
        &lt;span class="c1"&gt;##对曲线进行积分&lt;/span&gt;
        &lt;span class="n"&gt;area&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;roc_x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;roc_x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;roc_y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;AUC&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;area&lt;/span&gt;
        &lt;span class="c1"&gt;##将面积分成10片分别进行记录，用于后续计算方差。&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;last&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;fabs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;AUCList&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;AUCList&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;fabs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;last&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;fabs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;##因为跟计算方向有关系，有可能会算出负的来，所以要去掉符号。&lt;/span&gt;
    &lt;span class="n"&gt;AUC&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;fabs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;auc:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;

    &lt;span class="n"&gt;sump&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;sumn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;ttt&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sex&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;ttt&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;##相当于上面getROCPiont的参数boundary，这里我假设性别为1的为真。&lt;/span&gt;
            &lt;span class="n"&gt;sump&lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;sumn&lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="c1"&gt;##输出SPSS对应的观察值摘要表格&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;觀察值處理摘要:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;sump&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;sumn&lt;/span&gt;

    &lt;span class="n"&gt;q1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;AUCPow2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;q2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;AUCPow2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="o"&gt;+&lt;/span&gt;  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sumn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;AUCPow2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="o"&gt;+&lt;/span&gt;  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sump&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;AUCPow2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="o"&gt;/&lt;/span&gt;  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sump&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;sumn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;q:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;q1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;q2&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Error(auc):&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;error&lt;/span&gt;

    &lt;span class="c1"&gt;##标准误差&lt;/span&gt;
    &lt;span class="n"&gt;SE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;SE(auc):&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;SE&lt;/span&gt;

    &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;SE&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;z:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;p:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;poz&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;

    &lt;span class="n"&gt;meanAUC&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;

    &lt;span class="c1"&gt;##标准差&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;area&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;AUCList&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;area&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;meanAUC&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;SD&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sd&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;##置信区间上下限&lt;/span&gt;
    &lt;span class="n"&gt;Z_Alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.96&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;95% CI,下限:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;SE&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;Z_Alpha&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;95% CI,上限:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AUC&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;SE&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;Z_Alpha&lt;/span&gt;


    &lt;span class="c1"&gt;##绘图&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;duijiaoxian&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;duijiaoxian&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;duijiaoxian&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;coral&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;roc_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;roc_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1000&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;References:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/abcjennifer/article/details/7359370"&gt;ROC曲线-阈值评价标准. &lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic"&gt;Receiver operating characteristic. &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ncss.wpengine.netdna-cdn.com/wp-content/themes/ncss/pdf/Procedures/PASS/Confidence_Intervals_for_the_Area_Under_an_ROC_Curve.pdf"&gt;Confidence_Intervals_for_the_Area_Under_an_ROC_Curve. &lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="algorithm"></category><category term="ML"></category><category term="Math"></category></entry><entry><title>Chi-Square test</title><link href="/2017/ChiSquare_2017_06_28_19_29.html" rel="alternate"></link><updated>2017-06-28T19:29:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2017-06-28:2017/ChiSquare_2017_06_28_19_29.html</id><summary type="html">&lt;h2&gt;Chi-Square&lt;/h2&gt;
&lt;h3&gt;惯例的事前唠叨&lt;/h3&gt;
&lt;p&gt;最近医疗科研平台呢要开始制作数据数据分析部分，数据准备及数据操作设计好之后，利用空档期找找统计方法库，好安排后续的事。&lt;/p&gt;
&lt;p&gt;结果一调研下来，事情可没事前那么简单，SPSS的行为核心并不是统计方法本身，而是围绕着一个统计方法的一系列检测方法。这就麻烦了，特别是不同方法的误差估计不一样，即使同一个估计方法，不同的参数（人择），也会有不同结果。&lt;/p&gt;
&lt;p&gt;更要命的是，好多stat的库都是只提供统计方法本身，而不关注相关的分析部分。&lt;/p&gt;
&lt;p&gt;另，java上好用的库更少，现在项目要求又是java。&lt;/p&gt;
&lt;p&gt;由于我只是做设计验证，为了码代码的速度，主要过程用得还是python，中间会含一部分java，因为java中有现成的统计方法，顺便测试下方法计算结果对不对。&lt;/p&gt;
&lt;h3&gt;正文&lt;/h3&gt;
&lt;p&gt;本片主要是记录一下，以拟合SPSS输出结果为目的的卡方检验实现方法。&lt;/p&gt;
&lt;p&gt;卡方检验属于非参数检验的范畴，它是计算样本的实际值与理论值之间的偏离程度，这个程度值就是卡方值，卡方值越小，偏差越小，若两个值完全相等时，卡方值就为0，表明理论值完全符合。&lt;/p&gt;
&lt;p&gt;以下照着SPSS的输出，一步步处理数据。&lt;/p&gt;
&lt;h3&gt;第一步&lt;/h3&gt;
&lt;p&gt;统计有效值，计算缺失值，显示。这一步没什么好细说的，显示一下数据概况，然后把缺失值以及对应的数去掉。&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="1.png" src="/article_img/ChiSquare_2017_06_28_19_29/1.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h3&gt;第二步&lt;/h3&gt;
&lt;p&gt;绘制四格表，一般教学上是这样说的，但是SPSS支持N格表，并且支持非方阵形式的表格。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;取得自变量的分类以及因变量的分类。这时就相当于确定了这个N格表的行与列的内容了。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;columeKeys = list(set(X)) ##相当于groupby，确定输出的交叉表有几列
rowKeys = list(set(Y)) ##确定输出的交叉表有几行
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;计算自由度&lt;code&gt;df&lt;/code&gt;，&lt;code&gt;（row-1）*（column-1）&lt;/code&gt;,这玩意后面会作为各种方法的参数。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;df = (len(columeKeys)-1)*(len(rowKeys)-1)
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;根据统计出来列的种类按行去分拣数据，简单的理解就是按行列把表格画出来，统计每个格子对应的数据一共有几个。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;resualtMap = {}
for columeKey in columeKeys:
    indexsInX = [i for i,a in enumerate(X) if a==columeKey] ##查找每个columeKey在数据中的位置
    counts = {}
    for ix in indexsInX: ##统计每个columeKey在Y中有分别有几个rowKeys
        if counts.has_key(Y[ix]):
            counts[Y[ix]] += 1
        else:
            counts[Y[ix]] = 1
    resualtMap[columeKey] = counts
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;计算每行的统计结果总数，每列的统计结果总数。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;rowSum = {}
for rowKey in rowKeys:
    rowSum[rowKey] = len([i for i,a in enumerate(Y) if a==rowKey])
print &amp;#39;行统计的总数:&amp;#39;,rowSum

columeSum = {}
for columeKey in resualtMap.keys():
    columeSum[columeKey] = 0
    for rowkey in resualtMap[columeKey].keys():
        columeSum[columeKey] += resualtMap[columeKey][rowkey]
print &amp;#39;列统计的总数:&amp;#39;,columeSum
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接下来就可以绘制这张交叉表了。这里没做前端界面，所以以&lt;code&gt;print&lt;/code&gt;来假装自己在绘图。在绘图过程中，可以顺便把观察值和理论值（既期望）计算出来为后续分析过程做准备，免得后面再循环一遍。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;N = len(X)
observed = []
expected = []
for rowKey in rowKeys:
    for columeKey in resualtMap.keys():
        cell = resualtMap[columeKey].get(rowKey,0)##实际值
        observed.append(cell) ##顺便把每个实际值记入列队，用于后续计算
        rowRadio = float(cell)/rowSum[rowKey]##行占比
        columeRadio = float(cell)/columeSum[columeKey]##列占比
        expect = (float(columeSum[columeKey])/N)#该格对应的期望*rowSum[rowKey]
        expected.append(expect)##把每个期望值记入列队，用于后续计算

        print cell,&amp;quot;,&amp;quot;,&amp;quot;%.2f%%&amp;quot;%(rowRadio*100),&amp;quot;,&amp;quot;,&amp;quot;%.2f%%&amp;quot;%(columeRadio*100),&amp;quot;|&amp;quot;,
    print rowSum[rowKey],&amp;quot;,&amp;quot;,&amp;quot;100%&amp;quot;,&amp;quot;,&amp;quot;,&amp;quot;%.2f%%&amp;quot;%(float(rowSum[rowKey])/N*100)

for columeKey in resualtMap.keys():
    print columeSum[columeKey],&amp;quot;,&amp;quot;, &amp;quot;%.2f%%&amp;quot;%(float(columeSum[columeKey])/N*100),&amp;quot;,&amp;quot;,&amp;quot;100%&amp;quot;,&amp;quot;|&amp;quot;,
print N,&amp;quot;,&amp;quot;,&amp;quot;100%&amp;quot;,&amp;quot;,&amp;quot;,&amp;quot;100%&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="2.png" src="/article_img/ChiSquare_2017_06_28_19_29/2.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h3&gt;第三步&lt;/h3&gt;
&lt;p&gt;这步开始使用java方法，顺便验证第三方库&lt;a href="https://www.ee.ucl.ac.uk/~mflanaga/java/"&gt;flanagan&lt;/a&gt;，看看好不好使。&lt;/p&gt;
&lt;p&gt;选这个库的原因仅仅只是因为之前用它来做过一次正态性检验。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;拿着观察值和理论值去计算卡方值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.commons.math3.stat.inference.TestUtils&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;flanagan.analysis.Stat&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;java.lang.Math&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;double&lt;/span&gt; &lt;span class="n"&gt;cs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Stat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;chiSquareFreq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;observed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;计算卡方p值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;public static double getApproxGamma(double n) {
    // RECIP_E = (E^-1) = (1.0 / E)
    double RECIP_E = 0.36787944117144232159552377016147;
    // TWOPI = 2.0 * PI
    double TWOPI = 6.283185307179586476925286766559;
    double d = 1.0 / (10.0 * n);
    d = 1.0 / ((12* n) - d);
    d = (d + n) *RECIP_E;
    d = Math.pow(d,n);
    d *= Math.sqrt(TWOPI/ n);
    return d;
}

public static double log_igf(double s, double z) {
    if (z &amp;lt; 0.0) {
        return 0.0;
    }
    double sc = (Math.log(z) * s) - z - Math.log(s);
    double k = KM(s, z);
    return Math.log(k) + sc;
}

private static double KM(double s, double z) {
    double sum = 1.0;
    double nom = 1.0;
    double denom = 1.0;
    double log_nom = Math.log(nom);
    double log_denom = Math.log(denom);
    double log_s = Math.log(s);
    double log_z = Math.log(z);
    for (int i = 0; i &amp;lt; 1000; ++i) {
       log_nom += log_z;
       s++;
       log_s = Math.log(s);
       log_denom += log_s;
        double log_sum = log_nom - log_denom;
       sum += Math.exp(log_sum);
    }
    return sum;
}

private static double chisqr2pValue(int dof, double chi_squared) {
    if (chi_squared &amp;lt; 0 || dof &amp;lt; 1) {
        return 0.0;
    }
    double k = ((double) dof) * 0.5;
    double v = chi_squared * 0.5;
    if (dof == 2) {
        return Math.exp(-1.0 * v);
    }
    double incompleteGamma = log_igf(k,v);
    // 如果过小或者非数值或者无穷
    if (Math.exp(incompleteGamma) &amp;lt;= 1e-8
           || Double.isNaN(Math.exp(incompleteGamma))
           || Double.isInfinite(Math.exp(incompleteGamma))) {
        return 1e-14;
    }
    double gamma = Math.log(getApproxGamma(k));
   incompleteGamma -= gamma;
    if(Math.exp(incompleteGamma) &amp;gt; 1){
        return 1e-14;
    }
    double pValue = 1.0 - Math.exp(incompleteGamma);
    return (double) pValue;

chisqr2pValue(df, cs));//卡方p值（Asymp 2-sided）
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;取代观察值中的0值，以及对应的理论值，计算释然比，以及相对应的p值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;double likelihood = 0;
for(int i =0 ;i&amp;lt;20;i++){
    double tmp = observed2[i]*Math.log(observed2[i]/expected2[i]);
    likelihood += tmp;

}
likelihood = likelihood*2;//似然比 Likelihood Ratio
chisqr2pValue(df, likelihood);//似然比p值（Asymp 2-sided）
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这里吐槽一下，用JDK自带&lt;code&gt;math3.stat&lt;/code&gt;的似然比计算方法算出来结果居然不对。但我按它文档中函数注释的算法来计算，却得到正确的结果，神奇。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;使用自变量与因变量的原始值计算皮尔逊相关系数。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;double piersion= Stat.corrCoeff(X, Y);
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用皮尔逊相关系数计算线性关系LinearByLinearAssociation，以及对应的p值。此时自由度为1。(这理所当然嘛，是算自变量与因变量的关系，自由度自然是1。) &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;double linearByLinearAssociation= (N-1)*Math.pow(piersion,2);
//linear-By-Linear  Association 的p值（Asymp 2-sided）
chisqr2pValue(1, linearByLinearAssociation);
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="3.png" src="/article_img/ChiSquare_2017_06_28_19_29/3.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h3&gt;第四步（非必要）&lt;/h3&gt;
&lt;p&gt;接下来咱回到python&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;如果当&lt;strong&gt;交叉表为方阵&lt;/strong&gt;的时候，进行kappa检验。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;columeCount = len(columeKeys) ##取得行数，这里行列一样嘛
po = 0.0
for i in range(columeCount):
    po += float(observed[columeCount*i+i])  ##取得对角，相当于真阳性真阴性的observed的总和
po  =  po/N
pc = 0.0 ##真阳性的observed对应的  行总和 * 列总和
for i in range(columeCount):
    row = 0.0
    colume = 0.0
    for irow in range(columeCount):
        row+=float(observed[columeCount*i+irow])
    for icolume in range(columeCount):
        colume+=float(observed[i+icolume*columeCount])
    pc+=(row*colume)
pc = pc/(N**2)
k =  (po - pc)/(1-pc)
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;计算kappa值的标准误差。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ \sqrt{\frac{1}{(1-pc)^{2}}(Pe^{2}+Pe-\sum R_{i}C_{i}(R_{i}+C_{i})/N^{3})} $$&lt;/p&gt;
&lt;p&gt;pc 同1&lt;/p&gt;
&lt;p&gt;Ri，Ci 对应的行总和与列总和&lt;/p&gt;
&lt;p&gt;N 总样本数（去除无效值或缺失值）&lt;/p&gt;
&lt;p&gt;这里选用的计算方法如上,还有另一种计算方法，参照文章后1987的那篇引用。那篇论文是先去估计整体标准差，然后根据表格的不同大小，再进一步估计标准误差。感觉精度会更高一些，但这个估计过程太麻烦了，或者说按照论文中的做法，人的决定因数太大，暂时先不用。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;    ppp = 0.0
    for i in range(columeCount):
        row = 0.0
        colume = 0.0
        for irow in range(columeCount):
            row+=float(observed[columeCount*i+irow])
        for icolume in range(columeCount):
            colume+=float(observed[i+icolume*columeCount])
        ppp+=(row*colume)*(row+colume)
    E= 1/(N*(1-pc)**2)  * ( pc**2 + pc - ( ppp/math.pow(N,3) ) )
    SE = math.sqrt(E)
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;计算kappa的标准误差的p-value&lt;div class="highlight"&gt;&lt;pre&gt;def poSE(SE):
    SE_MAX = 6.0
    if (SE == 0.0) :
        x = 0.0
    else:
        y = 0.5 * math.fabs(SE)
        if y &amp;gt;= (SE_MAX * 0.5) :
            x = 1.0
        elif y &amp;lt; 1.0:
            w = y * y
            x = ((((((((0.000124818987 * w
                    - 0.001075204047) * w + 0.005198775019) * w
                    - 0.019198292004) * w + 0.059054035642) * w
                    - 0.151968751364) * w + 0.319152932694) * w
                    - 0.531923007300) * w + 0.797884560593) * y * 2.0
        else:
            y -= 2.0
            x = (((((((((((((-0.000045255659 * y
                    + 0.000152529290) * y - 0.000019538132) * y
                    - 0.000676904986) * y + 0.001390604284) * y
                    - 0.000794620820) * y - 0.002034254874) * y
                    + 0.006549791214) * y - 0.010557625006) * y
                    + 0.011630447319) * y - 0.009279453341) * y
                    + 0.005353579108) * y - 0.002141268741) * y
                    + 0.000535310849) * y + 0.999936657524
    if SE &amp;gt; 0.0:
        return (x + 1.0) * 0.5
    else:
        return (1.0 - x) * 0.5
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="4.png" src="/article_img/ChiSquare_2017_06_28_19_29/4.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;这里的图截图的时候，交叉表并不是方阵，所以kappa为0。&lt;/p&gt;
&lt;p&gt;不过这里的&lt;code&gt;gamma&lt;/code&gt; 和 &lt;code&gt;T&lt;/code&gt; 就没做，分析员说这些东西基本不看，所以就就没贴过来。&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://www.ee.ucl.ac.uk/~mflanaga/java/"&gt;Michael Thomas Flanagan's Java Scientific Library.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://influentialpoints.com/Training/standard_error_of_kappa.htm"&gt;Standard Error of Kappa.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.med.mcgill.ca/epidemiology/Hanley/Reprints/Standard_Error_1987.pdf"&gt;Standard Error of The Kappa Statistic（1987）.&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="algorithm"></category><category term="ML"></category><category term="Math"></category></entry><entry><title>Convolutional neural network</title><link href="/2015/Convolutional_neural_network_2015_10_22_23_18.html" rel="alternate"></link><updated>2015-10-22T23:18:00+08:00</updated><author><name>moonshile</name></author><id>tag:,2015-10-22:2015/Convolutional_neural_network_2015_10_22_23_18.html</id><summary type="html">&lt;h2&gt;卷积神经网络全面解析&lt;/h2&gt;
&lt;p&gt;最近仔细学习了一下卷积神经网络（CNN，Convolutional Neural Network），发现各处资料都不是很全面，经过艰苦努力终于弄清楚了。为了以后备查，以及传播知识，决定记录下来。本文将极力避免废话，重点聚焦在推导过程上，为打算从零开始的孩纸说清楚“为什么”。&lt;/p&gt;
&lt;p&gt;另外，因本人才疏学浅（是真的才疏学浅，不是谦虚），肯定会有很多谬误，欢迎大家指出！&lt;/p&gt;
&lt;h2&gt;卷积神经网络（CNN）概述&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;由来：神经元网络的直接升级版&lt;/li&gt;
&lt;li&gt;相关：Yann LeCun和他的LeNet&lt;/li&gt;
&lt;li&gt;影响：在图像、语音领域不断突破，复兴了神经元网络并进入“深度学习”时代&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;卷积神经网络沿用了普通的神经元网络即多层感知器的结构，是一个&lt;strong&gt;前馈网络&lt;/strong&gt;。以应用于图像领域的CNN为例，大体结构如图1。&lt;/p&gt;
&lt;p&gt;&lt;img alt="cnn_structure.png" src="/article_img/Convolutional_neural_network_2015_10_22_23_18/cnn_structure.png" /&gt;&lt;/p&gt;
&lt;p&gt;很明显，这个典型的结构分为四个大层次&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入图像I。为了减小复杂度，一般使用灰度图像。当然，也可以使用RGB彩色图像，此时输入图像有三张，分别为RGB分量。输入图像一般需要归一化，如果使用sigmoid激活函数，则归一化到[0, 1]，如果使用tanh激活函数，则归一化到[-1, 1]。&lt;/li&gt;
&lt;li&gt;多个卷积（C）-下采样（S）层。将上一层的输出与本层权重W做卷积得到各个C层，然后下采样得到各个S层。怎么做以及为什么，下面会具体分析。这些层的输出称为Feature Map。&lt;/li&gt;
&lt;li&gt;光栅化（X）。是为了与传统的多层感知器全连接。即将上一层的所有Feature Map的每个像素依次展开，排成一列。&lt;/li&gt;
&lt;li&gt;传统的多层感知器（N&amp;amp;O）。最后的分类器一般使用Softmax，如果是二分类，当然也可以使用LR。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;接下来，就开始深入探索这个结构吧！&lt;/p&gt;
&lt;h2&gt;从多层感知器（MLP）说起&lt;/h2&gt;
&lt;p&gt;卷积神经网络来源于普通的神经元网络。要了解个中渊源，就要先了解神经元网络的机制以及缺点。典型的神经元网络就是多层感知器。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;摘要：本节主要内容为多层感知器（MLP，Multi-Layer Perceptron）的原理、权重更新公式的推导。熟悉这一部分的童鞋可以直接跳过了~但是，一定一定要注意，&lt;strong&gt;本节难度比较大&lt;/strong&gt;，所以不熟悉的童鞋一定一定要认真看看！如果对推导过程没兴趣，可直接在本节最后看结论。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;感知器&lt;/h3&gt;
&lt;p&gt;感知器（Perceptron）是建立模型&lt;/p&gt;
&lt;p&gt;$$ f(x) = act(\theta^Tx + b) $$&lt;/p&gt;
&lt;p&gt;其中激活函数 &lt;em&gt;act&lt;/em&gt; 可以使用{&lt;em&gt;sign, sigmoid, tanh&lt;/em&gt;}之一。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;激活函数使用符号函数 &lt;em&gt;sign&lt;/em&gt; ，可求解损失函数最小化问题，通过梯度下降确定参数&lt;/li&gt;
&lt;li&gt;激活函数使用 &lt;em&gt;sigmoid&lt;/em&gt; （或者 &lt;em&gt;tanh&lt;/em&gt; ），则分类器事实上成为Logistic Regression（个人理解，请指正），可通过梯度上升极大化似然函数，或者梯度下降极小化损失函数，来确定参数&lt;/li&gt;
&lt;li&gt;如果需要多分类，则事实上成为Softmax Regression&lt;/li&gt;
&lt;li&gt;如要需要分离超平面恰好位于正例和负例的正中央，则成为支持向量机（SVM）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;感知器比较简单，资料也比较多，就不再详述。&lt;/p&gt;
&lt;h3&gt;多层感知器&lt;/h3&gt;
&lt;p&gt;感知器存在的问题是，对线性可分数据工作良好，如果设定迭代次数上限，则也能一定程度上处理近似线性可分数据。但是对于非线性可分的数据，比如最简单的异或问题，感知器就无能为力了。这时候就需要引入多层感知器这个大杀器。&lt;/p&gt;
&lt;p&gt;多层感知器的思路是，尽管原始数据是非线性可分的，但是可以通过某种方法将其映射到一个线性可分的高维空间中，从而使用线性分类器完成分类。图1中，从X到O这几层，正展示了多层感知器的一个典型结构，即输入层-隐层-输出层。&lt;/p&gt;
&lt;h4&gt;输入层-隐层&lt;/h4&gt;
&lt;p&gt;是一个全连接的网络，即每个输入节点都连接到所有的隐层节点上。更详细地说，可以把输入层视为一个向量 $ x $ ，而隐层节点 $j$ 有一个权值向量 $ \theta_j $ 以及偏置 $ b_j $ ，激活函数使用 &lt;em&gt;sigmoid&lt;/em&gt; 或 &lt;em&gt;tanh&lt;/em&gt; ，那么这个隐层节点的输出应该是&lt;/p&gt;
&lt;p&gt;$$ f_j(x) = act(\theta_j^Tx + b_j) $$&lt;/p&gt;
&lt;p&gt;也就是每个隐层节点都相当于一个感知器。每个隐层节点产生一个输出，那么隐层所有节点的输出就成为一个向量，即&lt;/p&gt;
&lt;p&gt;$$ f(x) =  act({\Theta}x + b) $$&lt;/p&gt;
&lt;p&gt;若输入层有 $ m $ 个节点，隐层有 $ n $ 个节点，那么 $ \Theta = [\theta^T] $ 为 $ n×m $ 的矩阵， $ x $  为长为$ m $ 的向量，$ b $ 为长为 $ n $ 的向量，激活函数作用在向量的每个分量上， $ f(x) $ 返回一个向量。&lt;/p&gt;
&lt;h4&gt;隐层-输出层&lt;/h4&gt;
&lt;p&gt;可以视为级联在隐层上的一个感知器。若为二分类，则常用Logistic Regression；若为多分类，则常用Softmax Regression。&lt;/p&gt;
&lt;h3&gt;Back Propagation&lt;/h3&gt;
&lt;p&gt;搞清楚了模型的结构，接下来就需要通过某种方法来估计参数了。对于一般的问题，可以通过求解损失函数极小化问题来进行参数估计。但是对于多层感知器中的隐层，因为无法直接得到其输出值，当然不能够直接使用到其损失了。这时，就需要将损失从顶层反向传播（Back Propagate）到隐层，来完成参数估计的目标。&lt;/p&gt;
&lt;p&gt;首先，约定标量为普通小写字母，向量为加粗小写字母，矩阵为加粗大写字母；再约定以下记号：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入样本为$ \mathbf x $ ，其标签为 $ \mathbf t $&lt;/li&gt;
&lt;li&gt;对某个层$ Q $ ，其输出为 $ \mathbf o_Q $ ，其第 $ j $ 个节点的输出为 $ o_Q^{(j)} $ ，其每个节点的输入均为上一层 $ P $ 的输出 $ \mathbf o_P $ ；层 $ Q $ 的权重为矩阵 $ \mathbf \Theta_Q $ ，连接层 $ P $ 的第 $ i $ 个节点与层 $ Q $ 的第 $ j $ 个节点的权重为 $ \theta_Q^{(ji)} $ &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;现在可以定义损失函数&lt;/p&gt;
&lt;p&gt;$$\left \lbrace \begin {aligned} E &amp;amp; = \frac {1} {2} \sum_{y \in Y}(t^{(y)} - o_Y^{(y)})^2 \\ o_Q^{(j)} &amp;amp; = \phi(n_Q^{(j)}) \\ n_Q^{(j)} &amp;amp; = \sum_{i \in P} \theta_Q^{(ji)} o_P^{(i)} + b_Q^{(j)} \end {aligned} \right. $$&lt;/p&gt;
&lt;p&gt;其中， $ \phi $ 为激活函数。我们依旧通过极小化损失函数的方法，尝试进行推导。则&lt;/p&gt;
&lt;p&gt;$$ \left \lbrace \begin {aligned} \frac {\partial E} {\partial \theta_Q^{(ji)}} &amp;amp; = \frac {\partial E} {\partial o_Q^{(j)}} \frac {\partial o_Q^{(j)}} {\partial n_Q^{(j)}} \frac {\partial n_Q^{(j)}} {\partial \theta_Q^{(ji)}} \\ \frac {\partial E} {\partial b_Q^{(j)}} &amp;amp; = \frac {\partial E} {\partial o_Q^{(j)}} \frac {\partial o_Q^{(j)}} {\partial n_Q^{(j)}} \frac {\partial n_Q^{(j)}} {\partial b_Q^{(j)}} \end{aligned}\right. $$&lt;/p&gt;
&lt;p&gt;上边两个式子的等号右边部有三个导数比较容易确定&lt;/p&gt;
&lt;p&gt;$$ \left \lbrace \begin {aligned} \frac {\partial o_Q^{(j)}} {\partial n_Q^{(j)}} &amp;amp; = \phi'(n_Q^{(j)}) \\ \frac {\partial n_Q^{(j)}} {\partial \theta_Q^{(ji)}} &amp;amp; = o_P^{(i)} \\ \frac {\partial n_Q^{(j)}} {\partial b_Q^{(j)}} &amp;amp; = 1 \end {aligned} \right. $$&lt;/p&gt;
&lt;p&gt;然后再看剩下的比较复杂的一个偏导数。考虑层 $ Q $ 的下一层 $ R $ ，其节点 $ k $ 的输入为层 $ Q $ 中每个节点的输出，也就是为 $ o_Q^{(j)} $ 的函数，考虑逆函数，可视 $ o_Q^{(j)} $ 为 $ o_R^{(k)} $ 的函数，也为 $ n_R^{(k)} $ 的函数。则对每个隐层&lt;/p&gt;
&lt;p&gt;$$ \begin {aligned} \frac {\partial E} {\partial o_Q^{(j)}} &amp;amp; = \frac {\partial E(n_R^{(1)}, n_R^{(2)}, ..., n_R^{(k)}, ..., n_R^{(K)})} {\partial o_Q^{(j)}} \\ &amp;amp; = \sum_{k \in R} \frac {\partial E} {\partial n_R^{(k)}} \frac {\partial n_R^{(k)}} {\partial o_Q^{(j)}} \\ &amp;amp; = \sum_{k \in R} \frac {\partial E} {\partial o_R^{(k)}} \frac {\partial o_R^{(k)}} {\partial n_R^{(k)}} \frac {\partial n_R^{(k)}} {\partial o_Q^{(j)}} \\ &amp;amp; = \sum_{k \in R} \frac {\partial E} {\partial o_R^{(k)}} \frac {\partial o_R^{(k)}} {\partial n_R^{(k)}} \theta_R^{(kj)} \end {aligned} $$&lt;/p&gt;
&lt;p&gt;令 $ \delta_Q^{(j)} = \frac {\partial E} {\partial o_Q^{(j)}} \frac {\partial o_Q^{(j)}} {\partial n_Q^{(j)}} $&lt;/p&gt;
&lt;p&gt;则对每个隐层&lt;/p&gt;
&lt;p&gt;$$ \frac {\partial E} {\partial o_Q^{(j)}} = \sum_{k \in R} \frac {\partial E} {\partial o_R^{(k)}} \frac {\partial o_R^{(k)}} {\partial n_R^{(k)}} \theta_R^{(kj)} = \sum_{k \in R} \delta_R^{(k)} \theta_R^{(kj)} $$&lt;/p&gt;
&lt;p&gt;考虑到输出层，有&lt;/p&gt;
&lt;p&gt;$$ \frac {\partial E} {\partial o_Q^{(j)}} = \left \lbrace \begin {aligned} \sum_{k \in R} \delta_R^{(k)} \theta_R^{(kj)}, &amp;amp; \qquad k\ has\ input\ node\ j \\ o_Y^{(j)} - t^{(j)}, &amp;amp; \qquad j\ is\ an\ output\ node,\ i.e.\ Q = Y \end {aligned} \right . $$&lt;/p&gt;
&lt;p&gt;故有&lt;/p&gt;
&lt;p&gt;$$ \delta_Q^{(j)} = \frac {\partial E} {\partial o_Q^{(j)}} \frac {\partial o_Q^{(j)}} {\partial n_Q^{(j)}} = \frac {\partial E} {\partial o_Q^{(j)}} \phi'(n_Q^{(j)}) = \left \lbrace \begin {aligned} (\sum_{k \in R} \delta_R^{(k)} \theta_R^{(kj)}) \phi'(n_Q^{(j)}), &amp;amp; \qquad k\ has\ input\ node\ j \\ (o_Y^{(j)} - t^{(j)}) \phi'(n_Y^{(j)}), &amp;amp; \qquad j\ is\ an\ output\ node,\ i.e.\ Q = Y \end {aligned} \right. $$&lt;/p&gt;
&lt;p&gt;综合以上各式，有梯度结果&lt;/p&gt;
&lt;p&gt;$$ \begin {aligned} \frac {\partial E} {\partial \theta_Q^{(ji)}} &amp;amp; = \frac {\partial E} {\partial o_Q^{(j)}} \frac {\partial o_Q^{(j)}} {\partial n_Q^{(j)}} \frac {\partial n_Q^{(j)}} {\partial \theta_Q^{(ji)}} = \delta_Q^{(j)} o_P^{(i)} \\ \frac {\partial E} {\partial b_Q^{(j)}} &amp;amp; = \frac {\partial E} {\partial o_Q^{(j)}} \frac {\partial o_Q^{(j)}} {\partial n_Q^{(j)}} \frac {\partial n_Q^{(j)}} {\partial b_Q^{(j)}} = \delta_Q^{(j)} \end {aligned} $$&lt;/p&gt;
&lt;p&gt;本来到这里应该就结束了，不过同正向的时候一样，为了计算方便，我们依然希望能够以矩阵或者向量的方式来表达。&lt;strong&gt;结论在这里&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;假设有层 $ P, Q, R $ ，分别有 $ l, m, n $ 个节点，依序前者输出全连接到后者作为输入。层 $ Q $ 有权重矩阵
$ [\mathbf \Theta_Q]&lt;em m_1="m×1"&gt;{m×l} $ ，偏置向量 $ [\mathbf b_Q]&lt;/em&gt; $ ，层 $ R $ 有权重矩阵 $ [\mathbf \Theta_R]&lt;em n_1="n×1"&gt;{n×m} $ ，偏置向量 $ [\mathbf b_R]&lt;/em&gt; $ 。
那么&lt;/p&gt;
&lt;p&gt;$$ \begin {aligned} \frac {\partial E} {\partial \mathbf \Theta_Q} &amp;amp; = \mathbf δ_Q \mathbf o_P^T \\ \frac {\partial E} {\partial \mathbf b_Q} &amp;amp; = \mathbf δ_Q \\ \mathbf δ_Q &amp;amp; = \left \lbrace \begin {aligned} (\mathbf \Theta_R^T \mathbf δ_R) \circ \phi'(\mathbf n_Q), &amp;amp; \qquad Q\ is\ a\ hidden\ layer \\ (\mathbf o_Y - \mathbf t) \circ \phi'(\mathbf n_Y), &amp;amp; \qquad Q = Y\ is\ the\ output\ layer \end {aligned} \right. \end {aligned} $$&lt;/p&gt;
&lt;p&gt;其中，运算 $ w = u \circ v  $ 表示  $ w_i = u_i v_i $  。函数作用在向量或者矩阵上，表示作用在其每个分量上。&lt;/p&gt;
&lt;p&gt;最后，补充几个常用的激活函数的导数结果，推导很简单，从略。&lt;/p&gt;
&lt;p&gt;$$ \begin {aligned} \phi'(x) &amp;amp; = sigmoid'(x) = sigmoid(x)(1 - sigmoid(x)) = \mathbf o_Q(1 - \mathbf o_Q)\\ \phi'(x) &amp;amp; = tanh'(x) = 1 - tanh^2(x) = 1 - \mathbf o_Q^2\\ \phi'(x) &amp;amp; = softmax'(x) = softmax(x) - softmax^2(x) = \mathbf o_Q - \mathbf o_Q^2 \end{aligned} $$&lt;/p&gt;
&lt;h3&gt;存在的问题&lt;/h3&gt;
&lt;p&gt;多层感知器存在的最大的问题就是，它是一个全连接的网络，因此在输入比较大的时候，权值会特别多。比如一个有1000个节点的隐层，连接到一个1000×1000的图像上，那么就需要 10^9 个权值参数（外加1000个偏置参数）！这个问题，一方面限制了每层能够容纳的最大神经元数目，另一方面也限制了多层感知器的层数即深度。&lt;/p&gt;
&lt;p&gt;多层感知器的另一个问题是梯度发散。 &lt;del&gt;（&lt;span style="color:red"&gt;这个问题的具体原因还没有完全弄清楚，求指教！&lt;/span&gt;）&lt;/del&gt; 一般情况下，我们需要把输入归一化，而每个神经元的输出在激活函数的作用下也是归一化的；另外，有效的参数其绝对值也一般是小于1的；这样，在BP过程中，多个小于1的数连乘，得到的会是更小的值。也就是说，在深度增加的情况下，从后传播到前边的残差会越来越小，甚至对更新权值起不到帮助，从而失去训练效果，使得前边层的参数趋于随机化（补充一下，其实随机参数也是能一定程度上捕捉到图像边缘的）。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;感谢&lt;a href="http://www.shwley.com/"&gt;shwley&lt;/a&gt;提供的帮助~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;因为这些问题，神经元网络在很长一段时间内都被冷落了。&lt;/p&gt;
&lt;h2&gt;从MLP到CNN&lt;/h2&gt;
&lt;p&gt;卷积神经网络的名字怪吓人，实际理解起来也挺吓人的。哈哈，其实只要看明白了多层感知器的推导过程，理解卷积神经网络就差不多可以信手拈来了。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;摘要：首先解释卷积神经网络为什么会“长”成现在这般模样。然后详细推导了卷积神经网络的预测过程和参数估计方法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;CNN的前世今生&lt;/h3&gt;
&lt;p&gt;既然多层感知器存在问题，那么卷积神经网络的出现，就是为了解决它的问题。卷积神经网络的核心出发点有三个。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;局部感受野&lt;/strong&gt;。形象地说，就是模仿你的眼睛，想想看，你在看东西的时候，目光是聚焦在一个相对很小的局部的吧？严格一些说，普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上，而在卷积神经网络中，每个隐层节点只连接到图像某个足够小局部的像素点上，从而大大减少需要训练的权值参数。举个栗子，依旧是1000×1000的图像，使用10×10的感受野，那么每个神经元只需要100个权值参数；不幸的是，由于需要将输入图像扫描一遍，共需要991×991个神经元！参数数目减少了一个数量级，不过还是太多。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;权值共享&lt;/strong&gt;。形象地说，就如同你的某个神经中枢中的神经细胞，它们的结构、功能是相同的，甚至是可以互相替代的。也就是，在卷积神经网中，同一个卷积核内，所有的神经元的权值是相同的，从而大大减少需要训练的参数。继续上一个栗子，虽然需要991×991个神经元，但是它们的权值是共享的呀，所以还是只需要100个权值参数，以及1个偏置参数。从MLP的 10^9 到这里的100，就是这么狠！作为补充，在CNN中的每个隐藏，一般会有多个卷积核。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;池化&lt;/strong&gt;。形象地说，你先随便看向远方，然后闭上眼睛，你仍然记得看到了些什么，但是你能完全回忆起你刚刚看到的每一个细节吗？同样，在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。以最大池化（Max Pooling）为例，1000×1000的图像经过10×10的卷积核卷积后，得到的是991×991的特征图，然后使用2×2的池化规模，即每4个点组成的小方块中，取最大的一个作为输出，最终得到的是496×496大小的特征图。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;现在来看，需要训练参数过多的问题已经完美解决。 &lt;del&gt;而梯度发散的问题，因为还不清楚具体缘由，依然留待讨论。&lt;/del&gt; 关于梯度发散，因为多个神经元共享权值，因此它们也会对同一个权值进行修正，积少成多，积少成多，积少成多，从而一定程度上解决梯度发散的问题！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下面我们来揭开卷积神经网络中“卷积”一词的神秘面纱。&lt;/p&gt;
&lt;h3&gt;CNN的预测过程&lt;/h3&gt;
&lt;p&gt;回到开头的图1，卷积神经网络的预测过程主要有四种操作：卷积、下采样、光栅化、多层感知器预测。&lt;/p&gt;
&lt;h4&gt;卷积&lt;/h4&gt;
&lt;p&gt;先抛开卷积这个概念不管。为简便起见，考虑一个大小为5×5的图像，和一个3×3的卷积核。这里的卷积核共有9个参数，就记为 $ \Theta = [\theta_{ij}]_{3×3} $ 吧。这种情况下，卷积核实际上有9个神经元，他们的输出又组成一个3×3的矩阵，称为特征图。第一个神经元连接到图像的第一个3×3的局部，第二个神经元则连接到第二个局部（注意，有重叠！就跟你的目光扫视时也是连续扫视一样）。具体如图2所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="cnn_conv.png" src="/article_img/Convolutional_neural_network_2015_10_22_23_18/cnn_conv.png" /&gt;&lt;/p&gt;
&lt;p&gt;图2的上方是第一个神经元的输出，下方是第二个神经元的输出。每个神经元的运算依旧是&lt;/p&gt;
&lt;p&gt;$$ f(x) = act(\sum_{i, j}^n \theta_{(n - i)(n - j)} x_{ij} + b) $$&lt;/p&gt;
&lt;p&gt;需要注意的是，平时我们在运算时，习惯使用 $ \theta_{ij}x_{ij} $ 这种写法，但事实上，我们这里使用的是 $ \theta_{(n - i)(n - j)}x_{ij} $  ，原因马上揭晓。&lt;/p&gt;
&lt;p&gt;现在我们回忆一下离散卷积运算。假设有二维离散函数 $ f(x, y), g(x, y) $ ， 那么它们的卷积定义为&lt;/p&gt;
&lt;p&gt;$$ f(m, n)*g(m, n) = \sum_u^\infty \sum_v^\infty {f(u, v)g(m - u, n - v)} $$&lt;/p&gt;
&lt;p&gt;现在发现了吧！上面例子中的9个神经元均完成输出后，实际上等价于图像和卷积核的卷积操作！这就是“卷积神经网络”名称的由来，也是为什么在神经元运算时使用  $ \theta_{(n - i)(n - j)}x_{ij} $ 。&lt;/p&gt;
&lt;p&gt;如果你足够细心，就会发现其实上述例子中的运算并不完全符合二维卷积的定义。实际上，我们需要用到的卷积操作有两种模式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;valid模式，用 $ *_v $ 表示。即上边例子中的运算。在这种模式下，卷积只发生在被卷积的函数的定义域“内部”。一个 $ m×n $ 的矩阵被一个 $ p×q $ 的矩阵卷积（$ m \ge p, n \ge q $ ），得到的是一个 $ (m - p + 1)×(n - q + 1) $ 的矩阵。&lt;/li&gt;
&lt;li&gt;full模式，用 $ *_f $ 表示。这种模式才是上边二维卷积的定义。一个 $ m×n $ 的矩阵被一个 $ p×q $ 的矩阵卷积，得到的是一个 $ (m + p - 1)×(n + q - 1) $ 的矩阵。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;现在总结一下卷积过程。如果卷积层 $ c $ 中的一个“神经中枢” $ j $  连接到特征图 $ \mathbf X_1, \mathbf X_2, ..., \mathbf X_i $ ，且这个卷积核的权重矩阵为 $ \mathbf \Theta_j $ ，那么这个神经中枢的输出为&lt;/p&gt;
&lt;p&gt;$$ \mathbf O_j = \phi (\sum_i \mathbf X_i *_v \mathbf \Theta_j + b_j) $$&lt;/p&gt;
&lt;h4&gt;下采样&lt;/h4&gt;
&lt;p&gt;下采样，即池化，目的是减小特征图，池化规模一般为2×2。常用的池化方法有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最大池化（Max Pooling）。取4个点的最大值。这是最常用的池化方法。&lt;/li&gt;
&lt;li&gt;均值池化（Mean Pooling）。取4个点的均值。&lt;/li&gt;
&lt;li&gt;高斯池化。借鉴高斯模糊的方法。不常用。具体过程不是很清楚。。。&lt;/li&gt;
&lt;li&gt;可训练池化。训练函数 $ f $ ，接受4个点为输入，出入1个点。不常用。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于特征图的变长不一定是2的倍数，所以在边缘处理上也有两种方案：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;忽略边缘。即将多出来的边缘直接省去。&lt;/li&gt;
&lt;li&gt;保留边缘。即将特征图的变长用0填充为2的倍数，然后再池化。一般使用这种方式。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对神经中枢 $ j $ 的输出 $ O_j $ ，使用池化函数 &lt;em&gt;downsample&lt;/em&gt; ，池化后的结果为&lt;/p&gt;
&lt;p&gt;$$ \mathbf S_j = downsample(\mathbf O_j) $$&lt;/p&gt;
&lt;h4&gt;光栅化&lt;/h4&gt;
&lt;p&gt;图像经过池化-下采样后，得到的是一系列的特征图，而多层感知器接受的输入是一个向量。因此需要将这些特征图中的像素依次取出，排列成一个向量。具体说，对特征图 $ \mathbf X_1, \mathbf X_2, ..., \mathbf X_j $ ，光栅化后得到的向量&lt;/p&gt;
&lt;p&gt;$$ \mathbf o_k = [x_{111}, x_{112}, ..., x_{11n}, x_{121}, x_{122}, ..., x_{12n}, ..., x_{1mn}, ..., x_{2mn}, ..., x_{jmn}]^T $$&lt;/p&gt;
&lt;h4&gt;多层感知器预测&lt;/h4&gt;
&lt;p&gt;将光栅化后的向量连接到多层感知器即可。&lt;/p&gt;
&lt;h3&gt;CNN的参数估计&lt;/h3&gt;
&lt;p&gt;卷积神经网络的参数估计依旧使用Back Propagation的方法，不过需要针对卷积神经网络的特点进行一些修改。我们从高层到底层，逐层进行分析。&lt;/p&gt;
&lt;h4&gt;多层感知器层&lt;/h4&gt;
&lt;p&gt;使用多层感知器的参数估计方法，得到其最低的一个隐层 $ S $ 的残差向量 $ \mathbf δ_s $ 。现在需要将这个残差传播到光栅化层$ R $ ，光栅化的时候并没有对向量的值做修改，因此其激活函数为恒等函数，其导数为单位向量。&lt;/p&gt;
&lt;p&gt;$$ \mathbf δ_R =(\mathbf \Theta_S^T \mathbf δ_S) \circ \phi'(\mathbf n_R) = \mathbf \Theta_S^T \mathbf δ_S $$&lt;/p&gt;
&lt;h4&gt;光栅化层&lt;/h4&gt;
&lt;p&gt;从上一层传过来的残差为&lt;/p&gt;
&lt;p&gt;$$ \mathbf δ_R = [\delta_{111}, \delta_{112}, ..., \delta_{11n}, \delta_{121}, \delta_{122}, ..., \delta_{12n}, ..., \delta_{1mn}, ..., \delta_{2mn}, ..., \delta_{jmn}]^T $$
重新整理成为一系列的矩阵即可，若上一层 $ Q $ 有 $ q $ 个池化核，则传播到池化层的残差&lt;/p&gt;
&lt;p&gt;$$ \Delta_Q = {\mathbf \Delta_1, \mathbf \Delta_2, ..., \mathbf \Delta_q} $$&lt;/p&gt;
&lt;h4&gt;池化层&lt;/h4&gt;
&lt;p&gt;对应池化过程中常用的两种池化方案，这里反传残差的时候也有两种上采样方案：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最大池化：将1个点的残差直接拷贝到4个点上。&lt;/li&gt;
&lt;li&gt;均值池化：将1个点的残差平均到4个点上。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;即传播到卷积层的残差&lt;/p&gt;
&lt;p&gt;$$ \mathbf \Delta_p = upsample(\mathbf \Delta_q) $$&lt;/p&gt;
&lt;h4&gt;卷积层&lt;/h4&gt;
&lt;p&gt;卷积层有参数，所以卷积层的反传过程有两个任务，一是更新权值，另一是反传残差。先看更新权值，即梯度的推导。&lt;/p&gt;
&lt;p&gt;&lt;img alt="cnn_update.png" src="/article_img/Convolutional_neural_network_2015_10_22_23_18/cnn_update.png" /&gt;&lt;/p&gt;
&lt;p&gt;如图三上方，先考虑卷积层的某个“神经中枢”中的第一个神经元。根据多层感知器的梯度公式&lt;/p&gt;
&lt;p&gt;$$ \frac {\partial E} {\partial \theta_{ji}} = \delta_j o_i $$&lt;/p&gt;
&lt;p&gt;那么在图三上方的例子中，有&lt;/p&gt;
&lt;p&gt;$$ \frac {\partial E} {\partial \theta_{11}} = \delta_{11} o_{22} \qquad \frac {\partial E} {\partial \theta_{12}} = \delta_{11} o_{21} \qquad \frac {\partial E} {\partial \theta_{21}} = \delta_{11} o_{12} \qquad \frac {\partial E} {\partial \theta_{22}} = \delta_{11} o_{11} $$&lt;/p&gt;
&lt;p&gt;考虑到其他的神经元，每次更新的都是这四个权值，因此实际上等价于一次更新这些偏导数的和。如果&lt;strong&gt;仅考虑对 $ \theta_{11} $ 的偏导数&lt;/strong&gt;，不难发现如图3下方所示，其值应该来自于淡蓝色和灰色区域。是不是似曾相识？是的，又是卷积！但是又有两处重要的不同：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在计算对 $ \theta_{11} $ 的偏导数时，淡蓝色区域和灰色区域的对应位置做运算，但是在卷积运算中，这些位置应该是旋转过来的！&lt;/li&gt;
&lt;li&gt;$ \Theta $ 矩阵的左上角，而淡蓝色区域在右下角，同样是旋转过的！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，对卷积层 $ P $ 中的某个“神经中枢” $ p $， 权值（以及偏置，不再具体推导）更新公式应该是&lt;/p&gt;
&lt;p&gt;$$ \begin {aligned} \frac {\partial E} {\partial \mathbf \Theta_p} &amp;amp; = rot180((\sum_{q'} \mathbf O_{q'}) *_v rot180(\mathbf \Delta_p)) \\ \frac {\partial E} {\partial b_p} &amp;amp; = \sum_{u, v} (\delta_p)_{uv} \end {aligned} $$&lt;/p&gt;
&lt;p&gt;其中，$ rot180 $ 是将一个矩阵旋转180度； $ O_{q'} $ 是连接到该“神经中枢”前的池化层的输出；对偏置的梯度即 $ \Delta_p $ 所有元素之和。&lt;/p&gt;
&lt;p&gt;下面讨论残差反传的问题。&lt;/p&gt;
&lt;p&gt;&lt;img alt="cnn_delta.png" src="/article_img/Convolutional_neural_network_2015_10_22_23_18/cnn_delta.png" /&gt;&lt;/p&gt;
&lt;p&gt;如图4，&lt;strong&gt;考虑淡蓝色像素点影响到的神经元&lt;/strong&gt;，在这个例子中，受影响的神经元有4个，他们分别以某个权值与淡蓝色像素运算后影响到对应位置的输出。再结合多层感知器的残差传播公式，不难发现这里又是一个卷积过程！同样需要注意的是，正如图4中的数字标号，这里的卷积是旋转过的；另外，这里用的卷积模式是full。&lt;/p&gt;
&lt;p&gt;如果前边的池化层 $ Q' $ 的某个特征图 $ q'$ 连接到这个卷积层 $ P $ 中的某“神经中枢”集合 $ C$ ，那么传播到$ q' $ 的残差为&lt;/p&gt;
&lt;p&gt;$$ \mathbf \Delta_{q'} = (\sum_{p \in C} \mathbf \Delta_p *_f rot180(\mathbf \Theta_p)) \circ \phi'(\mathbf O_{q'}) $$&lt;/p&gt;
&lt;h2&gt;最后一公里：Softmax&lt;/h2&gt;
&lt;p&gt;前边我有意忽略了对Softmax的讨论，在这里补上。因为Softmax的资料已经非常多了，所以这里不再详细讨论。具体可以参考&lt;a href="http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92"&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;需要补充说明的是，不难发现，Softmax的梯度公式与多层感知器的BP过程是兼容的；另外，实现Softmax的时候，如果需要分为 $ k $ 个类，同样也可以设置 $ k $ 个输出节点，这相当于隐含了一个类别名称为“其他”的类。&lt;/p&gt;
&lt;h2&gt;CNN的实现&lt;/h2&gt;
&lt;h3&gt;思路&lt;/h3&gt;
&lt;p&gt;以层为单位，分别实现卷积层、池化层、光栅化层、MLP隐层、Softmax层这五个层的类。其中每个类都有output和backpropagate这两个方法。&lt;/p&gt;
&lt;p&gt;另外，还需要一系列的辅助方法，包括：conv2d（二维离散卷积，valid和full模式），downsample（池化中需要的下采样，两种边界模式），upsample（池化中的上采样），以及dsigmoid和dtanh等。&lt;/p&gt;
&lt;h3&gt;其他&lt;/h3&gt;
&lt;p&gt;还需要考虑的是可扩展性和性能优化，这些以后再谈~&lt;/p&gt;</summary><category term="CNN"></category><category term="ML"></category></entry><entry><title>数据预处理技巧</title><link href="/2014/Data_Process_2014_08_18_20_05.html" rel="alternate"></link><updated>2014-08-18T20:05:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2014-08-18:2014/Data_Process_2014_08_18_20_05.html</id><summary type="html">&lt;p&gt;参考资料：
&lt;a href="http://ufldl.stanford.edu/wiki/index.php/Data_Preprocessing"&gt;Data Preprocessing&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;开头！&lt;/h2&gt;
&lt;p&gt;一般来说，算法的好坏一定程度上和数据是否归一化，是否白化有关。但是在具体问题中，这些数据预处理中的参数其实还是很难准确得到的，当然了，除非你对对应的算法有非常的深刻的理解。下面就从归一化和白化两个角度来介绍下数据预处理的相关技术。&lt;/p&gt;
&lt;h3&gt;数据归一化&lt;/h3&gt;
&lt;p&gt;数据的归一化一般包括样本尺度归一化，逐样本的均值相减，特征的标准化这3个。其中数据尺度归一化的原因是：数据中每个维度表示的意义不同，所以有可能导致该维度的变化范围不同，因此有必要将他们都归一化到一个固定的范围，一般情况下是归一化到[0 1]或者[-1 1]。这种数据归一化还有一个好处是对后续的一些默认参数（比如白化操作）不需要重新过大的更改。&lt;/p&gt;
&lt;p&gt;逐样本的均值相减主要应用在那些具有稳定性的数据集中，也就是那些数据的每个维度间的统计性质是一样的。比如说，在自然图片中，这样就可以减小图片中亮度对数据的影响，因为我们一般很少用到亮度这个信息。不过逐样本的均值相减这只适用于一般的灰度图，在rgb等色彩图中，由于不同通道不具备统计性质相同性所以基本不会常用。&lt;/p&gt;
&lt;p&gt;特征标准化是指对数据的每一维进行均值化和方差相等化。这在很多机器学习的算法中都非常重要，比如SVM等。&lt;/p&gt;
&lt;h3&gt;数据白化&lt;/h3&gt;
&lt;p&gt;数据的白化是在数据归一化之后进行的。实践证明，很多deep learning算法性能提高都要依赖于数据的白化。在对数据进行白化前要求先对数据进行特征零均值化，不过一般只要 我们做了特征标准化，那么这个条件必须就满足了。在数据白化过程中，最主要的还是参数epsilon的选择，因为这个参数的选择对deep learning的结果起着至关重要的作用。&lt;/p&gt;
&lt;h4&gt;简单缩放&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;在简单缩放中，我们的目的是通过对数据的每一个维度的值进行重新调节（这些维度可能是相互独立的），使得最终的数据向量落在 [0,1]或[ − 1,1] 的区间内（根据数据情况而定）。这对后续的处理十分重要，因为很多默认参数（如 PCA-白化中的 epsilon）都假定数据已被缩放到合理区间。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;例子:在处理自然图像时，我们获得的像素值在 [0,255] 区间中，常用的处理是将这些像素值除以 255，使它们缩放到 [0,1] 中.&lt;/p&gt;
&lt;h4&gt;逐样本均值消减&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;如果你的数据是平稳的（即数据每一个维度的统计都服从相同分布），那么你可以考虑在每个样本上减去数据的统计平均值(逐样本计算)。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;例子：对于图像，这种归一化可以移除图像的平均亮度值 (intensity)。很多情况下我们对图像的照度并不感兴趣，而更多地关注其内容，这时对每个数据点移除像素的均值是有意义的。注意：虽然该方法广泛地应用于图像，但在处理彩色图像时需要格外小心，具体来说，是因为不同色彩通道中的像素并不都存在平稳特性。&lt;/p&gt;
&lt;h4&gt;特征标准化&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;特征标准化指的是（独立地）使得数据的每一个维度具有零均值和单位方差。这是归一化中最常见的方法并被广泛地使用（例如，在使用支持向量机（SVM）时，特征标准化常被建议用作预处理的一部分）。在实际应用中，特征标准化的具体做法是：首先计算每一个维度上数据的均值（使用全体数据计算），之后在每一个维度上都减去该均值。下一步便是在数据的每一维度上除以该维度上数据的标准差。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;例子:处理音频数据时，常用 Mel 倒频系数 MFCCs 来表征数据。然而MFCC特征的第一个分量（表示直流分量）数值太大，常常会掩盖其他分量。这种情况下，为了平衡各个分量的影响，通常对特征的每个分量独立地使用标准化处理。 &lt;/p&gt;
&lt;p&gt;在基于重构的模型中（比如说常见的RBM，Sparse coding, autoencoder都属于这一类，因为他们基本上都是重构输入数据），通常是选择一个适当的epsilon值使得能够对输入数据进行低通滤波。但是何谓适当的epsilon呢？这还是很难掌握的，因为epsilon太小，则起不到过滤效果，会引入很多噪声，而且基于重构的模型又要去拟合这些噪声；epsilon太大，则又对元素数据有过大的模糊。因此一般的方法是画出变化后数据的特征值分布图，如果那些小的特征值基本都接近0，则此时的epsilon是比较合理的。如下图所示，让那个长长的尾巴接近于x轴。该图的横坐标表示的是第几个特征值，因为已经将数据集的特征值从大到小排序过。&lt;/p&gt;
&lt;p&gt;&lt;img alt="1.png" src="/article_img/Data_Process_2014_08_18_20_05/1.png" /&gt;&lt;/p&gt;
&lt;p&gt;文章中给出了个小小的实用技巧：如果数据已被缩放到合理范围(如[0,1])，可以从epsilon = 0.01或epsilon = 0.1开始调节epsilon。&lt;/p&gt;
&lt;p&gt;基于正交化的ICA模型中，应该保持参数epsilon尽量小，因为这类模型需要对学习到的特征做正交化，以解除不同维度之间的相关性。（暂时没看懂，因为还没有时间去研究过ICA模型，等以后研究过后再来理解）。&lt;/p&gt;
&lt;p&gt;教程中的最后是一些常见数据的预处理标准流程，其实也只是针对具体数据集而已的，所以仅供参考。&lt;/p&gt;
&lt;h3&gt;其他&lt;/h3&gt;
&lt;h4&gt;大图像&lt;/h4&gt;
&lt;p&gt;对于大图像，采用基于 PCA/ZCA 的白化方法是不切实际的，因为协方差矩阵太大。在这些情况下我们退而使用 1/f 白化方法&lt;/p&gt;
&lt;h4&gt;自然灰度图像&lt;/h4&gt;
&lt;p&gt;灰度图像具有平稳特性，我们通常在第一步对每个数据样本分别做均值消减（即减去直流分量），然后采用 PCA/ZCA 白化处理，其中的 epsilon 要足够大以达到低通滤波的效果。 &lt;/p&gt;
&lt;h4&gt;彩色图像&lt;/h4&gt;
&lt;p&gt;对于彩色图像，色彩通道间并不存在平稳特性。因此我们通常首先对数据进行特征缩放（使像素值位于 [0,1] 区间），然后使用足够大的 epsilon 来做 PCA/ZCA。注意在进行 PCA 变换前需要对特征进行分量均值归零化。&lt;/p&gt;
&lt;h4&gt;音频 (MFCC/频谱图)&lt;/h4&gt;
&lt;p&gt;对于音频数据 (MFCC 和频谱图)，每一维度的取值范围（方差）不同。例如 MFCC 的第一分量是直流分量，通常其幅度远大于其他分量，尤其当特征中包含时域导数 (temporal derivatives) 时（这是音频处理中的常用方法）更是如此。因此，对这类数据的预处理通常从简单的数据标准化开始（即使得数据的每一维度均值为零、方差为 1），然后进行 PCA/ZCA 白化（使用合适的 epsilon）。 &lt;/p&gt;
&lt;h4&gt;MNIST 手写数字&lt;/h4&gt;
&lt;p&gt;MNIST 数据集的像素值在 [0,255] 区间中。我们首先将其缩放到 [0,1] 区间。实际上，进行逐样本均值消去也有助于特征学习。注：也可选择以对 MNIST 进行 PCA/ZCA 白化，但这在实践中不常用。 &lt;/p&gt;</summary><category term="algorithm"></category><category term="PCA"></category><category term="ML"></category></entry><entry><title>ML中的PCA及白化</title><link href="/2014/PCA_in_ML_2014_08_18_19_50.html" rel="alternate"></link><updated>2014-08-18T19:50:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2014-08-18:2014/PCA_in_ML_2014_08_18_19_50.html</id><summary type="html">&lt;h3&gt;PCA&lt;/h3&gt;
&lt;p&gt;PCA的具有2个功能,一是维数约简（可以加快算法的训练速度，减小内存消耗等），一是数据的可视化。&lt;/p&gt;
&lt;p&gt;PCA并不是线性回归，因为线性回归是保证得到的函数是y值方面误差最小，而PCA是保证得到的函数到所降的维度上的误差最小。另外线性回归是通过x值来预测y值，而PCA中是将所有的x样本都同等对待。&lt;/p&gt;
&lt;p&gt;在使用PCA前需要对数据进行预处理，首先是均值化，即对每个特征维，都减掉该维的平均值，然后就是将不同维的数据范围归一化到同一范围，方法一般都是除以最大值。但是比较奇怪的是，在对自然图像进行均值处理时并不是不是减去该维的平均值，而是减去这张图片本身的平均值。因为PCA的预处理是按照不同应用场合来定的。&lt;/p&gt;
&lt;p&gt;自然图像指的是人眼经常看见的图像，其符合某些统计特征。一般实际过程中，只要是拿正常相机拍的，没有加入很多人工创作进去的图片都可以叫做是自然图片，因为很多算法对这些图片的输入类型还是比较鲁棒的。在对自然图像进行学习时，其实不需要太关注对图像做方差归一化，因为自然图像每一部分的统计特征都相似，只需做均值为0化就ok了。不过对其它的图片进行训练时，比如首先字识别等，就需要进行方差归一化了。&lt;/p&gt;
&lt;p&gt;PCA的计算过程主要是要求2个东西，一个是降维后的各个向量的方向，另一个是原先的样本在新的方向上投影后的值。&lt;/p&gt;
&lt;p&gt;首先需求出训练样本的协方差矩阵，如公式所示（输入数据已经均值化过）：&lt;/p&gt;
&lt;p&gt;$$ \Sigma = \frac{1}{m}\sum_{i=1}^{m}(x^i)(x^i)^T $$&lt;/p&gt;
&lt;p&gt;求出训练样本的协方差矩阵后，将其进行SVD分解，得出的U向量中的每一列就是这些数据样本的新的方向向量了，排在前面的向量代表的是主方向，依次类推。用U’*X得到的就是降维后的样本值z了，即：&lt;/p&gt;
&lt;p&gt;$$ x_{rot}=U^Tx=\begin{bmatrix}u_1^Tx \\ u_2^Tx\end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;（其实这个z值的几何意义是原先点到该方向上的距离值，但是这个距离有正负之分），这样PCA的2个主要计算任务已经完成了。用U*z就可以将原先的数据样本x给还原出来。&lt;/p&gt;
&lt;p&gt;在使用有监督学习时，如果要采用PCA降维，那么只需将训练样本的x值抽取出来，计算出主成分矩阵U以及降维后的值z，然后让z和原先样本的y值组合构成新的训练样本来训练分类器。在测试过程中，同样可以用原先的U来对新的测试样本降维，然后输入到训练好的分类器中即可。&lt;/p&gt;
&lt;p&gt;有一个观点需要注意，那就是PCA并不能阻止过拟合现象。表明上看PCA是降维了，因为在同样多的训练样本数据下，其特征数变少了，应该是更不容易产生过拟合现象。但是在实际操作过程中，这个方法阻止过拟合现象效果很小，主要还是通过规则项来进行阻止过拟合的。&lt;/p&gt;
&lt;p&gt;并不是所有ML算法场合都需要使用PCA来降维，因为只有当原始的训练样本不能满足我们所需要的情况下才使用，比如说模型的训练速度，内存大小，希望可视化等。如果不需要考虑那些情况，则也不一定需要使用PCA算法了。&lt;/p&gt;
&lt;h3&gt;Whitening&lt;/h3&gt;
&lt;p&gt;Whitening的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。数据的whitening必须满足两个条件：一是不同特征间相关性最小，接近0；二是所有特征的方差相等（不一定为1）。常见的白化操作有PCA whitening和ZCA whitening。&lt;/p&gt;
&lt;p&gt;PCA whitening是指将数据x经过PCA降维为z后，可以看出z中每一维是独立的，满足whitening白化的第一个条件，这是只需要将z中的每一维都除以标准差就得到了每一维的方差为1，也就是说方差相等。公式为：&lt;/p&gt;
&lt;p&gt;$$ x_{PCAwhite,i}=\frac{x_{rot,i}}{\sqrt{\lambda_i}} $$&lt;/p&gt;
&lt;p&gt;ZCA whitening是指数据x先经过PCA变换为z，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足whtienning的第一个条件，特征间相互独立。然后同样进行方差为1的操作，最后将得到的矩阵左乘一个特征向量矩阵U即可。&lt;/p&gt;
&lt;p&gt;ZCA whitening公式为：&lt;/p&gt;
&lt;p&gt;$$ x_{ZCAwhite}=Ux_{PCAwhite} $$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="http://www.cnblogs.com/90zeng/p/PCA_and_Whitening_Images.html?utm_source=tuicool&amp;amp;utm_medium=referral"&gt;过程参考&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="algorithm"></category><category term="PCA"></category><category term="ML"></category></entry><entry><title>matrix</title><link href="/2014/matrix_2014_08_17_10_44.html" rel="alternate"></link><updated>2014-08-17T13:37:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2014-08-17:2014/matrix_2014_08_17_10_44.html</id><summary type="html">&lt;h2&gt;矩阵基本概念&lt;/h2&gt;
&lt;h3&gt;梯度&lt;/h3&gt;
&lt;p&gt;量度标量场改变的速度与方向；标量场的斜度是个向量。&lt;/p&gt;
&lt;h3&gt;旋度&lt;/h3&gt;
&lt;p&gt;量度向量场倾向绕著一个点旋转的程度；向量的卷曲是个向量场。&lt;/p&gt;
&lt;h3&gt;散度(divergence)&lt;/h3&gt;
&lt;p&gt;量度向量场倾向源于一点的程度。&lt;/p&gt;
&lt;h3&gt;点积&lt;/h3&gt;
&lt;p&gt;在数学中，数量积（dot product; scalar product，也称为点积）是接受在实数R上的两个向量并返回一个实数值标量的二元运算。它是欧几里得空间的标准内积。&lt;/p&gt;
&lt;p&gt;两个向量$ a = [a_1, a_2, \cdot \cdot \cdot , a_n] $和$ b = [b_1, b_2, \cdot \cdot \cdot , b_n] $的点积定义为：
$$ a \cdot b=a_1b_1+a_2b_2+ \cdot \cdot \cdot +a_nb_n $$&lt;/p&gt;
&lt;p&gt;使用矩阵乘法并把（纵列）向量当作n*1矩阵，点积还可以写为：
$ a\cdot b=a*b^T $，这里的$ b^T $指示矩阵b的转置。&lt;/p&gt;
&lt;p&gt;上边的为代数定义，如果要写成几何定义的话：$ \vec{a}\cdot \vec{b} = \mid \vec{a} \mid \mid \vec{b} \mid \cos \theta $，$ \theta $为两个向量的夹角。该定义只对二维和三维空间有效。&lt;/p&gt;
&lt;h3&gt;正交&lt;/h3&gt;
&lt;p&gt;两个向量正交意味着它们是相互垂直的。若向量α与β正交，则记为α⊥β。&lt;/p&gt;
&lt;h3&gt;正交函数系&lt;/h3&gt;
&lt;p&gt;在三角函数系中任何不同的两个函数的乘积在区间[-π,π]上的积分等于0，则称这样的三角函数组成的体系叫正交函数系。&lt;/p&gt;
&lt;p&gt;三角函数系{1,cosx,sinx,cos2x,sin2x,……,cosnx,sinnx,……}&lt;/p&gt;
&lt;h3&gt;正交矩阵&lt;/h3&gt;
&lt;p&gt;A是一个n阶方阵,$ A^{-1}=A^T $  或 $AA^T=A^TA=EA $,则A叫做正交矩阵。&lt;/p&gt;
&lt;p&gt;条件:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A^T是正交矩阵&lt;/li&gt;
&lt;li&gt;$ AA^T=A^TA=EA $&lt;/li&gt;
&lt;li&gt;A的各行是单位向量且两两正交&lt;/li&gt;
&lt;li&gt;A的各列是单位向量且两两正交&lt;/li&gt;
&lt;li&gt;(Ax,Ay)=(x,y) x,y∈R&lt;/li&gt;
&lt;li&gt;|A| = 1或-1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;定理:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;方阵A正交的充要条件是A的行（列) 向量组是单位正交向量组；&lt;/li&gt;
&lt;li&gt;方阵A正交的充要条件是A的n个行（列)向量是n维向量空间的一组标准正交基；&lt;/li&gt;
&lt;li&gt;A是正交矩阵的充要条件是：A的行向量组两两正交且都是单位向量；&lt;/li&gt;
&lt;li&gt;A的列向量组也是正交单位向量组。&lt;/li&gt;
&lt;li&gt;正交方阵是欧氏空间中标准正交基到标准正交基的过渡矩阵。
在矩阵论中，实数正交矩阵是方块矩阵Q，它的转置矩阵是它的逆矩阵，如果正交矩阵的行列式为 +1，则我们称之为特殊正交矩阵&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;基矩阵&lt;/h3&gt;
&lt;p&gt;矩阵里的基是指能组成其他矩阵形式的基本矩阵。现在我们用矩阵形式写出基向量和基，这样的矩阵我们叫它基矩阵。&lt;/p&gt;
&lt;p&gt;例如：在3-D空间中，我们用空间坐标系来规范物体的位置，空间坐标系由3个相互垂直的坐标轴组成，我们就把它们作为我们观察3-D空间的基础，空间中物体的位置可以通过它们来衡量。当我们把这3个坐标轴上单位长度的向量记为3个相互正交的单位向量i,j,k，空间中每一个点的位置都可以被这3个向量线性表出，如P&amp;lt;1,-2,3&amp;gt;这个点可以表为i-2j+3k。&lt;/p&gt;
&lt;p&gt;我们把这3个正交的单位向量称为空间坐标系的基，它们单位长度为1且正交，所以可以成为标准正交基。三个向量叫做基向量。现在我们用矩阵形式写出基向量和基。&lt;/p&gt;
&lt;h3&gt;极大线性无关组&lt;/h3&gt;
&lt;p&gt;极大线性无关组，就是其他的东西都可以由“基”线性表示，而这些“基”本身又是线性无关的。&lt;/p&gt;
&lt;p&gt;设S是一个n维向量组,$ α_1,α_2,\cdot \cdot \cdot α_r $ 是S的一个部分组，如果&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;α1,α2,...αr 线性无关；&lt;/li&gt;
&lt;li&gt;向量组S中每一个向量均可由此部分组线性表示，&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;那么α1,α2,...αr 称为向量组S的一个极大线性无关组,或极大无关组。&lt;/p&gt;
&lt;p&gt;性质 :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;极大线性无关组与向量组本身等价&lt;/li&gt;
&lt;li&gt;两个极大线性无关组都是等价的&lt;/li&gt;
&lt;li&gt;向量个数小于或等于后者&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;酉矩阵&lt;/h3&gt;
&lt;p&gt;n阶复方阵U的n个列向量是U空间的一个标准正交基，则U是酉矩阵(Unitary Matrix)。显然酉矩阵是正交矩阵往复数域上的推广。酉矩阵又称为幺正矩阵。&lt;/p&gt;
&lt;p&gt;U是酉矩阵的充分必要条件是，它的n个列向量是两两正交的单位向量。&lt;/p&gt;
&lt;p&gt;一个简单的充分必要判别准则是：$ U^{-1} = U^H $
酉矩阵的共轭转置和它的逆矩阵相等。&lt;/p&gt;
&lt;p&gt;酉方阵在量子力学中有着重要的应用。酉等价是标准正交基到标准正交基的特殊基变换。&lt;/p&gt;
&lt;h3&gt;秩&lt;/h3&gt;
&lt;p&gt;讲到矩阵的秩，几乎必然要引入矩阵的SVD分解：X=USV'，U,V正交阵，S是对角阵。如果是完全SVD分解的话，那S对角线上非零元的个数就是这个矩阵的秩了（这些对角线元素叫做奇异值），还有些零元，这些零元对秩没有贡献。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;把矩阵当做样本集合，每一行（或每一列，这个无所谓）是一个样本，那么矩阵的秩就是这些样本所张成的线性子空间维数。如果矩阵秩远小于样本维数（即矩阵列数），那么这些样本相当于只生活在外围空间中的一个低维子空间，这样就能实施降维操作。举个例子，同一个人在不同光照下采得的正脸图像，假设每一张都是192x168的，且采集了50张，那构成的数据矩阵就为50行192x168列的，但是如果你做SVD分解就会发现，大概只有前10个奇异值比较大，其他的奇异值都接近零，因此实际上可以将接近零的奇异值所对应的那些维度丢掉，只保留前10个奇异值对应的子空间，从而将数据降维到10维的子空间了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;把矩阵当做一个映射，既然是映射，那就得考虑它作用在向量x上的效果Ax。注意Ax相当于A的列的某个线性组合，如果矩阵是低秩的，这意味着这些列所张成的空间是外围空间的一个低维子空间，这个空间由Ax表达（其中x任意）。换句话说，这个矩阵把R^n空间映射到R^m空间，但是其映射的像只在R^m空间的一个低维子空间内生活。从SVD理解的话，Ax=USV'x，因此有三个变换：第一是V'x，相当于在原始的R^n空间旋转了一下坐标轴，这样只是坐标的变化，不改变向量本身（例如长度不变）；第二是S(V'x)，这相当于沿着各个坐标轴做拉伸，并且如果S的对角线上某些元素为零，那么这些元素所对应的那些坐标轴就相当于直接丢掉了；最后再U(SV'x)，还是一个坐标轴旋转。总的来看，Ax就相当于把一个向量x沿着某些特定的方向做不同程度的拉伸（附带上一些不关乎本质的旋转），甚至丢弃，那些没被丢弃的方向个数就是秩了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;矩阵基本计算&lt;/h2&gt;
&lt;h3&gt;矩阵加法&lt;/h3&gt;
&lt;p&gt;前提条件：同型矩阵&lt;br&gt;
操作数：两个m*n矩阵$ A=[a_{ij}],B=[b_{ij}] $&lt;br&gt;
基本动作：元素对应相加 &lt;br&gt;&lt;/p&gt;
&lt;p&gt;$$ A+B = \begin{bmatrix} a_{11}+b_{11} &amp;amp;a_{12}+b_{12} &amp;amp;\cdot \cdot \cdot &amp;amp;a_{1n}+b_{1n} \\ a_{21}+b_{21} &amp;amp;a_{22}+b_{22} &amp;amp;\cdot \cdot \cdot &amp;amp;a_{2n}+b_{2n} \\ \cdot &amp;amp;\cdot  &amp;amp;\cdot  &amp;amp;\cdot \\ a_{m1}+b_{m1} &amp;amp;a_{m2}+b_{m2} &amp;amp;\cdot \cdot \cdot &amp;amp;a_{mn}+b_{mn} \end{bmatrix} = [a_{ij}+b_{ij}] $$&lt;/p&gt;
&lt;h3&gt;矩阵减法&lt;/h3&gt;
&lt;p&gt;前提条件：同型矩阵&lt;br&gt;
操作数：两个m*n矩阵$A=[a_{ij}],B=[b_{ij}]$&lt;br&gt;
基本动作：元素对应相减 &lt;br&gt;&lt;/p&gt;
&lt;p&gt;$$ A-B = \begin{bmatrix} a_{11}-b_{11} &amp;amp;a_{12}-b_{12} &amp;amp;\cdot \cdot \cdot &amp;amp;a_{1n}-b_{1n} \\ a_{21}-b_{21} &amp;amp;a_{22}-b_{22} &amp;amp;\cdot \cdot \cdot &amp;amp;a_{2n}-b_{2n} \\ \cdot &amp;amp;\cdot  &amp;amp;\cdot  &amp;amp;\cdot \\ a_{m1}-b_{m1} &amp;amp;a_{m2}-b_{m2} &amp;amp;\cdot \cdot \cdot &amp;amp;a_{mn}-b_{mn} \end{bmatrix} = [a_{ij}-b_{ij}] $$&lt;/p&gt;
&lt;h3&gt;矩阵取负&lt;/h3&gt;
&lt;p&gt;前提条件：无&lt;br&gt;
操作数：任意一个m*n矩阵$ A=[a_{ij}] $ &lt;br&gt;
基本动作：元素对应取负 &lt;br&gt;&lt;/p&gt;
&lt;p&gt;$$ -A = \begin{bmatrix} -a_{11} &amp;amp;-a_{12} &amp;amp;\cdot \cdot \cdot &amp;amp;-a_{1n} \\ -a_{21} &amp;amp;-a_{22} &amp;amp;\cdot \cdot \cdot &amp;amp;-a_{2n} \\ \cdot &amp;amp;\cdot  &amp;amp;\cdot  &amp;amp;\cdot \\ -a_{m1} &amp;amp;-a_{m2} &amp;amp;\cdot \cdot \cdot &amp;amp;-a_{mn} \end{bmatrix} = [-a_{ij}] $$&lt;/p&gt;
&lt;h3&gt;矩阵乘法&lt;/h3&gt;
&lt;p&gt;前提条件：左矩阵A的列数与右矩阵B的行数相等&lt;br&gt;
操作数：m*n矩阵$ A=[a_{ij}]$，n*m矩阵$B=[b_{ij}]$，A是具有m行的行矩阵$ A = \begin{bmatrix} a_1 \\ a_2 \\ \cdot \\ a_m \end{bmatrix} $，B是具有n列的列矩阵$ B = \begin{bmatrix} b_1 &amp;amp; b_2 &amp;amp; \cdot \cdot \cdot &amp;amp; b_n \end{bmatrix} $ &lt;br&gt;
基本动作：行列积 &lt;br&gt;&lt;/p&gt;
&lt;p&gt;$$ AB = \begin{bmatrix} a_1 \\ a_2 \\ \cdot \\ a_m \end{bmatrix} \begin{bmatrix} b_1 &amp;amp; b_2 &amp;amp; \cdot \cdot \cdot &amp;amp; b_n \end{bmatrix}  = \begin{bmatrix} a_1 b_1 &amp;amp;a_1 b_2  &amp;amp;\cdot \cdot \cdot  &amp;amp;a_1b_n \\ a_2 b_1 &amp;amp;a_2 b_2  &amp;amp;\cdot \cdot \cdot  &amp;amp;a_2 b_n \\ \cdot &amp;amp;\cdot  &amp;amp;\cdot \cdot \cdot  &amp;amp;\cdot \\ a_m b_1 &amp;amp;a_m b_2  &amp;amp;\cdot \cdot \cdot  &amp;amp;a_m b_n \end{bmatrix} $$&lt;/p&gt;
&lt;h3&gt;矩阵数乘&lt;/h3&gt;
&lt;p&gt;前提条件：无&lt;br&gt;
操作数：任意一个m*n矩阵$ A=[a_{ij}] $，数k&lt;br&gt;
基本动作：数k乘以每一个元素 &lt;br&gt;&lt;/p&gt;
&lt;p&gt;$$ kA = \begin{bmatrix} ka_{11} &amp;amp;ka_{12}  &amp;amp;\cdot \cdot \cdot  &amp;amp;ka_{1n} \\ ka_{21} &amp;amp;ka_{22}  &amp;amp;\cdot \cdot \cdot  &amp;amp;ka_{2n} \\ \cdot &amp;amp;\cdot  &amp;amp;\cdot \cdot \cdot  &amp;amp;\cdot \\ ka_{m1} &amp;amp;ka_{m2}  &amp;amp;\cdot \cdot \cdot  &amp;amp;ka_{mn} \end{bmatrix}= [ka_{ij}]$$&lt;/p&gt;
&lt;h3&gt;矩阵转置&lt;/h3&gt;
&lt;p&gt;前提条件：无&lt;br&gt;
操作数：任意一个m*n矩阵$ A=[a_{ij}] $ &lt;br&gt;
基本动作：行列互换，第i行第j列的元素换为第j行第i列的元素，m*n的矩阵转置后为n*m矩阵 &lt;br&gt;&lt;/p&gt;
&lt;p&gt;$$ A^T = \begin{bmatrix} a_{11} &amp;amp;a_{21}  &amp;amp;\cdot \cdot \cdot  &amp;amp;a_{m1} \\ a_{12} &amp;amp;a_{22}  &amp;amp;\cdot \cdot \cdot  &amp;amp;a_{m2} \\ \cdot &amp;amp;\cdot  &amp;amp;\cdot \cdot \cdot  &amp;amp;\cdot \\ a_{1n} &amp;amp;a_{2n} &amp;amp;\cdot \cdot \cdot  &amp;amp;a_{mn} \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;矩阵运算不满足交换律和消去率。&lt;/p&gt;</summary><category term="algorithm"></category><category term="Math"></category></entry><entry><title>PCA</title><link href="/2014/PCA_2014_08_15_22_49.html" rel="alternate"></link><updated>2014-08-15T22:49:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2014-08-15:2014/PCA_2014_08_15_22_49.html</id><summary type="html">&lt;h2&gt;Principal Component Analysis&lt;/h2&gt;
&lt;p&gt;PCA 不仅仅是对高维数据进行降维，更重要的是经过降维去除了噪声，发现了数据中的模式。&lt;/p&gt;
&lt;p&gt;PCA把原先的n个特征用数目更少的m个特征取代，新特征是旧特征的线性组合，这些线性组合最大化样本方差，尽量使新的m个特征互不相关。从旧特征到新特征的映射捕获数据中的固有变异性。&lt;/p&gt;
&lt;h3&gt;降维的必要性&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;多重共线性--预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。&lt;/li&gt;
&lt;li&gt;高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有0.02%。&lt;/li&gt;
&lt;li&gt;过多的变量会妨碍查找规律的建立。&lt;/li&gt;
&lt;li&gt;仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;降维的目的&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;减少预测变量的个数&lt;/li&gt;
&lt;li&gt;确保这些变量是相互独立的&lt;/li&gt;
&lt;li&gt;提供一个框架来解释结果&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;降维的方法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;主成分分析&lt;/li&gt;
&lt;li&gt;因子分析&lt;/li&gt;
&lt;li&gt;用户自定义复合&lt;/li&gt;
&lt;li&gt;等等......&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;预备知识&lt;/h2&gt;
&lt;h3&gt;协方差&lt;/h3&gt;
&lt;p&gt;样本X和样本Y的协方差(Covariance)：&lt;/p&gt;
&lt;p&gt;$$ Cov(X,Y)=\frac{\sum_{i=1}^{n}{(X_i-\overline{X})(Y_i-\overline{Y})}}{(n-1)} $$&lt;/p&gt;
&lt;p&gt;协方差为正时说明X和Y是正相关关系，协方差为负时X和Y是负相关关系，协方差为0时X和Y相互独立。&lt;/p&gt;
&lt;p&gt;Cov(X,X)就是X的方差(Variance)。&lt;/p&gt;
&lt;p&gt;当样本是n维数据时，它们的协方差实际上是协方差矩阵（对称方阵），方阵的边长是。比如对于3维数据(x,y,z)，计算它的协方差就是：$ C_n^2 $。比如对于3维数据(x,y,z)，计算它的协方差就是：&lt;/p&gt;
&lt;p&gt;$$ C=\begin{array}{ccc}cov(x,x)&amp;amp; cov(x,y)&amp;amp; cov(x,z) \\ cov(y,x)&amp;amp;cov(y,y)&amp;amp; cov(y,z) \\ cov(z,x)&amp;amp; cov(z,y)&amp;amp; cov(z,z)\end{array} $$&lt;/p&gt;
&lt;p&gt;若 $ AX=\lambda{X} $，则称$ \lambda $是A的特征值，X是对应的特征向量。实际上可以这样理解：矩阵A作用在它的特征向量X上，仅仅使得X的长度发生了变化，缩放比例就是相应的特征值$ \lambda $。&lt;/p&gt;
&lt;p&gt;当A是n阶可逆矩阵时，A与$ P^{-1}Ap $相似，相似矩阵具有相同的特征值。&lt;/p&gt;
&lt;p&gt;特别地，当A是对称矩阵时，A的奇异值等于A的特征值，存在正交矩阵$ Q(Q^{-1}=QT)$，使得：&lt;/p&gt;
&lt;p&gt;$$ Q^TAQ=\begin{pmatrix} &amp;amp;\lambda_1 &amp;amp; &amp;amp; &amp;amp; \\ &amp;amp; &amp;amp;\lambda_2 &amp;amp; &amp;amp;\\ &amp;amp; &amp;amp; &amp;amp;... &amp;amp; \\ &amp;amp; &amp;amp; &amp;amp; &amp;amp;\lambda_n \end{pmatrix} $$&lt;/p&gt;
&lt;p&gt;对A进行奇异值分解就能求出所有特征值和Q矩阵。&lt;/p&gt;
&lt;p&gt;$ A*Q=Q*D $, D是由特征值组成的对角矩阵&lt;/p&gt;
&lt;p&gt;由特征值和特征向量的定义知，Q的列向量就是A的特征向量。&lt;/p&gt;
&lt;h2&gt;正题&lt;/h2&gt;
&lt;h3&gt;步骤&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;指标数据标准化； 　　&lt;/li&gt;
&lt;li&gt;指标之间的相关性判定； 　&lt;/li&gt;
&lt;li&gt;计算特征值与特征向量&lt;/li&gt;
&lt;li&gt;计算主成分贡献率及累计贡献率&lt;/li&gt;
&lt;li&gt;计算主成分载荷&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;pca.py&lt;/em&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="c1"&gt;#-*- coding:utf-8 -*-&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pca&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;nRedDim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;normalise&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

        &lt;span class="c1"&gt;# 数据标准化&lt;/span&gt;
        &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;

        &lt;span class="c1"&gt;# 协方差矩阵&lt;/span&gt;
        &lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="c1"&gt;# 计算特征值特征向量，按降序排序&lt;/span&gt;
        &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;evecs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="kp"&gt;indices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="kp"&gt;indices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;[::&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;evecs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;evecs&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="kp"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;evals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kp"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;nRedDim&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;evecs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;evecs&lt;/span&gt;&lt;span class="p"&gt;[:,:&lt;/span&gt;&lt;span class="n"&gt;nRedDim&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;normalise&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evecs&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
                &lt;span class="n"&gt;evecs&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evecs&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="kp"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# 产生新的数据矩阵&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evecs&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="kp"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="c1"&gt;# 重新计算原数据&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kp"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evecs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;evecs&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;__main__:&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;#-*- coding:utf-8 -*-&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pca&lt;/span&gt;
&lt;span class="n"&gt;mpl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rcParams&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;font.sans-serif&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;SimHei&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;mpl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rcParams&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;axes.unicode_minus&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="kp"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="kp"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="kp"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="kp"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;原数据集&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;evecs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;重新构造数据&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;center&gt;&lt;img alt="1.png" src="/article_img/PCA_2014_08_15_22_49/1.png" /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img alt="2.png" src="/article_img/PCA_2014_08_15_22_49/2.png" /&gt;&lt;/center&gt;&lt;/p&gt;</summary><category term="algorithm"></category><category term="PCA"></category><category term="ML"></category><category term="Math"></category></entry><entry><title>奇异值</title><link href="/2014/SVD_2014_08_15_21_28.html" rel="alternate"></link><updated>2014-08-15T21:28:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2014-08-15:2014/SVD_2014_08_15_21_28.html</id><summary type="html">&lt;h2&gt;SVD&lt;/h2&gt;
&lt;h3&gt;先简单概念性质的东西&lt;/h3&gt;
&lt;p&gt;SVD分解（奇异值分解），本应是本科生就掌握的方法，然而却经常被忽视。实际上，SVD分解不但很直观，而且极其有用。SVD分解提供了一种方法将一个矩阵拆分成简单的，并且有意义的几块。它的几何解释可以看做将一个空间进行旋转，尺度拉伸，再旋转三步过程。&lt;/p&gt;
&lt;p&gt;首先来看一个对角矩阵&lt;/p&gt;
&lt;p&gt;$$ M=\begin{bmatrix}3 &amp;amp;0 \\ 0 &amp;amp;1 \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;几何上, 我们将一个矩阵理解为对于点(x, y)从一个平面到另一个平面的映射:&lt;/p&gt;
&lt;p&gt;$$ \begin{bmatrix}3 &amp;amp;0 \\ 0 &amp;amp;1 \end{bmatrix} \begin{bmatrix}x \\ y \end{bmatrix} = \begin{bmatrix}3x \\ y \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;下图显示了这个映射的效果: 平面被横向拉伸了3倍，纵向没有变化。&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="1.jpg" src="/article_img/SVD_2014_08_15_21_28/1.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;对于另一个矩阵&lt;/p&gt;
&lt;p&gt;$$ M=\begin{bmatrix}2 &amp;amp;1 \\ 1 &amp;amp;2 \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;它的效果是&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="2.jpg" src="/article_img/SVD_2014_08_15_21_28/2.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;这样一个变化并不是很好描述，然而当我们将坐标系旋转45度后，我们可以看出&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="3.jpg" src="/article_img/SVD_2014_08_15_21_28/3.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;这时，我们发现这个新的网格上发生的变化和网格在对角阵下发生变化的效果相似。&lt;/p&gt;
&lt;p&gt;这是一个对称矩阵的例子，可以看出，对称矩阵经过旋转后，其作用就和对角阵类似了。数学上，对于一个对称矩阵 M, 我们可以找到一组正交向量 $v_i$ 从而 $Mv_i$ 相当于 $v_i$ 上的标量乘积; 也就是&lt;/p&gt;
&lt;p&gt;$$ Mv_i = \lambda_i v_i $$&lt;/p&gt;
&lt;p&gt;$ \lambda_i $ 是标量，也就是对应对角阵中对角线上的元素. 由于这个性质，我们称 $v_i$ 是 M 的特征向量;  $\lambda_i$ 为特征值. 一个对称矩阵不同特征值对应的特征向量是正交的。&lt;/p&gt;
&lt;p&gt;对于更广泛的情况，我们看看是否能从一个正交网格转换到另一个正交网格. 考虑一个非对称矩阵:&lt;/p&gt;
&lt;p&gt;$$ M=\begin{bmatrix}1 &amp;amp;1 \\ 0 &amp;amp;1 \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;这个矩阵的效果形象的称为剃刀（shear）。&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="4.jpg" src="/article_img/SVD_2014_08_15_21_28/4.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;这个矩阵将网格在水平方向拉伸了，而垂直方向没有变化。如果我们将网格旋转大约58度，这两个网格就又会都变为正交的了。 &lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="5.jpg" src="/article_img/SVD_2014_08_15_21_28/5.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h3&gt;奇异值分解&lt;/h3&gt;
&lt;p&gt;考虑一个 2*2 矩阵, 我们可以找到两组网格的对应关系。用向量表示，那就是当我们选择合适的单位正交向量 $v_1$ 和 $v_2$, $Mv_1$ 和 $Mv_2$ 也是正交的。&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="6.jpg" src="/article_img/SVD_2014_08_15_21_28/6.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;我们使用 $u_1$ 和 $u_2$ 代表 $Mv_1$ 和 $Mv_2$的方向.  $Mv_1$ 和 $Mv_2$ 的长度表示为 $\sigma_1$ 和 $\sigma_2$，也就是网格在每个方向的拉伸. 这两个拉伸值叫做M的 奇异值（sigular value）&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="7.jpg" src="/article_img/SVD_2014_08_15_21_28/7.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;和前面类似，我们可以有  &lt;/p&gt;
&lt;p&gt;$$ \begin{align*}Mv_1 = \sigma_1u_1  \\ Mv_2 = \sigma_2u_2 \end{align*} $$&lt;/p&gt;
&lt;p&gt;我们一直讨论的 $v_1$ 和 $v_2$ 是一对正交向量， 对于一般的向量 x，我们有这样的投影关系&lt;/p&gt;
&lt;p&gt;$$ x=(v_1 \cdot x)v_1 + (v_2 \cdot x)v_2 $$&lt;/p&gt;
&lt;p&gt;也就是说  &lt;/p&gt;
&lt;p&gt;$$ Mx=(v_1 \cdot x)Mv_1 + (v_2 \cdot x)Mv_2 $$&lt;/p&gt;
&lt;p&gt;$$ Mx = (v_1 \cdot x) \sigma_1u_1 + (v_2 \cdot x) \sigma_2u_2 $$&lt;/p&gt;
&lt;p&gt;注意点积可以用向量的转置来计算&lt;/p&gt;
&lt;p&gt;$$ v \cdot x = v^Tx $$&lt;/p&gt;
&lt;p&gt;既&lt;/p&gt;
&lt;p&gt;$$ Mx = u_1\sigma_1 v_1^Tx + u_2\sigma_2 v_2^Tx  \rightarrow      M = u_1\sigma_1 v_1^T + u_2\sigma_2 v_2^T $$&lt;/p&gt;
&lt;p&gt;这个关系可以写成矩阵形式&lt;/p&gt;
&lt;p&gt;$$ M = U \Sigma V^T $$&lt;/p&gt;
&lt;p&gt;U 是列向量 $u_1$ 和 $u_2$组成的矩阵, $\Sigma$是非零项为$\sigma_1$ 和 $\sigma_2$ 的对角矩阵,  V是列向量 $v_1$ 和 $v_2$组成的矩阵. 带有上标T的矩阵V是矩阵V的转置. &lt;del&gt;即V描述了域中的一组正交基，U描述了相关域的另一组正交基，$\Sigma$ 表述了U中的向量与V中向量的拉伸关系。&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;上面描述了怎样将矩阵M分解成三个矩阵的乘积：V描述了原始空间中的正交基，U描述了相关空间的正交基，$\Sigma$描述了V中的向量变成U中的向量时被拉伸的倍数。&lt;/p&gt;
&lt;h3&gt;寻找奇异值分解&lt;/h3&gt;
&lt;p&gt;奇异值分解可以应用于任何矩阵，对于前面的例子，如果我们加上一个单位圆，那它会映射成一个椭圆，椭圆的长轴和短轴定义了新的域中的正交网格，可以被表示为$Mv_1$ and $Mv_2$。&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="8.jpg" src="/article_img/SVD_2014_08_15_21_28/8.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="9.jpg" src="/article_img/SVD_2014_08_15_21_28/9.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;换句话说，单位圆上的函数 $\left| Mx \right|$ 在  $v_1$ 取得最大值，在 $v_2$取得最小值. 这将单位圆上的函数优化问题简化了。可以证明，这个函数的极值点就出现在$M^TM$的特征向量上，这个矩阵一定是对称的，所以不同特征值对应的特征向量$v_i$是正交的.&lt;/p&gt;
&lt;p&gt;$ \sigma_i = \left| Mv_i \right| $就是奇异值,  $u_i$ 是 $Mv_i$方向的单位向量.&lt;/p&gt;
&lt;p&gt;$$ Mv_i = \sigma_iu_i \\ Mv_j = \sigma_ju_j \\ Mv_i \cdot  Mv_j = v_i^TM^T Mv_j = v_i \cdot M^TMv_j = \lambda_jv_i \cdot  v_j = 0  $$&lt;/p&gt;
&lt;p&gt;也就是&lt;/p&gt;
&lt;p&gt;$$ Mv_i \cdot Mv_j = \sigma_i\sigma_j u_i \cdot u_j = 0 $$&lt;/p&gt;
&lt;p&gt;因此, $u_i 和 u_j$ 也是正交的。所以我们就把一组正交基 $v_i$ 变换到了另一组正交基 $u_i$. &lt;/p&gt;
&lt;h3&gt;另一个例子&lt;/h3&gt;
&lt;p&gt;我们来看一个奇异矩阵（秩为1，或只有一个非零奇异值）&lt;/p&gt;
&lt;p&gt;$$ M=\begin{bmatrix}1 &amp;amp;1 \\ 2 &amp;amp;2 \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;它的效果如下&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="10.jpg" src="/article_img/SVD_2014_08_15_21_28/10.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;在这个例子中，第二个奇异值为0，所以 $M = u_1\sigma_1 v_1^T$. &lt;/p&gt;
&lt;p&gt;也就是说，如果有奇异值为0，那么这个矩阵就有降维的效果。因为0奇异值对应的维度就不会出现在右边。（换句话讲，如果一些奇异值为零，相应的项将不会出现在M的分解中。因此，矩阵M的秩（即线性独立的行或列的个数）等于非零奇异值的个数。）&lt;/p&gt;
&lt;h3&gt;数据压缩&lt;/h3&gt;
&lt;p&gt;这对于计算机科学中的数据压缩极其有用。例如我们想压缩下面的15*25 像素的黑白图像&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="11.jpg" src="/article_img/SVD_2014_08_15_21_28/11.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;因为在图像中只有三种类型的列（如下），它可以以更紧凑的形式被表示。 &lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="12.jpg" src="/article_img/SVD_2014_08_15_21_28/12.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="13.jpg" src="/article_img/SVD_2014_08_15_21_28/13.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="14.jpg" src="/article_img/SVD_2014_08_15_21_28/14.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;我们用15*25的矩阵来表示这个图像，其中每个元素非0即1，0表示黑色像素，1表示白色像素。如下所示，共有375个元素。&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="15.jpg" src="/article_img/SVD_2014_08_15_21_28/15.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;如果对M进行奇异值分解的话，我们只会得到三个非零的奇异值。&lt;/p&gt;
&lt;p&gt;$$ \sigma_1 = 14.72, \sigma_2 = 5.22,\sigma_3 = 3.31  $$&lt;/p&gt;
&lt;p&gt;因此，矩阵可以如下表示&lt;/p&gt;
&lt;p&gt;$$ M=u_1\sigma_1 v_1^T + u_2\sigma_2 v_2^T + u_3\sigma_3 v_3^T $$&lt;/p&gt;
&lt;p&gt;我们有三个包含15个元素的向量$v_i$，三个包含25个元素的向量$u_i$，以及三个奇异值$\sigma_i$。这意味着我们可以只用123个数字就能表示这个矩阵而不是出现在矩阵中的375个元素。奇异值分解找到了矩阵中的冗余信息实现了降维。(在这种方式下，我们看到在矩阵中有3个线性独立的列，也就是说矩阵的秩是3)。&lt;/p&gt;
&lt;h3&gt;降噪&lt;/h3&gt;
&lt;p&gt;从之前的例子看出我们利用了矩阵中有很多奇异值为0的特殊性。通常来说，越大的奇异值对应的信息越令人感兴趣。例如，想象我们用扫描仪将上面的图片输入到我们的计算机。但是，我们的扫描机会在图片上产生一些缺陷（通常称作“噪声”）。&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="16.jpg" src="/article_img/SVD_2014_08_15_21_28/16.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;我们以同样的方式处理：用15*25矩阵来表示图像，然后进行奇异值分解。我们得到以下奇异值：&lt;/p&gt;
&lt;p&gt;$$ \sigma_1 = 14.15,\sigma_2 = 4.67,\sigma_3 = 3.00,\sigma_4 = 0.21,\sigma_5 = 0.19,,,\sigma_15 = 0.05$$&lt;/p&gt;
&lt;p&gt;很明显，头三个奇异值是最重要的，所以我们假定其他的都是图像上的噪声，并假设假设&lt;/p&gt;
&lt;p&gt;$$ M \approx u_1\sigma_1 v_1^T + u_2\sigma_2 v_2^T + u_3\sigma_3 v_3^T $$&lt;/p&gt;
&lt;p&gt;这就产生了如下的优化后的图片&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="17.jpg" src="/article_img/SVD_2014_08_15_21_28/17.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h3&gt;数据分析&lt;/h3&gt;
&lt;p&gt;我们在收集数据的时候经常会遇到噪声：无论工具多好，总有一些误差在测量过程中。如果我们记得大的奇异值指向矩阵中重要的特征，很自然地想到用奇异值分解去研究被收集的数据。&lt;/p&gt;
&lt;p&gt;例如，我们收集了一些数据如下：&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="18.jpg" src="/article_img/SVD_2014_08_15_21_28/18.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;如下是我们获得的数据，将其放入矩阵中：&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="19.jpg" src="/article_img/SVD_2014_08_15_21_28/19.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;然后进行奇异值分解。我们得到奇异值&lt;/p&gt;
&lt;p&gt;$\sigma_1 = 6.04,\sigma_2 = 0.22 $&lt;/p&gt;
&lt;p&gt;其中第一个奇异值远远大于另外一个，很安全的假设小的奇异值$\sigma_2$是数据中的噪声并且可以理想地认为是0。这个例子中的矩阵的秩是1，意味着所有数据都位于 $u_i$ 定义的线上。&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="20.jpg" src="/article_img/SVD_2014_08_15_21_28/20.jpg" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;这个简短的例子引出了主成分分析领域，展示了一系列用奇异值分解来检测数据依赖和冗余的技术。&lt;/p&gt;
&lt;p&gt;同样地，奇异值分解可以用来检测数据中的簇，这就解释了奇异值分解可以用来尝试优化Netflix的电影推荐系统。程序会根据你看过的电影来对与你看过的电影相似的未看过的电影进行排序。推荐系统会挑选出你未看过的电影集合中预估分高的电影。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;无论是特征值分解还是奇异值分解，都是为了让人们对矩阵（或者线性变换）的作用有一个直观的认识。这是因为我们拿过来一个矩阵，很多情况下只能看到一堆排列有序的数字，而看不到这些数字背后的真实含义，特征值分解和奇异值分解告诉了我们这些数字背后的真实含义，换句话说，它告诉了我们关于矩阵作用的本质信息。&lt;/p&gt;
&lt;p&gt;奇异值分解的含义是，把一个矩阵A看成线性变换（当然也可以看成是数据矩阵或者样本矩阵），那么这个线性变换的作用效果是这样的，我们可以在原空间找到一组标准正交基V，同时可以在像空间找到一组标准正交基U，我们知道，看一个矩阵的作用效果只要看它在一组基上的作用效果即可，在内积空间上，我们更希望看到它在一组标准正交基上的作用效果。而矩阵A在标准正交基V上的作用效果恰好可以表示为在U的对应方向上只进行纯粹的伸缩！这就大大简化了我们对矩阵作用的认识，因为我们知道，我们面前不管是多么复杂的矩阵，它在某组标准正交基上的作用就是在另外一组标准正交基上进行伸缩而已。&lt;/p&gt;
&lt;p&gt;特征分解也是这样的，也可以简化我们对矩阵的认识。对于可对角化的矩阵，该线性变换的作用就是将某些方向（特征向量方向）在该方向上做伸缩。&lt;/p&gt;
&lt;p&gt;有了上述认识，当我们要看该矩阵对任一向量x的作用效果的时候，在特征分解的视角下，我们可以把x往特征向量方向上分解，然后每个方向上做伸缩，最后再把结果加起来即可；在奇异值分解的视角下，我们可以把x往V方向上分解，然后将各个分量分别对应到U方向上做伸缩，最后把各个分量上的结果加起来即可。&lt;/p&gt;
&lt;p&gt;当我们注意到，不是所有的矩阵都能对角化（对称矩阵总是可以），而所有矩阵总是可以做奇异值分解的。那么多类型的矩阵，我们居然总是可以从一个统一且简单的视角去看它，我们就会感叹奇异值分解是多么奇妙了！&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Gilbert Strang, Linear Algebra and Its Applications. Brooks Cole.Strang’s book is something of a classic though some may find it to be a little too formal.&lt;/p&gt;
&lt;p&gt;William H. Press et al, Numercial Recipes in C: The Art of Scientific Computing. Cambridge University Press.Authoritative, yet highly readable. Older versions &lt;a href="http://www.nr.com/oldverswitcher.html"&gt;are available online&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Dan Kalman, &lt;a href="http://www.math.umn.edu/~lerman/math5467/svd.pdf"&gt;A Singularly Valuable Decomposition: The SVD of a Matrix&lt;/a&gt;, The College Mathematics Journal 27 (1996), 2-23.Kalman’s article, like this one, aims to improve the profile of the singular value decomposition. It also a description of how least-squares computations are facilitated by the decomposition.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.nytimes.com/2008/11/23/magazine/23Netflix-t.html"&gt;If You Liked This, You’re Sure to Love That&lt;/a&gt;, The New York Times, November 21, 2008.This article describes Netflix’s prize competition as well as some of the challenges associated with it.&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="algorithm"></category><category term="PCA"></category><category term="ML"></category><category term="Math"></category></entry><entry><title>逻辑回归实现</title><link href="/2014/Logistic_regression_python_2014_08_09_20_07.html" rel="alternate"></link><updated>2014-08-09T20:07:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2014-08-09:2014/Logistic_regression_python_2014_08_09_20_07.html</id><summary type="html">&lt;h2&gt;Logistic regression in Python&lt;/h2&gt;
&lt;p&gt;本文基于yhat上&lt;a href="http://blog.yhathq.com/posts/logistic-regression-and-python.html"&gt;Logistic Regression in Python&lt;/a&gt;，作了中文翻译，并相应补充了一些内容。本文并不研究逻辑回归具体算法实现，而是使用了一些算法库，旨在帮助需要用Python来做逻辑回归的训练和预测的读者快速上手。&lt;/p&gt;
&lt;p&gt;逻辑回归是一项可用于预测二分类结果(binary outcome)的统计技术，广泛应用于金融、医学、犯罪学和其他社会科学中。逻辑回归使用简单且非常有效，你可以在许多机器学习、应用统计的书中的前几章中找到个关于逻辑回归的介绍。逻辑回归在许多统计课程中都会用到。&lt;/p&gt;
&lt;p&gt;我们不难找到使用R语言的高质量的逻辑回归实例，如UCLA的教程R Data Analysis Examples: Logit Regression就是一个很好的资源。Python是机器学习领域最流行的语言之一，并且已有许多Python的资源涵盖了支持向量积和文本分类等话题，但少有关于逻辑回归的资料。&lt;/p&gt;
&lt;p&gt;本文介绍了如何使用Python来完成逻辑回归。&lt;/p&gt;
&lt;h3&gt;逻辑回归的实例&lt;/h3&gt;
&lt;p&gt;在此使用与&lt;a href="http://www.ats.ucla.edu/stat/r/dae/logit.htm"&gt;Logit Regression in R&lt;/a&gt;相同的数据集来研究Python中的逻辑回归，目的是要辨别不同的因素对研究生录取的影响。&lt;/p&gt;
&lt;p&gt;数据集中的前三列可作为预测变量(predictor variables)：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;gpa&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gre&lt;/code&gt; 分数&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rank&lt;/code&gt; 表示本科生母校的声望&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第四列&lt;code&gt;admit&lt;/code&gt;则是二分类目标变量(binary target variable)，它表明考生最终是否被录用。&lt;/p&gt;
&lt;h3&gt;加载数据&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;pandas.read_csv&lt;/code&gt;加载数据，这样我们就有了可用于探索数据的&lt;code&gt;DataFrame&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.api&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sm&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pl&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="c1"&gt;# 加载数据&lt;/span&gt;
&lt;span class="c1"&gt;# 备用地址: http://cdn.powerxing.com/files/lr-binary.csv&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http://www.ats.ucla.edu/stat/data/binary.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 浏览数据集&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c1"&gt;#    admit  gre   gpa  rank&lt;/span&gt;
&lt;span class="c1"&gt;# 0      0  380  3.61     3&lt;/span&gt;
&lt;span class="c1"&gt;# 1      1  660  3.67     3&lt;/span&gt;
&lt;span class="c1"&gt;# 2      1  800  4.00     1&lt;/span&gt;
&lt;span class="c1"&gt;# 3      1  640  3.19     4&lt;/span&gt;
&lt;span class="c1"&gt;# 4      0  520  2.93     4&lt;/span&gt;
&lt;span class="c1"&gt;# 重命名&amp;#39;rank&amp;#39;列，因为dataframe中有个方法名也为&amp;#39;rank&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;admit&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gre&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gpa&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;prestige&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;
&lt;span class="c1"&gt;# array([admit, gre, gpa, prestige], dtype=object)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;注意到有一列属性名为&lt;code&gt;ran&lt;/code&gt;k，但因为&lt;code&gt;rank&lt;/code&gt;也是&lt;code&gt;pandas dataframe&lt;/code&gt;中一个方法的名字，因此需要将该列重命名为&lt;code&gt;prestige&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;统计摘要(Summary Statistics) 以及 查看数据&lt;/h3&gt;
&lt;p&gt;现在我们就将需要的数据正确载入到Python中了，现在来看下数据。我们可以使用&lt;code&gt;pandas&lt;/code&gt;的函数&lt;code&gt;describe&lt;/code&gt;来给出数据的摘要&lt;code&gt;describe&lt;/code&gt;与R语言中的&lt;code&gt;summay&lt;/code&gt;类似。这里也有一个用于计算标准差的函数&lt;code&gt;std&lt;/code&gt;，但在&lt;code&gt;describe&lt;/code&gt;中已包括了计算标准差。&lt;/p&gt;
&lt;p&gt;我特别喜欢&lt;code&gt;pandas&lt;/code&gt;的&lt;code&gt;pivot_table/crosstab&lt;/code&gt;聚合功能。&lt;code&gt;crosstab&lt;/code&gt;可方便的实现多维频率表(frequency tables)(有点像R语言中的&lt;code&gt;table&lt;/code&gt;)。你可以用它来查看不同数据所占的比例。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# summarize the data
print df.describe()
#             admit         gre         gpa   prestige
# count  400.000000  400.000000  400.000000  400.00000
# mean     0.317500  587.700000    3.389900    2.48500
# std      0.466087  115.516536    0.380567    0.94446
# min      0.000000  220.000000    2.260000    1.00000
# 25%      0.000000  520.000000    3.130000    2.00000
# 50%      0.000000  580.000000    3.395000    2.00000
# 75%      1.000000  660.000000    3.670000    3.00000
# max      1.000000  800.000000    4.000000    4.00000
# 查看每一列的标准差
print df.std()
# admit      0.466087
# gre      115.516536
# gpa        0.380567
# prestige   0.944460
# 频率表，表示prestige与admin的值相应的数量关系
print pd.crosstab(df[&amp;#39;admit&amp;#39;], df[&amp;#39;prestige&amp;#39;], rownames=[&amp;#39;admit&amp;#39;])
# prestige   1   2   3   4
# admit                   
# 0         28  97  93  55
# 1         33  54  28  12
# plot all of the columns
df.hist()
pl.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;运行代码后，绘制的柱状统计图如下所示：
&lt;center&gt;&lt;img alt="logit_hist.png" src="/article_img/Logistic_regression_python_2014_08_09_20_07/logit_hist.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h3&gt;虚拟变量(dummy variables)&lt;/h3&gt;
&lt;p&gt;虚拟变量，也叫哑变量，可用来表示分类变量、非数量因素可能产生的影响。在计量经济学模型，需要经常考虑属性因素的影响。例如，职业、文化程度、季节等属性因素往往很难直接度量它们的大小。只能给出它们的“Yes—D=1”或”No—D=0”，或者它们的程度或等级。为了反映属性因素和提高模型的精度，必须将属性因素“量化”。通过构造0-1型的人工变量来量化属性因素。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pandas&lt;/code&gt;提供了一系列分类变量的控制。我们可以用&lt;code&gt;get_dummies&lt;/code&gt;来将&lt;code&gt;prestige&lt;/code&gt;一列虚拟化。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;get_dummies&lt;/code&gt;为每个指定的列创建了新的带二分类预测变量的&lt;code&gt;DataFrame&lt;/code&gt;，在本例中，&lt;code&gt;prestige&lt;/code&gt;有四个级别：1，2，3以及4（1代表最有声望），&lt;code&gt;prestige&lt;/code&gt;作为分类变量更加合适。当调用&lt;code&gt;get_dummies&lt;/code&gt;时，会产生四列的&lt;code&gt;dataframe&lt;/code&gt;，每一列表示四个级别中的一个。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# 将prestige设为虚拟变量
dummy_ranks = pd.get_dummies(df[&amp;#39;prestige&amp;#39;], prefix=&amp;#39;prestige&amp;#39;)
print dummy_ranks.head()
#    prestige_1  prestige_2  prestige_3  prestige_4
# 0           0           0           1           0
# 1           0           0           1           0
# 2           1           0           0           0
# 3           0           0           0           1
# 4           0           0           0           1
# 为逻辑回归创建所需的data frame
# 除admit、gre、gpa外，加入了上面常见的虚拟变量（注意，引入的虚拟变量列数应为虚拟变量总列数减1，减去的1列作为基准）
cols_to_keep = [&amp;#39;admit&amp;#39;, &amp;#39;gre&amp;#39;, &amp;#39;gpa&amp;#39;]
data = df[cols_to_keep].join(dummy_ranks.ix[:, &amp;#39;prestige_2&amp;#39;:])
print data.head()
#    admit  gre   gpa  prestige_2  prestige_3  prestige_4
# 0      0  380  3.61           0           1           0
# 1      1  660  3.67           0           1           0
# 2      1  800  4.00           0           0           0
# 3      1  640  3.19           0           0           1
# 4      0  520  2.93           0           0           1
# 需要自行添加逻辑回归所需的intercept变量
data[&amp;#39;intercept&amp;#39;] = 1.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这样，数据原本的&lt;code&gt;prestige&lt;/code&gt;属性就被&lt;code&gt;prestige_x&lt;/code&gt;代替了，例如原本的数值为2，则&lt;code&gt;prestige_2&lt;/code&gt;为1，&lt;code&gt;prestige_1&lt;/code&gt;、&lt;code&gt;prestige_3&lt;/code&gt;、&lt;code&gt;prestige_4&lt;/code&gt;都为0。&lt;/p&gt;
&lt;p&gt;将新的虚拟变量加入到了原始的数据集中后，就不再需要原来的&lt;code&gt;prestige&lt;/code&gt;列了。在此要强调一点，生成m个虚拟变量后，只要引入m-1个虚拟变量到数据集中，未引入的一个是作为基准对比的。&lt;/p&gt;
&lt;p&gt;最后，还需加上常数&lt;code&gt;intercept&lt;/code&gt;，&lt;code&gt;statemodels&lt;/code&gt;实现的逻辑回归需要显式指定。&lt;/p&gt;
&lt;h3&gt;执行逻辑回归&lt;/h3&gt;
&lt;p&gt;实际上完成逻辑回归是相当简单的，首先指定要预测变量的列，接着指定模型用于做预测的列，剩下的就由算法包去完成了。&lt;/p&gt;
&lt;p&gt;本例中要预测的是&lt;code&gt;admin&lt;/code&gt;列，使用到&lt;code&gt;gre&lt;/code&gt;、&lt;code&gt;gpa&lt;/code&gt;和虚拟变量&lt;code&gt;prestige_2&lt;/code&gt;、&lt;code&gt;prestige_3&lt;/code&gt;、&lt;code&gt;prestige_4&lt;/code&gt;。&lt;code&gt;prestige_1&lt;/code&gt;作为基准，所以排除掉，以防止&lt;a href="http://en.wikipedia.org/wiki/Multicollinearity#Remedies_for_multicollinearity"&gt;多元共线性&lt;/a&gt;(multicollinearity)和引入分类变量的所有虚拟变量值所导致的陷阱(&lt;a href="http://en.wikipedia.org/wiki/Dummy_variable_(statistics)"&gt;dummy variable trap&lt;/a&gt;)。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# 指定作为训练变量的列，不含目标列`admit`
train_cols = data.columns[1:]
# Index([gre, gpa, prestige_2, prestige_3, prestige_4], dtype=object)
logit = sm.Logit(data[&amp;#39;admit&amp;#39;], data[train_cols])
# 拟合模型
result = logit.fit()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;在这里是使用了&lt;code&gt;statesmodels&lt;/code&gt;的&lt;code&gt;Logit&lt;/code&gt;函数，更多的模型细节可以查阅&lt;code&gt;statesmodels&lt;/code&gt;的文档。&lt;/p&gt;
&lt;h3&gt;使用训练模型预测数据&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;# 构建预测集&lt;/span&gt;
&lt;span class="c1"&gt;# 与训练集相似，一般也是通过 pd.read_csv() 读入&lt;/span&gt;
&lt;span class="c1"&gt;# 在这边为方便，我们将训练集拷贝一份作为预测集（不包括 admin 列）&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;copy&lt;/span&gt;
&lt;span class="n"&gt;combos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;deepcopy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 数据中的列要跟预测时用到的列一致&lt;/span&gt;
&lt;span class="n"&gt;predict_cols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;combos&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="c1"&gt;# 预测集也要添加intercept变量&lt;/span&gt;
&lt;span class="n"&gt;combos&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;intercept&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="c1"&gt;# 进行预测，并将预测评分存入 predict 列中&lt;/span&gt;
&lt;span class="n"&gt;combos&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;predict&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;combos&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predict_cols&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="c1"&gt;# 预测完成后，predict 的值是介于 [0, 1] 间的概率值&lt;/span&gt;
&lt;span class="c1"&gt;# 我们可以根据需要，提取预测结果&lt;/span&gt;
&lt;span class="c1"&gt;# 例如，假定 predict &amp;gt; 0.5，则表示会被录取&lt;/span&gt;
&lt;span class="c1"&gt;# 在这边我们检验一下上述选取结果的精确度&lt;/span&gt;
&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;hit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;combos&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="c1"&gt;# 预测分数 predict, 是数据中的最后一列&lt;/span&gt;
  &lt;span class="n"&gt;predict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="c1"&gt;# 实际录取结果&lt;/span&gt;
  &lt;span class="n"&gt;admit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
  &lt;span class="c1"&gt;# 假定预测概率大于0.5则表示预测被录取&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;predict&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="c1"&gt;# 表示预测命中&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;admit&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;hit&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="c1"&gt;# 输出结果&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Total: &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;, Hit: &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;, Precision: &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;100.0&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;hit&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;total&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Total: 49, Hit: 30, Precision: 61.22&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;在这里，我是简单的将原始数据再作为待预测的数据进行检验。通过上述步骤得到的是一个概率值，而不是一个直接的二分类结果（被录取/不被录取）。通常，我们可以设定一个阈值，若 &lt;code&gt;predict&lt;/code&gt; 大于该阈值，则认为是被录取了，反之，则表示不被录取。&lt;/p&gt;
&lt;p&gt;在上面的例子中，假定预测概率大于 0.5 则表示预测被录取，一共预测有 49 个被录取，其中有 30 个预测命中，精确度为 61.22%。&lt;/p&gt;
&lt;h3&gt;结果解释&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;statesmodels&lt;/code&gt;提供了结果的摘要，如果你使用过R语言，你会发现结果的输出与之相似。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# 查看数据的要点
print result.summary()

Logit Regression Results                           
==============================================================================
Dep. Variable:                  admit   No. Observations:                  400
Model:                          Logit   Df Residuals:                      394
Method:                           MLE   Df Model:                            5
Date:                Sun, 03 Mar 2013   Pseudo R-squ.:                 0.08292
Time:                        12:34:59   Log-Likelihood:                -229.26
converged:                       True   LL-Null:                       -249.99
                                        LLR p-value:                 7.578e-08
==============================================================================
                 coef    std err          z      P&amp;gt;|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
gre            0.0023      0.001      2.070      0.038         0.000     0.004
gpa            0.8040      0.332      2.423      0.015         0.154     1.454
prestige_2    -0.6754      0.316     -2.134      0.033        -1.296    -0.055
prestige_3    -1.3402      0.345     -3.881      0.000        -2.017    -0.663
prestige_4    -1.5515      0.418     -3.713      0.000        -2.370    -0.733
intercept     -3.9900      1.140     -3.500      0.000        -6.224    -1.756
==============================================================================
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;你可以看到模型的系数，系数拟合的效果，以及总的拟合质量，以及一些统计度量。[待补充: 模型结果主要参数的含义]&lt;/p&gt;
&lt;p&gt;当然你也可以只观察结果的某部分，如置信区间(confidence interval)可以看出模型系数的健壮性。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# 查看每个系数的置信区间
print result.conf_int()
#                    0         1
# gre         0.000120  0.004409
# gpa         0.153684  1.454391
# prestige_2 -1.295751 -0.055135
# prestige_3 -2.016992 -0.663416
# prestige_4 -2.370399 -0.732529
# intercept  -6.224242 -1.755716
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;在这个例子中，我们可以肯定被录取的可能性与应试者毕业学校的声望存在着逆相关的关系。&lt;/p&gt;
&lt;p&gt;换句话说，高排名学校（prestige_1==True）的湘鄂生呗录取的概率比低排名学校（prestige_4==True）要高。&lt;/p&gt;
&lt;h3&gt;相对危险度（odds ratio）&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;odds ratio&lt;br&gt;OR值，是相对危险度，又称比值比、优势比。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;使用每个变量系数的指数来生成odds ratio，可知变量每单位的增加、减少对录取几率的影响。例如，如果学校的声望为2，则我们可以期待被录取的几率减少大概50%。UCLA上有一个对odds ratio更为深入的解释: &lt;a href="http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm"&gt;在逻辑回归中如何解释odds ratios?&lt;/a&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# 输出 odds ratio
print np.exp(result.params)
# gre           1.002267
# gpa           2.234545
# prestige_2    0.508931
# prestige_3    0.261792
# prestige_4    0.211938
# intercept     0.018500
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;我们也可以使用置信区间来计算系数的影响，来更好地估计一个变量影响录取率的不确定性。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# odds ratios and 95% CI
params = result.params
conf = result.conf_int()
conf[&amp;#39;OR&amp;#39;] = params
conf.columns = [&amp;#39;2.5%&amp;#39;, &amp;#39;97.5%&amp;#39;, &amp;#39;OR&amp;#39;]
print np.exp(conf)
#                   2.5%     97.5%        OR
# gre           1.000120  1.004418  1.002267
# gpa           1.166122  4.281877  2.234545
# prestige_2    0.273692  0.946358  0.508931
# prestige_3    0.133055  0.515089  0.261792
# prestige_4    0.093443  0.480692  0.211938
# intercept     0.001981  0.172783  0.018500
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;更深入的挖掘&lt;/h3&gt;
&lt;p&gt;为了评估我们分类器的效果，我们将使用每个输入值的逻辑组合（logical combination）来重新创建数据集，如此可以得知在不同的变量下预测录取可能性的增加、减少。首先我们使用名为 &lt;code&gt;cartesian&lt;/code&gt; 的辅助函数来生成组合值（来源于: &lt;a href="http://stackoverflow.com/questions/1208118/using-numpy-to-build-an-array-of-all-combinations-of-two-arrays"&gt;如何使用numpy构建两个数组的组合&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;我们使用 &lt;code&gt;np.linspace&lt;/code&gt; 创建 &lt;code&gt;gre&lt;/code&gt; 和 &lt;code&gt;gpa&lt;/code&gt; 值的一个范围，即从指定的最大、最小值来创建一个线性间隔的值的范围。在本例子中，取已知的最大、最小值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# 根据最大、最小值生成 GRE、GPA 均匀分布的10个值，而不是生成所有可能的值
gres = np.linspace(data[&amp;#39;gre&amp;#39;].min(), data[&amp;#39;gre&amp;#39;].max(), 10)
print gres
# array([ 220.        ,  284.44444444,  348.88888889,  413.33333333,
#         477.77777778,  542.22222222,  606.66666667,  671.11111111,
#         735.55555556,  800.        ])
gpas = np.linspace(data[&amp;#39;gpa&amp;#39;].min(), data[&amp;#39;gpa&amp;#39;].max(), 10)
print gpas
# array([ 2.26      ,  2.45333333,  2.64666667,  2.84      ,  3.03333333,
#         3.22666667,  3.42      ,  3.61333333,  3.80666667,  4.        ])
# 枚举所有的可能性
combos = pd.DataFrame(cartesian([gres, gpas, [1, 2, 3, 4], [1.]]))
# 重新创建哑变量
combos.columns = [&amp;#39;gre&amp;#39;, &amp;#39;gpa&amp;#39;, &amp;#39;prestige&amp;#39;, &amp;#39;intercept&amp;#39;]
dummy_ranks = pd.get_dummies(combos[&amp;#39;prestige&amp;#39;], prefix=&amp;#39;prestige&amp;#39;)
dummy_ranks.columns = [&amp;#39;prestige_1&amp;#39;, &amp;#39;prestige_2&amp;#39;, &amp;#39;prestige_3&amp;#39;, &amp;#39;prestige_4&amp;#39;]
# 只保留用于预测的列
cols_to_keep = [&amp;#39;gre&amp;#39;, &amp;#39;gpa&amp;#39;, &amp;#39;prestige&amp;#39;, &amp;#39;intercept&amp;#39;]
combos = combos[cols_to_keep].join(dummy_ranks.ix[:, &amp;#39;prestige_2&amp;#39;:])
# 使用枚举的数据集来做预测
combos[&amp;#39;admit_pred&amp;#39;] = result.predict(combos[train_cols])
print combos.head()
#    gre       gpa  prestige  intercept  prestige_2  prestige_3  prestige_4  admit_pred
# 0  220  2.260000         1          1           0           0           0    0.157801
# 1  220  2.260000         2          1           1           0           0    0.087056
# 2  220  2.260000         3          1           0           1           0    0.046758
# 3  220  2.260000         4          1           0           0           1    0.038194
# 4  220  2.453333         1          1           0           0           0    0.179574
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;现在我们已生成了预测结果，接着通过画图来呈现结果。我编写了一个名为 &lt;code&gt;isolate_and_plot&lt;/code&gt; 的辅助函数，可以比较给定的变量与不同的声望等级、组合的平均可能性。为了分离声望和其他变量，我使用了 &lt;code&gt;pivot_table&lt;/code&gt; 来简单地聚合数据。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def isolate_and_plot(variable):
    # isolate gre and class rank
    grouped = pd.pivot_table(combos, values=[&amp;#39;admit_pred&amp;#39;], index=[variable, &amp;#39;prestige&amp;#39;],
                            aggfunc=np.mean)
    # in case you&amp;#39;re curious as to what this looks like
    # print grouped.head()
    #                      admit_pred
    # gre        prestige            
    # 220.000000 1           0.282462
    #            2           0.169987
    #            3           0.096544
    #            4           0.079859
    # 284.444444 1           0.311718
    # make a plot
    colors = &amp;#39;rbgyrbgy&amp;#39;
    for col in combos.prestige.unique():
        plt_data = grouped.ix[grouped.index.get_level_values(1)==col]
        pl.plot(plt_data.index.get_level_values(0), plt_data[&amp;#39;admit_pred&amp;#39;],
                color=colors[int(col)])
    pl.xlabel(variable)
    pl.ylabel(&amp;quot;P(admit=1)&amp;quot;)
    pl.legend([&amp;#39;1&amp;#39;, &amp;#39;2&amp;#39;, &amp;#39;3&amp;#39;, &amp;#39;4&amp;#39;], loc=&amp;#39;upper left&amp;#39;, title=&amp;#39;Prestige&amp;#39;)
    pl.title(&amp;quot;Prob(admit=1) isolating &amp;quot; + variable + &amp;quot; and presitge&amp;quot;)
    pl.show()
isolate_and_plot(&amp;#39;gre&amp;#39;)
isolate_and_plot(&amp;#39;gpa&amp;#39;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;center&gt;&lt;img alt="isolated_rank_and_gre.png" src="/article_img/Logistic_regression_python_2014_08_09_20_07/isolated_rank_and_gre.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="isolated_rank_and_gpa.png" src="/article_img/Logistic_regression_python_2014_08_09_20_07/isolated_rank_and_gpa.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;结果图显示了 gre, gpa 和 prestige 如何影响录取。可以看出，随着 gre 和 gpa 的增加，录取可能性如何逐渐增加，并且，不同的学校声望对录取可能性的增加程度相差很大。&lt;/p&gt;
&lt;h3&gt;结束语&lt;/h3&gt;
&lt;p&gt;逻辑回归是用于分类的优秀算法，尽管有一些更加性感的，或是黑盒分类器算法，如SVM和随机森林（RandomForest）在一些情况下性能更好，但深入了解你正在使用的模型是很有价值的。很多时候你可以使用随机森林来筛选模型的特征，并基于筛选出的最佳的特征，使用逻辑回归来重建模型。&lt;/p&gt;</summary><category term="classification"></category><category term="algorithm"></category><category term="Python"></category></entry><entry><title>逻辑回归</title><link href="/2014/Logistic_regression_2014_08_07_19_47.html" rel="alternate"></link><updated>2014-08-07T19:47:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2014-08-07:2014/Logistic_regression_2014_08_07_19_47.html</id><summary type="html">&lt;h2&gt;Logistic regression&lt;/h2&gt;
&lt;p&gt;设样本 $ {x,y} $，$ y=0 $ 或 $ y=1 $，表正类或负类。 $ x $是$ m $维样本特征向量。那么，$ x $属于正类，也就是$ y=1 $的“概率”表示为：&lt;/p&gt;
&lt;p&gt;$$ P(y=1\mid x;\theta)=\sigma(\theta^Tx)=\frac{1}{1+\exp(-\theta^Tx)} $$&lt;/p&gt;
&lt;p&gt;$ \theta $是模型参数（或叫回归系数），$ \sigma $是sigmoid函数。&lt;/p&gt;
&lt;p&gt;实际上，是由 $ x $属于正类的可能性和负类的可能性的比值的对数变换得到的：&lt;/p&gt;
&lt;p&gt;$$ \begin{aligned}\log it(x)&amp;amp;=\ln(\frac{P(y=1\mid x)}{P(y=0\mid x)}) \\ &amp;amp;=\ln(\frac{P(y=1\mid x)}{1-P(y=1\mid x)}) \\ &amp;amp;=\theta_0+\theta_1 x_1+\theta_2 x_2+\cdot \cdot \cdot +\theta_m x_m \end{aligned} $$&lt;/p&gt;
&lt;p&gt;$ \theta_m $为权重（或叫回归系数），$ x_m $为因数。&lt;/p&gt;
&lt;p&gt;它跟线性回归最大的区别在于将线性回归中很大的数压缩到[0,1]之间，这样值输出表达为“可能性”增加说服力（？？？？），同时弱化冒尖变量的影响&lt;/p&gt;</summary><category term="classification"></category><category term="algorithm"></category></entry><entry><title>极大似然估计</title><link href="/2014/MLE_2014_08_07_19_09.html" rel="alternate"></link><updated>2014-08-07T19:09:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2014-08-07:2014/MLE_2014_08_07_19_09.html</id><summary type="html">&lt;h2&gt;Maximum Likelihood Estimate&lt;/h2&gt;
&lt;h3&gt;似然函数&lt;/h3&gt;
&lt;p&gt;在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。&lt;/p&gt;
&lt;p&gt;似然函数在统计推断中有重大作用，如在最大似然估计和费雪信息之中的应用等等。“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”和“或然性”或“概率”又有明确的区分。&lt;/p&gt;
&lt;p&gt;概率 用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而&lt;/p&gt;
&lt;p&gt;似然性 则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。&lt;/p&gt;
&lt;p&gt;在这种意义上，似然函数可以理解为条件概率的逆反。&lt;/p&gt;
&lt;p&gt;在已知某个参数&lt;strong&gt;B&lt;/strong&gt;时，事件&lt;strong&gt;A&lt;/strong&gt;会发生的概率写作 $ \mathbb{P}(A \mid B) $。&lt;/p&gt;
&lt;p&gt;$$ P(A \mid B) = \frac{P(A , B)}{P(B)} $$&lt;/p&gt;
&lt;p&gt;利用贝叶斯定理，&lt;/p&gt;
&lt;p&gt;$$ P(B \mid A) = \frac{P(A \mid B)\;P(B)}{P(A)} $$&lt;/p&gt;
&lt;p&gt;因此，我们可以反过来构造表示似然性的方法：已知有事件&lt;strong&gt;A&lt;/strong&gt;发生，运用似然函数$$ \mathbb{L}(B \mid A) $$，我们估计参数&lt;strong&gt;B&lt;/strong&gt;的可能性。&lt;/p&gt;
&lt;p&gt;形式上，似然函数也是一种条件概率函数，但我们关注的变量改变了：&lt;/p&gt;
&lt;p&gt;$$ b\mapsto P(A \mid B=b) $$&lt;/p&gt;
&lt;p&gt;注意到这里并不要求似然函数满足归一性：&lt;/p&gt;
&lt;p&gt;$$ \sum_{b \in \mathcal{B}}{P(A \mid B=b)} = 1  $$&lt;/p&gt;
&lt;p&gt;一个似然函数乘以一个正的常数之后仍然是似然函数。对所有$ \alpha &amp;gt; 0 $，都可以有似然函数：&lt;/p&gt;
&lt;p&gt;$$ L(b \mid A) = \alpha \; P(A \mid B=b) $$&lt;/p&gt;
&lt;p&gt;例子：&lt;/p&gt;
&lt;p&gt;考虑投掷一枚硬币的实验。通常来说，已知投出的硬币正面朝上和反面朝上的概率各自是$ p_H = 0.5 $，便可以知道投掷若干次后出现各种结果的可能性。比如说，投两次都是正面朝上的概率是0.25。用条件概率表示，就是：&lt;/p&gt;
&lt;p&gt;$$ P(\mbox{HH} \mid p_H = 0.5) = 0.5^2 = 0.25 $$&lt;/p&gt;
&lt;p&gt;其中&lt;strong&gt;H&lt;/strong&gt;表示正面朝上。&lt;/p&gt;
&lt;p&gt;在统计学中，我们关心的是在已知一系列投掷的结果时，关于硬币投掷时正面朝上的可能性的信息。我们可以建立一个统计模型：假设硬币投出时会有$ p_H $ 的概率正面朝上，而有$ 1 − p_H $的概率反面朝上。这时，条件概率可以改写成似然函数：&lt;/p&gt;
&lt;p&gt;$$ L(p_H = 0.5 \mid \mbox{HH}) = P(\mbox{HH}\mid p_H = 0.5) =0.25 $$&lt;/p&gt;
&lt;p&gt;也就是说，对于取定的似然函数，在观测到两次投掷都是正面朝上时，$ p_H = 0.5 $ 的似然性是0.25（这并不表示当观测到两次正面朝上时$ p_H = 0.5 $ 的&lt;strong&gt;概率&lt;/strong&gt;是0.25）。&lt;/p&gt;
&lt;p&gt;如果考虑$ p_H = 0.6 $，那么似然函数的值也会改变。&lt;/p&gt;
&lt;p&gt;$$ L(p_H = 0.6 \mid \mbox{HH}) = P(\mbox{HH}\mid p_H = 0.6) =0.36 $$&lt;/p&gt;
&lt;p&gt;注意到似然函数的值变大了。这说明，如果参数$ p_H $的取值变成0.6的话，结果观测到连续两次正面朝上的概率要比假设$ p_H = 0.5 $时更大。也就是说，参数 $ p_H $ 取成0.6 要比取成0.5 更有说服力，更为“合理”。总之，似然函数的重要性不是它的具体取值，而是当参数变化时函数到底变小还是变大。对同一个似然函数，如果存在一个参数值，使得它的函数值达到最大的话，那么这个值就是最为“合理”的参数值。&lt;/p&gt;
&lt;p&gt;在这个例子中，似然函数实际上等于：
$$ L(p_H = \theta \mid \mbox{HH}) = P(\mbox{HH}\mid p_H = \theta) =\theta^2 $$ 
其中 $0 \le p_H \le 1 $。&lt;/p&gt;
&lt;p&gt;如果取$ p_H = 1 $ ，那么似然函数达到最大值1。也就是说，当连续观测到两次正面朝上时，假设硬币投掷时正面朝上的概率为1是最合理的。&lt;/p&gt;
&lt;p&gt;类似地，如果观测到的是三次投掷硬币，头两次正面朝上，第三次反面朝上，那么似然函数将会是：
$$L(p_H = \theta \mid \mbox{HHT}) = P(\mbox{HHT}\mid p_H = \theta) =\theta^2(1 - \theta)$$ ，
其中T表示反面朝上，$ 0 \le p_H \le 1 $ 。&lt;/p&gt;
&lt;p&gt;这时候，似然函数的最大值将会在$ p_H = \frac{2}{3} $的时候取到。也就是说，当观测到三次投掷中前两次正面朝上而后一次反面朝上时，估计硬币投掷时正面朝上的概率$ p_H = \frac{2}{3} $ 是最合理的。&lt;/p&gt;
&lt;h3&gt;似然估计&lt;/h3&gt;
&lt;p&gt;$$ L(\theta)=L(x_1,x_2,\cdot \cdot \cdot ,x_n;\theta) = \prod_{i=1}^{n}P(x_i;\theta),\theta\in \Theta $$&lt;/p&gt;
&lt;p&gt;既概率密度参数是 $ \theta $ 时，得到x组样本的概率。&lt;br&gt;
x是已知的，只有 $ \theta $ 是未知的。&lt;br&gt;
它是$ \theta $的函数，表示不同$ \theta $下取得当前样本的可能性。称为参数$ \theta $相对于样本集x的似然函数。&lt;/p&gt;
&lt;h3&gt;最大似然估计&lt;/h3&gt;
&lt;p&gt;$$ \theta = argmax L(\theta) $$&lt;/p&gt;
&lt;p&gt;有时候$ L(\theta) $ 是连乘的，还可以定义对数似然，将其变为连加：&lt;/p&gt;
&lt;p&gt;$$ H(\theta) = \ln L(\theta) = ln\prod_{i=1}^{n}P(x_i;\theta) = \sum_{i=1}^{n}\ln P(x_i;\theta) $$&lt;/p&gt;
&lt;p&gt;以上，要求 $ \theta $ ，只要使 $ \theta $ 的似然函数 $ L(\theta) $极大化，然后极大值对应的$ \theta $就是我们的估计。&lt;/p&gt;
&lt;p&gt;最值等于求导，导数为0，那么方程解就是$ \theta $了。&lt;br&gt;
如果$ \theta $包括多个参数向量，那么求的是$ L(\theta) $对应所有参数的偏导，也就是梯度。&lt;/p&gt;
&lt;p&gt;n个未知数就是n个方程，方程组的解就是极值点，当然也就得到几个参数。&lt;/p&gt;
&lt;p&gt;总结：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;写出似然函数。&lt;/li&gt;
&lt;li&gt;对似然函数取对数，整理。&lt;/li&gt;
&lt;li&gt;求导数，令导数得0，得到似然方程。&lt;/li&gt;
&lt;li&gt;求解。&lt;/li&gt;
&lt;/ol&gt;</summary><category term="algorithm"></category><category term="Math"></category></entry><entry><title>梯度下降</title><link href="/2014/gradient_descent_2014_08_02_21_29.html" rel="alternate"></link><updated>2014-08-02T21:29:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2014-08-02:2014/gradient_descent_2014_08_02_21_29.html</id><summary type="html">&lt;h2&gt;gradient descent&lt;/h2&gt;
&lt;p&gt;这是求解无约束优化问题最简单和最古老的方法，常用于机器学习和人工智能当中用来递归性地逼近最小偏差模型by wiki。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;f = lambda x : x**3 #随便写的个函数
x = 2
step = 0.01 
lossChange = f(x)
lossed = f(x)
print &amp;#39;x:&amp;#39;, x, &amp;#39;lossChange:&amp;#39;, lossChange, &amp;#39;loss:&amp;#39;, lossed
while lossChange &amp;gt; 0.00000001: #当变化小到一定程度是认为是局部最小
    x = x - step * 3 *( x**2) #减的是梯度方向上的变化
    lossChange = lossed - f(x)
    lossed = f(x)
    print &amp;#39;x:&amp;#39;, x, &amp;#39;lossChange:&amp;#39;, lossChange, &amp;#39;loss:&amp;#39;, lossed
    print x
    print f(x)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;最后找到个局部最优解！&lt;/p&gt;
&lt;p&gt;其迭代公式为 $ f(k+1)=f(k)-\theta f'(k) $,其中 $ \theta $是步长，或叫学习速率。步长越小收敛速度越慢，但步长过大有可能跳过不保证每次迭代都减少，甚至不一定收敛。&lt;/p&gt;
&lt;p&gt;最后再说说意义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在标量场f中的一点处存在一个矢量G(或者上边的f`)，该矢量方向为f在该点处变化率最大的方向，其模也等于这个最大变化率的数值，则矢量G称为标量场f的梯度。&lt;br&gt;
在向量微积分中，标量场的梯度是一个向量场。&lt;br&gt;
标量场中某一点上的梯度指向标量场增长最快的方向，梯度的长度是这个最大的变化率。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;通过负梯度，一步步逼近某个局部的最值。&lt;/p&gt;
&lt;h2&gt;进阶--随机梯度下降法&lt;/h2&gt;
&lt;p&gt;stochastic gradient descent，也叫增量梯度下降。&lt;/p&gt;
&lt;p&gt;由于梯度下降法收敛速度慢，而随机梯度下降法会快很多&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;根据某个单独样例的误差增量计算权值更新，得到近似的梯度下降搜索。（随机取一个样例）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可以看作为每个单独的训练样例定义不同的误差函数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在迭代所有训练样例时，这些权值更新的序列给出了对于原来误差函数的梯度下降的一个合理近似。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过使下降速率的值足够小，可以使随机梯度下降以任意程度接近于真实梯度下降。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;标准梯度下降和随机梯度下降之间的关键区别&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;标准梯度下降是在权值更新前对所有样例汇总误差，而随机梯度下降的权值是通过考查某个训练样例来更新的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在标准梯度下降中，权值更新的每一步对多个样例求和，需要更多的计算。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;标准梯度下降，由于使用真正的梯度，标准梯度下降对于每一次权值更新经常使用比随机梯度下降大的步长。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果标准误差曲面有多个局部极小值，随机梯度下降有时可能避免陷入这些局部极小值中。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary><category term="classification"></category><category term="algorithm"></category><category term="Python"></category><category term="Math"></category></entry><entry><title>neural network</title><link href="/2014/neural_network_2014_06_14_19_58.html" rel="alternate"></link><updated>2014-06-14T19:58:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2014-06-14:2014/neural_network_2014_06_14_19_58.html</id><summary type="html">&lt;h2&gt;Principles of training multi-layer neural network using backpropagation&lt;/h2&gt;
&lt;p&gt;The project describes teaching process of multi-layer neural network employing &lt;em&gt;backpropagation&lt;/em&gt; algorithm. To illustrate this 
process the three layer neural network with two inputs and one output,which is shown in the picture below, is used:&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img01.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Each neuron is composed of two units. First unit adds products of weights coefficients and input signals. The second unit realise nonlinear
 function, called neuron activation function. Signal &lt;em&gt;e&lt;/em&gt; is adder output signal, and &lt;em&gt;y = f(e)&lt;/em&gt; is output signal of nonlinear 
element. Signal &lt;em&gt;y&lt;/em&gt; is also output signal of neuron.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img01b.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;To teach the neural network we need training data set. The training data set consists of input signals (&lt;em&gt;x&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and 
&lt;em&gt;x&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt; ) assigned with corresponding target (desired output) &lt;em&gt;z&lt;/em&gt;. The network training is an iterative process. In each 
iteration weights coefficients of nodes are modified using new data from training data set. Modification is calculated using algorithm 
described below:
Each teaching step starts with forcing both input signals from training set. After this stage we can determine output signals values for 
each neuron in each network layer. Pictures below illustrate how signal is propagating through the network, Symbols &lt;em&gt;w&lt;sub&gt;(xm)n&lt;/sub&gt;&lt;/em&gt;
 represent weights of connections between network input &lt;em&gt;x&lt;sub&gt;m&lt;/sub&gt;&lt;/em&gt; and neuron &lt;em&gt;n&lt;/em&gt; in input layer. Symbols &lt;em&gt;y&lt;sub&gt;n&lt;/sub&gt;&lt;/em&gt;
 represents output signal of neuron &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img02.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img03.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img04.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Propagation of signals through the hidden layer. Symbols &lt;em&gt;w&lt;sub&gt;mn&lt;/sub&gt;&lt;/em&gt; represent weights of connections between output of neuron
 &lt;em&gt;m&lt;/em&gt; and input of neuron &lt;em&gt;n&lt;/em&gt; in the next layer.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img05.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img06.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Propagation of signals through the output layer.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img07.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;In the next algorithm step the output signal of the network &lt;em&gt;y&lt;/em&gt; is compared with the desired output value (the target), which is found
 in training data set. The difference is called error signal &lt;font face="symbol"&gt;&lt;span style="font-family: Symbol"&gt;&lt;em&gt;d&lt;/em&gt;&lt;/span&gt;&lt;/font&gt; of
 output layer neuron. &lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img08.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;It is impossible to compute error signal for internal neurons directly, because output values of these neurons are unknown. For many years 
the effective method for training  multiplayer networks has been unknown. Only in the middle eighties the backpropagation algorithm has been
 worked out. The idea is to propagate error signal &lt;font face="symbol"&gt;&lt;span style="font-family: Symbol"&gt;&lt;em&gt;d&lt;/em&gt;&lt;/span&gt;&lt;/font&gt; (computed in 
single teaching step) back to all neurons, which output signals were input for discussed neuron.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img09.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img10.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The weights' coefficients &lt;em&gt;w&lt;sub&gt;mn&lt;/sub&gt;&lt;/em&gt; used to propagate errors back are equal to this used during computing output value. Only the 
direction of data flow is changed (signals are propagated from output to inputs one after the other). This technique is used for all network
 layers. If propagated errors came from few neurons they are added. The illustration is below:&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img11.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img12.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img13.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;When the error signal for each neuron is computed, the weights coefficients of each neuron input node may be modified. In formulas below 
&lt;em&gt;df(e)/de&lt;/em&gt; represents derivative of neuron activation function (which weights are modified).&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img14.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img15.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img16.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img17.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img18.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="" src="/article_img/neural_network_2014_06_14_19_58/img19.gif" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Coefficient &lt;font face="symbol"&gt;&lt;span style="font-family: Symbol"&gt;&lt;em&gt;h&lt;/em&gt;&lt;/span&gt;&lt;/font&gt; affects network teaching speed. There are a few
 techniques to select this parameter. The first method is to start teaching process with large value of the parameter. While weights 
coefficients are being established the parameter is being decreased gradually. The second, more complicated, method starts teaching with 
small parameter value. During the teaching process the parameter is being increased when the teaching is advanced and then decreased again in 
the final stage. Starting teaching process with low parameter value enables to determine weights coefficients signs.&lt;/p&gt;</summary><category term="CNN"></category><category term="ML"></category></entry><entry><title>k-means++</title><link href="/2014/k_means_plus_2014_03_15_23_18.html" rel="alternate"></link><updated>2014-03-15T23:18:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2014-03-15:2014/k_means_plus_2014_03_15_23_18.html</id><summary type="html">&lt;h2&gt;k-means++&lt;/h2&gt;
&lt;h3&gt;过程&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;随机抽几个种子点。&lt;/li&gt;
&lt;li&gt;对每个点，都计算其和最近的“种子点”的距离$ D(X) $并保存在一个数组中，然后求得$ \sum D(X) $。&lt;/li&gt;
&lt;li&gt;再取1个随机值，用权重的方式计算下一个种子点。具体为：先取一个能落在$ \sum D(X) $ 中的random，然后&lt;code&gt;random-=D(X)&lt;/code&gt;,知道其&amp;lt;=0。此点为下一个种子。&lt;/li&gt;
&lt;li&gt;重复2，3。&lt;/li&gt;
&lt;li&gt;进行k-means。&lt;/li&gt;
&lt;/ol&gt;</summary><category term="ML"></category><category term="clustering"></category><category term="algorithm"></category><category term="Math"></category></entry><entry><title>k-means</title><link href="/2014/k_means_2014_03_15_22_51.html" rel="alternate"></link><updated>2014-03-15T22:51:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2014-03-15:2014/k_means_2014_03_15_22_51.html</id><summary type="html">&lt;h2&gt;k-means&lt;/h2&gt;
&lt;p&gt;典型的基于原型的目标函数聚类方法的代表，它是数据点到原型的某种距离作为优化的目标函数，利用函数求极值的方法得到迭代运算的调整规则。K-means算法以某种距离作为相似度测度，它是求对应某一初始聚类中心向量V最优分类，使得评价指标J最小。算法采用误差平方和准则函数作为聚类准则函数。&lt;/p&gt;
&lt;h3&gt;标量&lt;/h3&gt;
&lt;p&gt;嗯，各种距离&lt;/p&gt;
&lt;h3&gt;二元变量&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;对称&lt;/li&gt;
&lt;li&gt;非对称：1与1同，0与1不同，0与0不同。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;分类变量&lt;/h3&gt;
&lt;p&gt;略&lt;/p&gt;
&lt;h3&gt;序数变量&lt;/h3&gt;
&lt;p&gt;略&lt;/p&gt;
&lt;h3&gt;向量&lt;/h3&gt;
&lt;p&gt;余弦度量(相似度)：
$$ \frac{xy}{\left | \left \| x \right \| \left \| y \right \| \right |}=\sigma(x,y)  $$&lt;/p&gt;
&lt;h3&gt;标量规格化&lt;/h3&gt;
&lt;p&gt;反正就是映射到[0,1]上，略&lt;/p&gt;
&lt;h3&gt;算法过程&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;从N个文档随机选取K个文档作为质心&lt;/li&gt;
&lt;li&gt;对剩余的每个文档测量其到每个质心的距离，并把它归到最近的质心的类&lt;/li&gt;
&lt;li&gt;重新计算已经得到的各个类的质心&lt;/li&gt;
&lt;li&gt;迭代2～3步直至新的质心与原质心相等或小于指定阈值，算法结束&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;简单的思路来说就是：
先定：k，既分为k个簇，定义中心点，做第一次聚类&lt;br&gt;
调整：某类每一个项分别算术平均，更新中心点&lt;br&gt;
再聚类，再调整，直到中心值变化小于阈值。&lt;/p&gt;</summary><category term="ML"></category><category term="clustering"></category><category term="algorithm"></category><category term="Math"></category></entry><entry><title>“距离”计算</title><link href="/2014/distance_2014_03_15_22_14.html" rel="alternate"></link><updated>2014-03-15T22:14:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2014-03-15:2014/distance_2014_03_15_22_14.html</id><summary type="html">&lt;h2&gt;各种距离&lt;/h2&gt;
&lt;h3&gt;闵可夫斯基距离&lt;/h3&gt;
&lt;p&gt;$$ d_{ij}=\sqrt[\lambda]{\sum_{k=1}^{n}\left|x_{ik}-x_{jk}\right|^{\lambda}}  $$&lt;/p&gt;
&lt;p&gt;以星型逼近,$ \lambda = 0.25 $时，几乎为一个点，$ \lambda = 1 $ 相当于曼哈顿距离，而$ \lambda = 2 $时，逼近范围是个圈，既欧式距离，$ \lambda = \infty $ 时，方型，切比雪夫距离。&lt;/p&gt;
&lt;h3&gt;欧式距离&lt;/h3&gt;
&lt;p&gt;$$ d_{ij}=\sqrt[2]{\sum_{k=1}^{n}\left|x_{ik}-x_{jk}\right|^{2}} $$&lt;/p&gt;
&lt;p&gt;以圆型逼近。&lt;/p&gt;
&lt;p&gt;曼哈顿距离：&lt;/p&gt;
&lt;p&gt;$$ d_{ij}=\sum_{k=1}^{n}\left|x_{ik}-x_{jk}\right| $$&lt;/p&gt;
&lt;p&gt;以菱型逼近。&lt;/p&gt;
&lt;h4&gt;问题&lt;/h4&gt;
&lt;p&gt;闵氏距离，曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点。
举个例子：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;二维样本(身高,体重)，其中身高范围是150~190，体重范围是50~60，有三个样本：a(180,50)，b(190,50)，c(180,60)。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;简单说来，闵氏距离的缺点主要有两个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。&lt;/li&gt;
&lt;li&gt;没有考虑各个分量的分布（期望，方差等)可能是不同的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;标准化欧氏距离&lt;/h3&gt;
&lt;p&gt;标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，好吧！那我先将各个分量都“标准化”到均值、方差相等吧。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standarddeviation)为s，那么X的“标准化变量”表示为：&lt;br&gt;
而且标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是：
$$ X^{*} = \frac{X-m}{s} $$&lt;/p&gt;
&lt;p&gt;标准化后的值 =  ( 标准化前的值  － 分量的均值 ) /分量的标准差&lt;/p&gt;
&lt;p&gt;标准化欧氏距离的公式：
$$ d_{ij}=\sqrt{\sum_{k=1}^{n}(\frac{x_{ik}-x_{jk}}{s_{k}})^{2}} $$  &lt;/p&gt;
&lt;p&gt;如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(WeightedEuclidean distance)。&lt;/p&gt;
&lt;h3&gt;马氏距离&lt;/h3&gt;
&lt;p&gt;它是一种有效的计算两个未知样本集的相似度的方法。与欧氏距离不同的是它考虑到各种特性之间的联系,是一种采样协方差来计算两点之间距离的方法。(欧氏距离是马氏距离的特殊情形)&lt;/p&gt;
&lt;p&gt;对于一个均值为 $ \mu=(\mu_1,\mu_2,\mu_3,\dots,\mu_p)^T $协方差矩阵为$ \sum $的多变量向量 $ x=(x_1,x_2,x_3,\dots,x_p)^T $,&lt;br&gt;
其马氏距离为&lt;/p&gt;
&lt;p&gt;$$ D_M(x)=\sqrt{(x-\mu)^T\Sigma^{-1}(x-\mu)} $$&lt;/p&gt;
&lt;p&gt;马氏距离也可以定义为两个服从同一分布并且其协方差矩阵为Σ的随机变量 $\vec{x}$与 $\vec{y}$ 的差异程度:&lt;/p&gt;
&lt;p&gt;$$ d(\vec{x},\vec{y})=\sqrt{(\vec{x}-\vec{y})^T\Sigma^{-1} (\vec{x}-\vec{y})} $$&lt;/p&gt;
&lt;p&gt;如果协方差矩阵为单位矩阵,那么马氏距离就简化为欧式距离,如果协方差矩阵为对角阵,则其也可称为正规化的欧氏距离'。&lt;/p&gt;
&lt;p&gt;$$ d(\vec{x},\vec{y})= \sqrt{\sum_{i=1}^p {(x_i - y_i)^2 \over \sigma_i^2}} $$&lt;/p&gt;
&lt;p&gt;其中 $ \sigma_i $ 是 $ x_i $ 的标准差。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;两点之间的马氏距离与原始数据的测量单位无关。&lt;/li&gt;
&lt;li&gt;标准化数据和中心化数据(即原始数据与均值之差）计算出的二点之间的马氏距离相同。&lt;/li&gt;
&lt;li&gt;可以排除变量之间的相关性的干扰。&lt;/li&gt;
&lt;li&gt;满足距离的四个基本公理：非负性、自反性、对称性和三角不等式。&lt;/li&gt;
&lt;/ul&gt;</summary><category term="ML"></category><category term="clustering"></category><category term="algorithm"></category><category term="Math"></category></entry><entry><title>指数平滑</title><link href="/2013/ES_2013_12_18_21_11.html" rel="alternate"></link><updated>2013-12-18T21:11:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2013-12-18:2013/ES_2013_12_18_21_11.html</id><summary type="html">&lt;h2&gt;Exponential Smoothing&lt;/h2&gt;
&lt;p&gt;时间序列：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;趋势性&lt;/li&gt;
&lt;li&gt;周期性（季节性）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$ S_{t} = \alpha y_{t-1}+(1-\alpha)S_{t-1} $$&lt;/p&gt;
&lt;p&gt;$S_{t}$ 为平滑值，$ y_{t-1} $ 为t-1实际值，
&lt;br&gt;
$ \alpha $ 为平滑函数，$ \alpha \in \left [ 0,1 \right ] $，远期对本期平滑值影响程度。$ \alpha $ 越小，本期的影响程度越低，时间波动大时应取较小的$ \alpha $。&lt;/p&gt;
&lt;h3&gt;一次平滑&lt;/h3&gt;
&lt;p&gt;时间数列无明显变化，例如上边的。&lt;/p&gt;
&lt;h3&gt;二次平滑&lt;/h3&gt;
&lt;p&gt;是对一次平滑的再次平滑，适用于线性趋势的时间序列。&lt;/p&gt;
&lt;h4&gt;基本公式&lt;/h4&gt;
&lt;p&gt;$$ S_{t}^1 = \alpha y_{t}+(1-\alpha)S_{t-1}^1 \\ S_{t}^2 = \alpha S_{t}^1+(1-\alpha)S_{t-1}^2 $$&lt;/p&gt;
&lt;p&gt;&lt;span style="color:red"&gt;&lt;strong&gt;注意St上标为期数的前后顺序！！！下同！&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;预测模型&lt;/h4&gt;
&lt;p&gt;$$ Y_{t+T}=a_{t}+b_{t}T \\ a_{t}=2S_{t}^1-S_{t}^2 \\ b_{t}=(\frac{\alpha}{1-\alpha})(S_{t}^1-S_{t}^2) $$
&lt;center&gt;T为t期到预测期的隔期数。&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;平滑值并不直接用于预测，只是求出线性预测模型的平滑系数。&lt;br&gt;
基本思想：预测值是以前观测值的加权和，且对不同的数据给与不同的权，新数据权重大，旧的较小。&lt;/p&gt;
&lt;h3&gt;三次平滑&lt;/h3&gt;
&lt;p&gt;当时间序列呈现出非线性趋势变化时，采用！！&lt;/p&gt;
&lt;h4&gt;基本公式&lt;/h4&gt;
&lt;p&gt;$$ S_{t}^1 = \alpha y_{t}+(1-\alpha)S_{t-1}^1 \\ S_{t}^2 = \alpha S_{t}^1+(1-\alpha)S_{t-1}^2 \\ S_{t}^3 = \alpha S_{t}^2+(1-\alpha)S_{t-1}^3 $$&lt;/p&gt;
&lt;h4&gt;预测模型&lt;/h4&gt;
&lt;p&gt;$$ Y_{t+T}=a_{t}+b_{t}T+c_{t}T^2 \\ a_{t}=3S_{t}^1-3S_{t}^2+S_{t}^3 \\ b_{t}=(\frac{\alpha^2}{2(1-\alpha)^2})\left [ (6-5\alpha)S_{t}^1-(10-8\alpha)S_{t}^2+(4-3\alpha)S_{t}^3 \right ]  \\ c_{t}=(\frac{\alpha^2}{(1-\alpha)^2})(S_{t}^1-2S_{t}^2+S_{t}^3) $$&lt;/p&gt;</summary><category term="ML"></category><category term="Linear"></category><category term="DataProcessing"></category><category term="Math"></category></entry><entry><title>最小二乘法</title><link href="/2013/curve_fitting_2013_12_16_17_59.html" rel="alternate"></link><updated>2013-12-16T17:59:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2013-12-16:2013/curve_fitting_2013_12_16_17_59.html</id><summary type="html">&lt;h2&gt;curve fitting&lt;/h2&gt;
&lt;p&gt;曲线拟合的最小二乘法：&lt;/p&gt;
&lt;p&gt;由已知离散点选择与测试点误差最小的曲线。&lt;/p&gt;
&lt;p&gt;$$ S(x) = a_{0}\varphi_{0}(x)+a_{1}\varphi_{1}(x)+...+a_{n}\varphi_{n}(x) $$ &lt;/p&gt;
&lt;p&gt;若&lt;/p&gt;
&lt;p&gt;$$ (\varphi_{j},\varphi_{k})=\sum_{i=0}^{m}\omega(x_{i})\varphi_{j}(x_{i})\varphi_{k}(x_{i}) $$ &lt;/p&gt;
&lt;p&gt;$$ (f,\varphi_{k})=\sum_{i=0}^{m}\omega(x_{i})f(x_{i})\varphi_{k}(x_{i})\equiv d_{k} $$ &lt;/p&gt;
&lt;p&gt;上式可写为：&lt;/p&gt;
&lt;p&gt;$$ \sum_{j=0}^{m}(\varphi_{k},\varphi_{j})a_{j}=d_{k};(k=0,1,...,n) $$ &lt;/p&gt;
&lt;p&gt;将该法方程改写为矩阵：&lt;/p&gt;
&lt;p&gt;$$ Ga=d $$ &lt;/p&gt;
&lt;p&gt;其中$ a=(a_{0},a_{1},...,a_{n})^{T},d=(d_{0},d_{1},...,d_{n})^{T} $ ，&lt;/p&gt;
&lt;p&gt;$$ \begin{aligned} &amp;amp;G= \end{aligned}\begin{bmatrix}&amp;amp;(\varphi_{0},\varphi_{0})  &amp;amp;(\varphi_{0},\varphi_{1})  &amp;amp;\cdot \cdot \cdot  &amp;amp;(\varphi_{0},\varphi_{n}) \\ &amp;amp;(\varphi_{1},\varphi_{0})  &amp;amp;(\varphi_{1},\varphi_{1})  &amp;amp;\cdot \cdot \cdot  &amp;amp;(\varphi_{1},\varphi_{n}) \\ &amp;amp;(\varphi_{n},\varphi_{0})  &amp;amp;(\varphi_{n},\varphi_{1})  &amp;amp;\cdot \cdot \cdot  &amp;amp;(\varphi_{n},\varphi_{n}) \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;其平方误差为：&lt;/p&gt;
&lt;p&gt;$$ \left \| \delta  \right \|_{2}^{2} = \sum_{i=0}^{m}\omega(x_{i})[S(x_{i})-f(x_{i})]^2 $$ &lt;/p&gt;</summary><category term="ML"></category><category term="Linear"></category><category term="DataProcessing"></category><category term="Math"></category></entry><entry><title>傅立叶级数</title><link href="/2013/fourier_series_2013_01_30_00_17.html" rel="alternate"></link><updated>2013-01-30T00:17:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2013-01-30:2013/fourier_series_2013_01_30_00_17.html</id><summary type="html">&lt;h2&gt;fourier series&lt;/h2&gt;
&lt;p&gt;周期为T的函数x(t)标识为无穷级数：&lt;/p&gt;
&lt;p&gt;$$ x(t)=\sum_{k=-\infty}^{+\infty}a_k*e^{jk(\frac{2\pi}{T})t} $$
j为虚数&lt;/p&gt;
&lt;p&gt;其中$ a_k $可按下式计算：
$$ a_k=\frac{1}{T}\int_T x(t)*e^{-jk(\frac{2\pi}{T})t} dt $$&lt;/p&gt;
&lt;p&gt;注意到：$ f_k(t)=e^{jk(\frac{2\pi}{T})t} $ 是周期为T的函数，故k取不同值时的周期信号具有谐波关系（既具有同一个周期T）。&lt;br&gt;
k=0时，第一个式子中对应的这一项称为直流分量。&lt;br&gt;
k=1时，具有基波频率$ \omega_0=\frac{2\pi}{T} $ ，称为一次谐波或基波。&lt;br&gt;
类似的有二次谐波，三次谐波&lt;/p&gt;</summary><category term="DataProcessing"></category><category term="Math"></category></entry><entry><title>泰勒级数</title><link href="/2013/Taylor_series_2013_01_29_22_43.html" rel="alternate"></link><updated>2013-01-29T22:43:00+08:00</updated><author><name>kai_kai03</name></author><id>tag:,2013-01-29:2013/Taylor_series_2013_01_29_22_43.html</id><summary type="html">&lt;h2&gt;Taylor Series&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;幂级数的求导和积分可以逐项进行，因此求和函数相对比较容易。&lt;/li&gt;
&lt;li&gt;一个解析函数可被延伸为一个定义在复平面上的一个开区域上的泰勒级数通过解析延拓得到的函数，并使得复分析这种手法可行。&lt;/li&gt;
&lt;li&gt;泰勒级数可以用来近似计算函数的值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$ f(x)=f(a)+\frac{f'(a)}{1!}(x-a)+\frac{f''(a)}{2!}(x-a)^2+\cdot\cdot\cdot+\frac{f^n(a)}{n!}(x-a)^n+R_{n}(x) $$&lt;/p&gt;
&lt;p&gt;$$ R_{n}(x)=f^{n+1}(a+\theta(x-a))\frac{(x-a)^{n+1}}{(n+1)!} $$&lt;/p&gt;
&lt;p&gt;当然看这个的目的嘛，它怎么说也算傅立叶变换的前置科技。&lt;/p&gt;</summary><category term="DataProcessing"></category><category term="Math"></category></entry></feed>